diff --git a/AGENTS.md b/AGENTS.md
index 7b274f8..2bbe4a7 100644
--- a/AGENTS.md
+++ b/AGENTS.md
@@ -18,6 +18,7 @@ You are a Senior Full-Stack Engineer and **Pragmatic Quality Specialist** for **
 - ✅ GitHub repo analysis via Claude extracts MCP config
 - ✅ Deployments create Fly MCP machines (when Fly secrets are set)
 - ✅ Streamable HTTP works end-to-end (backend `/api/mcp/{deployment_id}` → machine `/mcp`)
+- ✅ RegistryService hardened (concurrency safe, timeouts)
 
 **What's NOT Working Yet**:
 - ❌ Health monitoring loop + “unhealthy” state management (beyond Fly restart policy)
diff --git a/backend/alembic/env.py b/backend/alembic/env.py
index 7162759..08234a5 100644
--- a/backend/alembic/env.py
+++ b/backend/alembic/env.py
@@ -1,7 +1,8 @@
 import asyncio
-from logging.config import fileConfig
-import sys
+import logging
 import os
+import sys
+from logging.config import fileConfig
 
 # Add the project root to the python path so we can import 'app'
 # Assuming env.py is in backend/alembic/
@@ -9,13 +10,18 @@
 
 from sqlalchemy import pool
 from sqlalchemy.engine import Connection
+from sqlalchemy.exc import OperationalError
 from sqlalchemy.ext.asyncio import async_engine_from_config
-from alembic import context
 
+import app.models  # noqa: F401
+from alembic import context
 from app.core.config import settings
 from app.db.base import Base
-# Import all models so they are registered in metadata
-from app.models import *
+
+# Alembic Constants
+DB_CONNECT_TIMEOUT = 10
+DB_RETRY_ATTEMPTS = 10
+DB_RETRY_MAX_DELAY = 10
 
 config = context.config
 
@@ -56,16 +62,38 @@ async def run_async_migrations() -> None:
 
     """
 
+    engine_kwargs = {}
+    if settings.DATABASE_URL.startswith("postgresql"):
+        engine_kwargs["connect_args"] = {"connect_timeout": DB_CONNECT_TIMEOUT}
+
     connectable = async_engine_from_config(
         config.get_section(config.config_ini_section, {}),
         prefix="sqlalchemy.",
         poolclass=pool.NullPool,
+        **engine_kwargs,
     )
 
-    async with connectable.connect() as connection:
-        await connection.run_sync(do_run_migrations)
+    last_error: Exception | None = None
+    for attempt in range(1, DB_RETRY_ATTEMPTS + 1):
+        try:
+            async with connectable.connect() as connection:
+                await connection.run_sync(do_run_migrations)
+            last_error = None
+            break
+        except OperationalError as exc:
+            last_error = exc
+            if attempt >= DB_RETRY_ATTEMPTS:
+                break
+            delay_s = min(2 ** (attempt - 1), DB_RETRY_MAX_DELAY)
+            logging.warning(
+                "Database connection failed during migrations "
+                f"(attempt {attempt}/{DB_RETRY_ATTEMPTS}). Retrying in {delay_s}s...",
+            )
+            await asyncio.sleep(delay_s)
 
     await connectable.dispose()
+    if last_error is not None:
+        raise last_error
 
 def run_migrations_online() -> None:
     """Run migrations in 'online' mode."""
diff --git a/backend/app/__pycache__/main.cpython-313.pyc b/backend/app/__pycache__/main.cpython-313.pyc
index ec9a7ac..b56f4ea 100644
Binary files a/backend/app/__pycache__/main.cpython-313.pyc and b/backend/app/__pycache__/main.cpython-313.pyc differ
diff --git a/backend/app/api/__pycache__/analyze.cpython-313.pyc b/backend/app/api/__pycache__/analyze.cpython-313.pyc
index d522e0b..a06668c 100644
Binary files a/backend/app/api/__pycache__/analyze.cpython-313.pyc and b/backend/app/api/__pycache__/analyze.cpython-313.pyc differ
diff --git a/backend/app/api/__pycache__/health.cpython-313.pyc b/backend/app/api/__pycache__/health.cpython-313.pyc
index ce36607..953af00 100644
Binary files a/backend/app/api/__pycache__/health.cpython-313.pyc and b/backend/app/api/__pycache__/health.cpython-313.pyc differ
diff --git a/backend/app/api/deployments.py b/backend/app/api/deployments.py
index 17a8522..e060ccd 100644
--- a/backend/app/api/deployments.py
+++ b/backend/app/api/deployments.py
@@ -8,13 +8,29 @@
 from app.schemas.deployment import DeploymentCreate, DeploymentResponse
 from app.services.encryption import EncryptionService
 from app.services.mcp_process_manager import start_server
+from app.services.package_validator import PackageValidator
+from app.services.credential_validator import CredentialValidator
 from app.core.config import settings
 import logging
+import json
 
 router = APIRouter()
 encryption_service = EncryptionService()
+package_validator = PackageValidator()
+credential_validator = CredentialValidator()
 logger = logging.getLogger(__name__)
 
+
+def _get_error_help(error_type: str, package: str = None) -> str:
+    """Return actionable help text for different error types"""
+    help_messages = {
+        "package_not_found": f"Verify '{package}' is published to npm or PyPI. Check spelling and package name.",
+        "credential_validation_failed": "Provide all required credentials. Check the credential form for required fields.",
+        "package_validation_failed": "Could not validate package. Check your internet connection or try again later.",
+        "runtime_detection_failed": "Could not determine if package is npm or Python. Check package name format.",
+    }
+    return help_messages.get(error_type, "Check deployment logs for details.")
+
 @router.post("", response_model=DeploymentResponse)
 async def create_deployment(
     deployment_in: DeploymentCreate,
@@ -25,12 +41,130 @@ async def create_deployment(
     deployment = Deployment(
         name=deployment_in.name,
         schedule_config=deployment_in.schedule_config,
-        status="active" # Auto-activate for now
+        status="pending"  # Start as pending, will update to active/failed after validation
     )
     db.add(deployment)
-    await db.flush() # Generate ID without committing yet
+    await db.flush()  # Generate ID without committing yet
+
+    # 2. Extract and validate package BEFORE creating resources
+    mcp_config = deployment.schedule_config.get("mcp_config", {}) if deployment.schedule_config else {}
+    package = mcp_config.get("package")
+
+    logger.info(f"Deployment {deployment.id} MCP config: {mcp_config}")
+    logger.info(f"Extracted package: {package}")
+
+    if not package:
+        deployment.status = "failed"
+        deployment.error_message = "No 'package' specified in MCP config"
+        await db.commit()
+        logger.warning(f"No package configured for deployment {deployment.id}")
+        raise HTTPException(
+            status_code=400,
+            detail={
+                "error": "package_missing",
+                "message": deployment.error_message,
+                "deployment_id": str(deployment.id),
+                "help": "MCP config must include a 'package' field"
+            }
+        )
+
+    # 2a. Validate package exists in registry
+    try:
+        # Detect runtime based on package name
+        if package.startswith("@") or "/" in package:
+            runtime = "npm"
+            validation_result = await package_validator.validate_npm_package(package)
+        elif "." in package or "_" in package:
+            runtime = "python"
+            validation_result = await package_validator.validate_python_package(package)
+        else:
+            # Cannot determine runtime
+            deployment.status = "failed"
+            deployment.error_message = f"Cannot determine runtime for package: {package}"
+            await db.commit()
+            logger.error(f"Runtime detection failed for package: {package}")
+            raise HTTPException(
+                status_code=400,
+                detail={
+                    "error": "runtime_detection_failed",
+                    "message": deployment.error_message,
+                    "deployment_id": str(deployment.id),
+                    "package": package,
+                    "help": _get_error_help("runtime_detection_failed", package)
+                }
+            )
+
+        # Check if package validation passed
+        if not validation_result["valid"]:
+            deployment.status = "failed"
+            deployment.error_message = validation_result["error"]
+            await db.commit()
+            logger.warning(f"Package validation failed for {package}: {validation_result['error']}")
+            raise HTTPException(
+                status_code=400,
+                detail={
+                    "error": "package_not_found",
+                    "message": validation_result["error"],
+                    "deployment_id": str(deployment.id),
+                    "package": package,
+                    "help": _get_error_help("package_not_found", package)
+                }
+            )
+
+        # Package is valid - store runtime and version in mcp_config
+        logger.info(f"Package {package} validated successfully: runtime={runtime}, version={validation_result.get('version')}")
+        if "mcp_config" not in deployment.schedule_config:
+            deployment.schedule_config["mcp_config"] = {}
+        deployment.schedule_config["mcp_config"]["runtime"] = runtime
+        deployment.schedule_config["mcp_config"]["version"] = validation_result.get("version")
+
+    except HTTPException:
+        # Re-raise HTTPException as-is
+        raise
+    except Exception as e:
+        # Unexpected error during validation
+        deployment.status = "failed"
+        deployment.error_message = f"Package validation error: {str(e)}"
+        await db.commit()
+        logger.exception(f"Unexpected error during package validation for {package}")
+        raise HTTPException(
+            status_code=500,
+            detail={
+                "error": "package_validation_failed",
+                "message": deployment.error_message,
+                "deployment_id": str(deployment.id),
+                "package": package,
+                "help": _get_error_help("package_validation_failed", package)
+            }
+        )
+
+    # 2b. Validate credentials BEFORE encrypting and saving
+    required_env_vars = mcp_config.get("env_vars", [])
+    if required_env_vars:
+        credential_validation = credential_validator.validate_credentials(
+            deployment_in.credentials,
+            required_env_vars
+        )
+
+        if not credential_validation["valid"]:
+            deployment.status = "failed"
+            deployment.error_message = json.dumps(credential_validation["errors"])
+            await db.commit()
+            logger.warning(f"Credential validation failed for deployment {deployment.id}: {credential_validation['errors']}")
+            raise HTTPException(
+                status_code=400,
+                detail={
+                    "error": "credential_validation_failed",
+                    "message": "Required credentials are missing or invalid",
+                    "deployment_id": str(deployment.id),
+                    "errors": credential_validation["errors"],
+                    "help": _get_error_help("credential_validation_failed")
+                }
+            )
+
+        logger.info(f"Credential validation passed for deployment {deployment.id}")
 
-    # 2. Encrypt and Save Credentials
+    # 3. Encrypt and Save Credentials (validation passed)
     for service, secret in deployment_in.credentials.items():
         encrypted_val = encryption_service.encrypt(secret)
         credential = Credential(
@@ -40,84 +174,74 @@ async def create_deployment(
         )
         db.add(credential)
 
+    # Update status to active now that validation passed
+    deployment.status = "active"
     await db.commit()
     await db.refresh(deployment)
 
-    # 3. Start the MCP server
-    mcp_config = deployment.schedule_config.get("mcp_config", {}) if deployment.schedule_config else {}
-    package = mcp_config.get("package")
+    # 4. Start the MCP server (package already validated above)
+    try:
+        # Query credentials separately to avoid greenlet issues
+        credentials_result = await db.execute(
+            select(Credential).where(Credential.deployment_id == deployment.id)
+        )
+        credentials_list = credentials_result.scalars().all()
 
-    logger.info(f"Deployment {deployment.id} MCP config: {mcp_config}")
-    logger.info(f"Extracted package: {package}")
+        # Prepare environment variables from credentials
+        env_vars = {}
+        for cred in credentials_list:
+            # Decrypt the credential value
+            decrypted_value = encryption_service.decrypt(cred.encrypted_data)
+            # Strip 'env_' prefix from service_name to get actual env var name
+            # e.g., 'env_TICKTICK_CLIENT_ID' -> 'TICKTICK_CLIENT_ID'
+            env_var_name = cred.service_name.removeprefix("env_")
+            env_vars[env_var_name] = decrypted_value
+
+        logger.info(f"Starting MCP server for deployment {deployment.id}: {package}")
 
-    if package:
-        try:
-            # Query credentials separately to avoid greenlet issues
-            credentials_result = await db.execute(
-                select(Credential).where(Credential.deployment_id == deployment.id)
+        # CHECK FOR FLY.IO DEPLOYMENT FIRST
+        if settings.FLY_API_TOKEN:
+            from app.services.fly_deployment_service import FlyDeploymentService
+            fly_service = FlyDeploymentService()
+
+            logger.info("FLY_API_TOKEN found, attempting to create Fly.io machine...")
+            machine_id = await fly_service.create_machine(
+                deployment_id=str(deployment.id),
+                mcp_config=mcp_config,
+                credentials=env_vars
             )
-            credentials_list = credentials_result.scalars().all()
-
-            # Prepare environment variables from credentials
-            env_vars = {}
-            for cred in credentials_list:
-                # Decrypt the credential value
-                decrypted_value = encryption_service.decrypt(cred.encrypted_data)
-                # Strip 'env_' prefix from service_name to get actual env var name
-                # e.g., 'env_TICKTICK_CLIENT_ID' -> 'TICKTICK_CLIENT_ID'
-                env_var_name = cred.service_name.removeprefix("env_")
-                env_vars[env_var_name] = decrypted_value
-
-            logger.info(f"Starting MCP server for deployment {deployment.id}: {package}")
-
-            # CHECK FOR FLY.IO DEPLOYMENT FIRST
-            if settings.FLY_API_TOKEN:
-                from app.services.fly_deployment_service import FlyDeploymentService
-                fly_service = FlyDeploymentService()
-                
-                logger.info("FLY_API_TOKEN found, attempting to create Fly.io machine...")
-                machine_id = await fly_service.create_machine(
-                    deployment_id=str(deployment.id),
-                    mcp_config=mcp_config,
-                    credentials=env_vars
-                )
-                
-                if machine_id:
-                    deployment.machine_id = machine_id
-                    deployment.status = "running"
-                    await db.commit()
-                    logger.info(f"Fly machine {machine_id} started successfully")
-                else:
-                    # Should be unreachable if create_machine raises exception on failure
-                    deployment.status = "failed"
-                    deployment.error_message = "Unknown error: Machine ID not returned"
-                    await db.commit()
-                    
+
+            if machine_id:
+                deployment.machine_id = machine_id
+                deployment.status = "running"
+                await db.commit()
+                logger.info(f"Fly machine {machine_id} started successfully")
             else:
-                # FALLBACK TO LOCAL SUBPROCESS (Dev/WSL2 only)
-                logger.info("No FLY_API_TOKEN, falling back to local subprocess manager")
-                success = await start_server(
-                    deployment_id=str(deployment.id),
-                    package=package,
-                    env_vars=env_vars
-                )
-
-                if not success:
-                    deployment.status = "failed"
-                    deployment.error_message = "Failed to start local subprocess"
-                    await db.commit()
-                    logger.error(f"Failed to start local MCP server for deployment {deployment.id}")
-
-        except Exception as e:
-            logger.exception(f"Deployment failed for {deployment.id}")
-            deployment.status = "failed"
-            deployment.error_message = str(e)
-            await db.commit()
-    else:
+                # Should be unreachable if create_machine raises exception on failure
+                deployment.status = "failed"
+                deployment.error_message = "Unknown error: Machine ID not returned"
+                await db.commit()
+
+        else:
+            # FALLBACK TO LOCAL SUBPROCESS (Dev/WSL2 only)
+            logger.info("No FLY_API_TOKEN, falling back to local subprocess manager")
+            success = await start_server(
+                deployment_id=str(deployment.id),
+                package=package,
+                env_vars=env_vars
+            )
+
+            if not success:
+                deployment.status = "failed"
+                deployment.error_message = "Failed to start local subprocess"
+                await db.commit()
+                logger.error(f"Failed to start local MCP server for deployment {deployment.id}")
+
+    except Exception as e:
+        logger.exception(f"Deployment failed for {deployment.id}")
         deployment.status = "failed"
-        deployment.error_message = "No 'package' specified in MCP config"
+        deployment.error_message = str(e)
         await db.commit()
-        logger.warning(f"No package configured for deployment {deployment.id}, skipping server start")
 
     # 4. Construct Response
     # Base URL + /api/mcp/{id} (Streamable HTTP unified endpoint)
diff --git a/backend/app/api/forms.py b/backend/app/api/forms.py
index bc49aa9..e933868 100644
--- a/backend/app/api/forms.py
+++ b/backend/app/api/forms.py
@@ -1,17 +1,21 @@
-from fastapi import APIRouter, HTTPException, Depends, Query
-from typing import Literal, Optional
-from app.schemas.dynamic_form import FormSchema, FormField
-from app.core.form_schemas import FORM_SCHEMAS
-from app.services.cache import CacheService
 import json
+import logging
+import re
+from typing import Optional
 
-# Create a new APIRouter instance for form-related endpoints
-router = APIRouter()
+from fastapi import APIRouter, Depends, HTTPException, Query
+from sqlalchemy.ext.asyncio import AsyncSession
 
+from app.core.form_schemas import FORM_SCHEMAS
+from app.db.session import get_db
+from app.schemas.dynamic_form import FormField, FormSchema
 from app.services.analysis import AnalysisService
+from app.services.cache import CacheService
+from app.services.registry_service import RegistryService
 
-from app.db.session import get_db
-from sqlalchemy.ext.asyncio import AsyncSession
+# Create a new APIRouter instance for form-related endpoints
+router = APIRouter()
+logger = logging.getLogger(__name__)
 
 def get_cache_service(db: AsyncSession = Depends(get_db)):
     return CacheService(db)
@@ -19,6 +23,9 @@ def get_cache_service(db: AsyncSession = Depends(get_db)):
 def get_analysis_service():
     return AnalysisService()
 
+def get_registry_service():
+    return RegistryService.get_instance()
+
 @router.get("/generate/{service_type}", response_model=FormSchema)
 async def get_form_schema(
     service_type: str,
@@ -36,7 +43,7 @@ async def get_form_schema(
         
         if not analysis:
             # Fallback: Run analysis on the fly if cache is missing
-            print(f"Cache miss for {repo_url}. Running analysis on the fly...")
+            logger.info(f"Cache miss for {repo_url}. Running analysis on the fly...")
             analysis = await analysis_service.analyze_repo(repo_url)
             
             # Use 'error' check from analyze_repo return format
@@ -71,11 +78,14 @@ async def get_form_schema(
                            # Try to parse just the valid part up to the error
                            try:
                                data = json.loads(json_str[:e.pos])
-                           except Exception:
-                               print(f"Failed to recover JSON from extra data error: {e}")
+                           except Exception as e_inner:
+                               logger.warning(
+                                   "Failed to recover JSON from extra data error: %s",
+                                   e_inner
+                               )
                                data = {}
                        else:
-                           print(f"JSON decode error: {e}")
+                           logger.warning(f"JSON decode error: {e}")
                            data = {}
                 else:
                    data = {} # Fallback
@@ -86,7 +96,15 @@ async def get_form_schema(
             fields = []
             
             # Common fields
-            fields.append(FormField(name="name", label="Deployment Name", type="text", required=True, default=data.get("name", "New Deployment")))
+            fields.append(
+                FormField(
+                    name="name",
+                    label="Deployment Name",
+                    type="text",
+                    required=True,
+                    default=data.get("name", "New Deployment"),
+                )
+            )
              
             # Environment Variables
             env_vars = data.get("env_vars", [])
@@ -94,13 +112,30 @@ async def get_form_schema(
                 # Extract key logic (e.g. if it's a dict or str)
                 key = env if isinstance(env, str) else env.get("name")
                 # Heuristic for type
-                ftype = "password" if "KEY" in key.upper() or "SECRET" in key.upper() or "TOKEN" in key.upper() else "text"
-                fields.append(FormField(name=f"env_{key}", label=key, type=ftype, required=True, description=f"Environment variable for {key}"))
+                is_secret = any(
+                    token in key.upper()
+                    for token in ("KEY", "SECRET", "TOKEN")
+                )
+                ftype = "password" if is_secret else "text"
+                fields.append(
+                    FormField(
+                        name=f"env_{key}",
+                        label=key,
+                        type=ftype,
+                        required=True,
+                        description=f"Environment variable for {key}",
+                    )
+                )
 
             # Extract MCP server configuration (tools, resources, prompts, package)
-            # This will be passed to the frontend and included in schedule_config when creating deployment
+            # This will be passed to the frontend and included in schedule_config
+            # when creating deployment.
             mcp_config = {
-                "package": data.get("package") or data.get("npm_package") or data.get("package_name"),
+                "package": (
+                    data.get("package")
+                    or data.get("npm_package")
+                    or data.get("package_name")
+                ),
                 "tools": data.get("tools", []),
                 "resources": data.get("resources", []),
                 "prompts": data.get("prompts", []),
@@ -113,15 +148,22 @@ async def get_form_schema(
 
             return FormSchema(
                 title=f"Configure {data.get('name', 'Deployment')}",
-                description=data.get("description", "Enter the required configuration."),
+                description=data.get(
+                    "description",
+                    "Enter the required configuration.",
+                ),
                 fields=fields,
                 mcp_config=mcp_config
             )
             
         except Exception as e:
-            print(f"Error generating form: {e}")
+            logger.error(f"Error generating form: {e}", exc_info=True)
             # Fallback
-            return FormSchema(title="Configuration Error", description="Failed to generate form.", fields=[])
+            return FormSchema(
+                title="Configuration Error",
+                description="Failed to generate form.",
+                fields=[],
+            )
 
     # 2. Handle Static Schemas
     # Retrieve the schema from the centralized configuration
@@ -130,8 +172,144 @@ async def get_form_schema(
     if not schema:
          # If requesting 'custom' but no repo_url, or unknown type
          if service_type == "custom":
-             return FormSchema(title="Missing Repository", description="No repository URL provided.", fields=[])
+             return FormSchema(
+                 title="Missing Repository",
+                 description="No repository URL provided.",
+                 fields=[],
+             )
          
          raise HTTPException(status_code=404, detail="Service schema not found")
-        
+
     return schema
+
+@router.get("/generate/registry/{registry_id:path}", response_model=FormSchema)
+async def get_registry_form_schema(
+    registry_id: str,
+    registry_service: RegistryService = Depends(get_registry_service)
+):
+    """
+    Generate form schema directly from Glama registry API data (no LLM analysis).
+
+    Glama provides environmentVariablesJsonSchema for form generation, eliminating
+    the need to run expensive repository analysis for registry deployments.
+
+    Args:
+        registry_id: Full registry ID (e.g., "ai.exa/exa")
+
+    Returns:
+        FormSchema with fields for environment variables and mcp_config
+
+    Raises:
+        HTTPException 404: Registry server not found
+        HTTPException 400: Server not deployable (no packages)
+        HTTPException 500: Failed to parse registry data
+    """
+    try:
+        # Fetch server from Glama cache
+        server = await registry_service.get_server(registry_id)
+        if not server:
+            raise HTTPException(
+                status_code=404,
+                detail=f"Server '{registry_id}' not found in Glama registry",
+            )
+
+        # Verify server is deployable
+        if not server.capabilities.deployable:
+            raise HTTPException(
+                status_code=400,
+                detail=(
+                    f"Server '{registry_id}' is local-only and cannot be deployed "
+                    f"to cloud. "
+                    f"Please use a remote-capable server."
+                )
+            )
+
+        # Extract form data from Glama JSON Schema
+        form_data = registry_service.extract_form_data(server)
+
+        # Build form fields
+        fields = []
+
+        # Deployment name field (always required)
+        fields.append(
+            FormField(
+                name="name",
+                label="Deployment Name",
+                type="text",
+                required=True,
+                default=form_data["name"],
+                description="A friendly name for this deployment"
+            )
+        )
+
+        # Environment variable fields
+        for env_var in form_data["env_vars"]:
+            field_type = "text"
+            if env_var.get("options"):
+                field_type = "select"
+            elif env_var.get("format") in ("boolean", "bool"):
+                field_type = "checkbox"
+            elif env_var.get("format") in ("number", "integer", "int"):
+                field_type = "number"
+            elif env_var.get("secret"):
+                field_type = "password"
+
+            # Sanitize env var name to prevent invalid form characters
+            raw_name = env_var["name"]
+            sanitized_name = re.sub(r"[^A-Za-z0-9_]", "_", raw_name)
+
+            fields.append(
+                FormField(
+                    name=f"env_{sanitized_name}",
+                    label=raw_name,
+                    type=field_type,
+                    required=env_var["required"],
+                    default=env_var.get("default"),
+                    options=env_var.get("options"),
+                    description=env_var["description"]
+                    or f"Environment variable: {raw_name}",
+                )
+            )
+
+        raw_server = registry_service.get_raw_server(registry_id) or {}
+
+        # Build mcp_config (Glama can provide richer metadata than the legacy registry)
+        mcp_config = {
+            "package": form_data["package"],
+            "tools": raw_server.get("tools", []),
+            "resources": raw_server.get("resources", []),
+            "prompts": raw_server.get("prompts", []),
+            "server_info": {
+                "name": form_data["name"],
+                "version": form_data["version"],
+                "description": form_data["description"],
+                "source": "glama",
+                "registry_id": registry_id,
+                "glama_id": registry_id,
+                "glama_url": raw_server.get("url", ""),
+                "license": (raw_server.get("spdxLicense", {}) or {}).get("name", ""),
+                "is_official": False,
+            }
+        }
+
+        return FormSchema(
+            title=f"Configure {form_data['name']}",
+            description=form_data["description"]
+            or f"Configure environment variables for {form_data['name']}",
+            fields=fields,
+            mcp_config=mcp_config
+        )
+
+    except HTTPException:
+        # Re-raise HTTP exceptions as-is
+        raise
+    except ValueError as e:
+        # Handle extraction errors (missing raw server payload, etc.)
+        raise HTTPException(status_code=400, detail=str(e))
+    except Exception as e:
+        # Log unexpected errors and return 500
+        logger.error(f"Error generating registry form for {registry_id}: {e}", exc_info=True)
+        raise HTTPException(
+            status_code=500,
+            detail=f"Failed to generate form schema: {str(e)}"
+        )
diff --git a/backend/app/api/registry.py b/backend/app/api/registry.py
index 42e2135..50be582 100644
--- a/backend/app/api/registry.py
+++ b/backend/app/api/registry.py
@@ -1,7 +1,9 @@
-from fastapi import APIRouter, Depends, HTTPException
 from typing import List, Optional
+
+from fastapi import APIRouter, Depends, HTTPException
+
+from app.schemas.registry import RegistrySearchParams, RegistryServer
 from app.services.registry_service import RegistryService
-from app.schemas.registry import RegistryServer, RegistrySearchParams
 
 router = APIRouter()
 
@@ -15,7 +17,7 @@ async def search_registry(
     offset: int = 0,
     service: RegistryService = Depends(get_registry_service)
 ):
-    """Search for MCP servers in the official registry."""
+    """Search for MCP servers in the Glama MCP registry."""
     params = RegistrySearchParams(query=q, limit=limit, offset=offset)
     return await service.search_servers(params)
 
@@ -27,5 +29,8 @@ async def get_server(
     """Get details for a specific MCP server by ID (e.g. 'ai.exa/exa')."""
     server = await service.get_server(server_id)
     if not server:
-        raise HTTPException(status_code=404, detail=f"Server '{server_id}' not found in registry")
+        raise HTTPException(
+            status_code=404,
+            detail=f"Server '{server_id}' not found in Glama registry",
+        )
     return server
diff --git a/backend/app/services/__pycache__/analysis.cpython-313.pyc b/backend/app/services/__pycache__/analysis.cpython-313.pyc
index 527663e..b850a87 100644
Binary files a/backend/app/services/__pycache__/analysis.cpython-313.pyc and b/backend/app/services/__pycache__/analysis.cpython-313.pyc differ
diff --git a/backend/app/services/__pycache__/cache.cpython-313.pyc b/backend/app/services/__pycache__/cache.cpython-313.pyc
index 09c2726..0d3d30d 100644
Binary files a/backend/app/services/__pycache__/cache.cpython-313.pyc and b/backend/app/services/__pycache__/cache.cpython-313.pyc differ
diff --git a/backend/app/services/analysis.py b/backend/app/services/analysis.py
index e2fe070..4d1b872 100644
--- a/backend/app/services/analysis.py
+++ b/backend/app/services/analysis.py
@@ -1,9 +1,14 @@
+import json
+import logging
 import os
+import re
 from typing import Optional, Dict, Any
 from openai import AsyncOpenAI
 
 from app.core.config import settings
 
+logger = logging.getLogger(__name__)
+
 # Service responsible for analyzing GitHub repositories using Claude via OpenRouter
 class AnalysisService:
     # Initialize the service with the OpenRouter client
@@ -12,7 +17,7 @@ def __init__(self) -> None:
         api_key: Optional[str] = settings.OPENROUTER_API_KEY
         if not api_key:
             # Handle missing API key case (could raise error or log warning)
-            print("Warning: OPENROUTER_API_KEY not found in environment variables.")
+            logger.warning("OPENROUTER_API_KEY not found in environment variables.")
         
         # Initialize the AsyncOpenAI client pointing to OpenRouter
         self.client: AsyncOpenAI = AsyncOpenAI(
@@ -22,7 +27,7 @@ def __init__(self) -> None:
         
         # Define the model to use (Claude Haiku 4.5)
         self.model: str = "anthropic/claude-haiku-4.5"
-### USER RULE : NEVER EVER CHANGE THE MODEL FROM CLAUDE HAIKU 4.5 UNLESS SPECIFICALLY INSTRUCTED TO DO SO
+    ### USER RULE : NEVER EVER CHANGE THE MODEL FROM CLAUDE HAIKU 4.5 UNLESS SPECIFICALLY INSTRUCTED TO DO SO
     # Analyze a GitHub repository to extract MCP configuration
     async def analyze_repo(self, repo_url: str) -> Dict[str, Any]:
         """
@@ -64,12 +69,23 @@ async def analyze_repo(self, repo_url: str) -> Dict[str, Any]:
             # Extract the content from the response
             content = response.choices[0].message.content
             
-            # TODO: Add robust JSON parsing/cleaning here if the model adds markdown ticks
-            # For now, return the raw content or parsed dict if possible
-            # We assume the model follows the instruction to output JSON.
-            return {"raw_analysis": content}
+            # Clean and parse the JSON content
+            cleaned_content = content.strip()
+            # Remove markdown code blocks if present
+            if "```" in cleaned_content:
+                cleaned_content = re.sub(r"^```json\s*", "", cleaned_content, flags=re.MULTILINE)
+                cleaned_content = re.sub(r"^```\s*", "", cleaned_content, flags=re.MULTILINE)
+                cleaned_content = re.sub(r"```$", "", cleaned_content, flags=re.MULTILINE).strip()
+            
+            try:
+                parsed_data = json.loads(cleaned_content)
+                return parsed_data
+            except json.JSONDecodeError:
+                # Fallback or error if parsing fails
+                logger.error("Failed to parse JSON analysis: %s", cleaned_content)
+                return {"error": "Failed to parse analysis results", "raw": content}
             
         except Exception as e:
             # Log the error (in a real app) and re-raise or return error status
-            print(f"Error analyzing repo {repo_url}: {str(e)}")
+            logger.error(f"Error analyzing repo {repo_url}: {str(e)}")
             return {"error": str(e), "status": "failed"}
diff --git a/backend/app/services/credential_validator.py b/backend/app/services/credential_validator.py
new file mode 100644
index 0000000..f6d53fe
--- /dev/null
+++ b/backend/app/services/credential_validator.py
@@ -0,0 +1,131 @@
+"""
+Credential Validator Service
+
+Validates that all required credentials are provided before deployment.
+Checks that required environment variables are present and non-empty.
+"""
+
+from typing import Dict, List, Any
+import logging
+
+logger = logging.getLogger(__name__)
+
+
+class CredentialValidator:
+    """Service for validating deployment credentials"""
+
+    def validate_credentials(
+        self,
+        provided_credentials: Dict[str, str],
+        required_env_vars: List[Dict[str, Any]]
+    ) -> Dict[str, Any]:
+        """
+        Validate that all required credentials are provided and non-empty.
+
+        Args:
+            provided_credentials: Dict of credential key-value pairs
+                Example: {"env_TICKTICK_TOKEN": "abc123", "env_API_KEY": "xyz"}
+            required_env_vars: List of env var definitions from mcp_config
+                Example: [
+                    {"name": "TICKTICK_TOKEN", "required": true},
+                    {"name": "API_KEY", "required": false}
+                ]
+
+        Returns:
+            Dict with validation result:
+            {
+                "valid": bool,
+                "errors": List[str]  # List of error messages for missing/invalid credentials
+            }
+
+        Note:
+            - Frontend sends credentials with 'env_' prefix (e.g., 'env_TICKTICK_TOKEN')
+            - MCP config defines env vars without prefix (e.g., 'TICKTICK_TOKEN')
+            - This method handles the prefix stripping automatically
+        """
+        errors: List[str] = []
+
+        logger.info(f"Validating credentials: {len(provided_credentials)} provided, "
+                   f"{len(required_env_vars)} env vars defined")
+
+        # Filter to only required env vars (ignore optional ones)
+        required_only = [
+            env_var for env_var in required_env_vars
+            if env_var.get("required", False)
+        ]
+
+        logger.debug(f"Required credentials: {[v.get('name') for v in required_only]}")
+
+        # Check each required env var
+        for env_var in required_only:
+            var_name = env_var.get("name")
+
+            if not var_name:
+                # Skip env vars without a name (malformed config)
+                logger.warning(f"Skipping env var without name: {env_var}")
+                continue
+
+            # Check if credential is provided
+            # Try both with and without 'env_' prefix to handle different formats
+            credential_key_with_prefix = f"env_{var_name}"
+            credential_key_without_prefix = var_name
+
+            credential_value = (
+                provided_credentials.get(credential_key_with_prefix) or
+                provided_credentials.get(credential_key_without_prefix)
+            )
+
+            # Validate credential exists and is not empty
+            if credential_value is None:
+                error_msg = f"Required credential '{var_name}' is missing"
+                errors.append(error_msg)
+                logger.warning(error_msg)
+
+            elif isinstance(credential_value, str) and not credential_value.strip():
+                # Credential exists but is empty string or whitespace
+                error_msg = f"Required credential '{var_name}' is empty"
+                errors.append(error_msg)
+                logger.warning(error_msg)
+
+            else:
+                # Credential is valid
+                logger.debug(f"Credential '{var_name}' is valid")
+
+        # Determine overall validation result
+        is_valid = len(errors) == 0
+
+        if is_valid:
+            logger.info("Credential validation passed: all required credentials provided")
+        else:
+            logger.warning(f"Credential validation failed: {len(errors)} error(s)")
+            for error in errors:
+                logger.warning(f"  - {error}")
+
+        return {
+            "valid": is_valid,
+            "errors": errors
+        }
+
+    def validate_credentials_simple(
+        self,
+        provided_credentials: Dict[str, str],
+        required_var_names: List[str]
+    ) -> Dict[str, Any]:
+        """
+        Simplified validation when you only have a list of required variable names.
+
+        Args:
+            provided_credentials: Dict of credential key-value pairs
+            required_var_names: List of required env var names (strings)
+                Example: ["TICKTICK_TOKEN", "API_KEY"]
+
+        Returns:
+            Same format as validate_credentials()
+        """
+        # Convert simple list to the full format expected by validate_credentials
+        required_env_vars = [
+            {"name": var_name, "required": True}
+            for var_name in required_var_names
+        ]
+
+        return self.validate_credentials(provided_credentials, required_env_vars)
diff --git a/backend/app/services/package_validator.py b/backend/app/services/package_validator.py
new file mode 100644
index 0000000..bb24d70
--- /dev/null
+++ b/backend/app/services/package_validator.py
@@ -0,0 +1,230 @@
+"""
+Package Validator Service
+
+Validates that npm and Python packages exist in their respective registries
+before attempting deployment. This prevents deployment failures due to
+non-existent or misspelled package names.
+"""
+
+import httpx
+from typing import Dict, Any
+import logging
+
+logger = logging.getLogger(__name__)
+
+
+class PackageValidator:
+    """Service for validating package existence in npm and PyPI registries"""
+
+    def __init__(self):
+        # Initialize with reasonable timeout for registry checks
+        # 5 seconds should be enough for registry API calls
+        self.timeout = 5.0
+
+    async def validate_npm_package(self, package: str) -> Dict[str, Any]:
+        """
+        Check if an npm package exists in the npm registry.
+
+        Args:
+            package: Package name (e.g., '@user/package' or 'package-name')
+
+        Returns:
+            Dict with validation result:
+            {
+                "valid": bool,
+                "error": str | None,
+                "version": str | None
+            }
+        """
+        # Validate package name is not empty
+        if not package or not package.strip():
+            error_msg = "Package name cannot be empty"
+            logger.warning(error_msg)
+            return {
+                "valid": False,
+                "error": error_msg,
+                "version": None
+            }
+
+        # URL encode scoped packages: @user/package → @user%2Fpackage
+        # This is required for npm registry API URLs
+        encoded_package = package.replace("/", "%2F")
+        url = f"https://registry.npmjs.org/{encoded_package}"
+
+        logger.info(f"Validating npm package: {package}")
+        logger.debug(f"npm registry URL: {url}")
+
+        try:
+            async with httpx.AsyncClient(timeout=self.timeout) as client:
+                response = await client.get(url)
+
+                # Package found - extract version from dist-tags.latest
+                if response.status_code == 200:
+                    data = response.json()
+                    version = data.get("dist-tags", {}).get("latest")
+
+                    logger.info(f"npm package '{package}' found, version: {version}")
+
+                    return {
+                        "valid": True,
+                        "error": None,
+                        "version": version
+                    }
+
+                # Package not found in registry
+                elif response.status_code == 404:
+                    error_msg = f"Package '{package}' not found in npm registry"
+                    logger.warning(error_msg)
+
+                    return {
+                        "valid": False,
+                        "error": error_msg,
+                        "version": None
+                    }
+
+                # Unexpected registry response
+                else:
+                    error_msg = f"npm registry error: HTTP {response.status_code}"
+                    logger.error(f"{error_msg} for package '{package}'")
+
+                    return {
+                        "valid": False,
+                        "error": error_msg,
+                        "version": None
+                    }
+
+        except httpx.TimeoutException:
+            # Network timeout - likely slow connection or registry issues
+            error_msg = f"Timeout while validating npm package '{package}' (exceeded {self.timeout}s)"
+            logger.error(error_msg)
+
+            return {
+                "valid": False,
+                "error": "Package validation timed out. Please try again.",
+                "version": None
+            }
+
+        except httpx.RequestError as e:
+            # Network error (connection failed, DNS resolution, etc.)
+            error_msg = f"Network error while validating npm package '{package}': {str(e)}"
+            logger.error(error_msg)
+
+            return {
+                "valid": False,
+                "error": "Failed to connect to npm registry. Check your internet connection.",
+                "version": None
+            }
+
+        except Exception as e:
+            # Unexpected error (malformed response, JSON parsing, etc.)
+            error_msg = f"Unexpected error validating npm package '{package}': {str(e)}"
+            logger.exception(error_msg)
+
+            return {
+                "valid": False,
+                "error": f"Failed to validate package: {str(e)}",
+                "version": None
+            }
+
+    async def validate_python_package(self, package: str) -> Dict[str, Any]:
+        """
+        Check if a Python package exists in the PyPI registry.
+
+        Args:
+            package: Package name (e.g., 'mcp-server-git' or 'requests')
+
+        Returns:
+            Dict with validation result:
+            {
+                "valid": bool,
+                "error": str | None,
+                "version": str | None
+            }
+        """
+        # Validate package name is not empty
+        if not package or not package.strip():
+            error_msg = "Package name cannot be empty"
+            logger.warning(error_msg)
+            return {
+                "valid": False,
+                "error": error_msg,
+                "version": None
+            }
+
+        # PyPI JSON API endpoint
+        url = f"https://pypi.org/pypi/{package}/json"
+
+        logger.info(f"Validating Python package: {package}")
+        logger.debug(f"PyPI API URL: {url}")
+
+        try:
+            async with httpx.AsyncClient(timeout=self.timeout) as client:
+                response = await client.get(url)
+
+                # Package found - extract version from info.version
+                if response.status_code == 200:
+                    data = response.json()
+                    version = data.get("info", {}).get("version")
+
+                    logger.info(f"Python package '{package}' found, version: {version}")
+
+                    return {
+                        "valid": True,
+                        "error": None,
+                        "version": version
+                    }
+
+                # Package not found in registry
+                elif response.status_code == 404:
+                    error_msg = f"Package '{package}' not found in PyPI registry"
+                    logger.warning(error_msg)
+
+                    return {
+                        "valid": False,
+                        "error": error_msg,
+                        "version": None
+                    }
+
+                # Unexpected registry response
+                else:
+                    error_msg = f"PyPI registry error: HTTP {response.status_code}"
+                    logger.error(f"{error_msg} for package '{package}'")
+
+                    return {
+                        "valid": False,
+                        "error": error_msg,
+                        "version": None
+                    }
+
+        except httpx.TimeoutException:
+            # Network timeout - likely slow connection or registry issues
+            error_msg = f"Timeout while validating Python package '{package}' (exceeded {self.timeout}s)"
+            logger.error(error_msg)
+
+            return {
+                "valid": False,
+                "error": "Package validation timed out. Please try again.",
+                "version": None
+            }
+
+        except httpx.RequestError as e:
+            # Network error (connection failed, DNS resolution, etc.)
+            error_msg = f"Network error while validating Python package '{package}': {str(e)}"
+            logger.error(error_msg)
+
+            return {
+                "valid": False,
+                "error": "Failed to connect to PyPI registry. Check your internet connection.",
+                "version": None
+            }
+
+        except Exception as e:
+            # Unexpected error (malformed response, JSON parsing, etc.)
+            error_msg = f"Unexpected error validating Python package '{package}': {str(e)}"
+            logger.exception(error_msg)
+
+            return {
+                "valid": False,
+                "error": f"Failed to validate package: {str(e)}",
+                "version": None
+            }
diff --git a/backend/app/services/registry_service.py b/backend/app/services/registry_service.py
index 869f551..a465c16 100644
--- a/backend/app/services/registry_service.py
+++ b/backend/app/services/registry_service.py
@@ -1,23 +1,54 @@
-import logging
-import httpx
 import asyncio
+import logging
+import re
 from datetime import datetime, timedelta
-from typing import List, Optional, Dict
-from app.schemas.registry import RegistryServer, RegistrySearchParams, RegistryServerCapabilities, RegistryServerTrust
+from typing import Any, Dict, List, Optional
+from xml.etree import ElementTree
+
+import httpx
+
+from app.schemas.registry import (
+    RegistrySearchParams,
+    RegistryServer,
+    RegistryServerCapabilities,
+    RegistryServerTrust,
+)
 
 logger = logging.getLogger(__name__)
 
-REGISTRY_API_URL = "https://registry.modelcontextprotocol.io/v0/servers"
+GLAMA_API_URL = "https://glama.ai/api/mcp/v1/servers"
+GLAMA_SERVERS_SITEMAP_URL = "https://glama.ai/sitemaps/mcp-servers.xml"
+
+# Service Constants
+DEFAULT_TIMEOUT_SECONDS = 10.0
+MAX_PAGES_TO_FETCH = 25
+BATCH_SIZE = 20
+GLAMA_ITEMS_PER_PAGE = 100
+MAX_SEARCH_LIMIT = 101
+CACHE_TTL_HOURS = 1
+SITEMAP_TTL_HOURS = 12
+
+_SITEMAP_XMLNS = "http://www.sitemaps.org/schemas/sitemap/0.9"
+_SITEMAP_LOC_TAG = f"{{{_SITEMAP_XMLNS}}}loc"
+_SITEMAP_SERVER_RE = re.compile(
+    r"^https?://glama\.ai/mcp/servers/@(?P<namespace>[^/]+)/(?P<slug>[^/?#]+)$"
+)
 
 class RegistryService:
     _instance = None
     _lock = asyncio.Lock()
-    
+
     # Instance level cache
     def __init__(self):
         self._cache: Dict[str, RegistryServer] = {}
+        self._raw_cache: Dict[str, Dict] = {}  # Store raw registry API data
         self._last_updated: Optional[datetime] = None
-        self._cache_ttl = timedelta(hours=1)
+        self._cache_ttl = timedelta(hours=CACHE_TTL_HOURS)
+
+        # Full directory index (12k+) via Glama sitemap for reliable lookup.
+        self._sitemap_ids: List[str] = []
+        self._sitemap_last_updated: Optional[datetime] = None
+        self._sitemap_ttl = timedelta(hours=SITEMAP_TTL_HOURS)
 
     @classmethod
     def get_instance(cls):
@@ -28,37 +59,102 @@ def get_instance(cls):
     async def get_servers(self, force_refresh: bool = False) -> List[RegistryServer]:
         """Get all servers from the registry, using cache if valid."""
         if not force_refresh and self._is_cache_valid():
+            # Read from cache with lock? dict reads are atomic in python,
+            # but for consistency we can trust the atomic swap pattern used below.
             return list(self._cache.values())
         
         await self._fetch_and_cache_registry()
         return list(self._cache.values())
 
-    async def search_servers(self, params: RegistrySearchParams) -> List[RegistryServer]:
-        """Search servers with simple text matching."""
-        servers = await self.get_servers()
-        
-        # Simple local search (scalability concern noted: move to DB in future)
-        filtered = servers
-        if params.query:
-            q = params.query.lower()
-            filtered = [
-                s for s in servers 
-                if q in s.name.lower() or 
-                   q in s.description.lower() or 
-                   q in s.namespace.lower() or
-                   (s.install_ref and q in s.install_ref.lower())
-            ]
-            
-        # Pagination
-        start = params.offset
-        end = params.offset + params.limit
-        return filtered[start:end]
+    async def search_servers(
+        self,
+        params: RegistrySearchParams,
+    ) -> List[RegistryServer]:
+        """
+        Search Glama servers.
+
+        Important: Glama's `GET /v1/servers` "browse" response is not a complete
+        listing of the full directory; the `query` parameter must be used to
+        search across the broader catalog.
+        """
+        query = (params.query or "").strip()
+
+        # For the default "browse" feed, use our cached list (fast path).
+        if not query:
+            servers = await self.get_servers()
+            deployable = [s for s in servers if s.capabilities.deployable]
+            deployable = self._disambiguate_display_names(deployable)
+            return deployable[params.offset : params.offset + params.limit]
+
+        # If the user enters a direct ID like "namespace/slug" (or "@namespace/slug"),
+        # short-circuit to a detail lookup so we can find servers outside the
+        # limited search result window.
+        direct_id = self._parse_direct_server_id(query)
+        if direct_id:
+            server = await self.get_server(direct_id)
+            if server and server.capabilities.deployable:
+                return self._disambiguate_display_names([server])
+            return []
+
+        target_count = max(0, params.offset) + max(0, params.limit)
+        if target_count == 0:
+            return []
+
+        collected: List[RegistryServer] = []
+        seen_ids: set[str] = set()
+
+        # 1) High-precision: match against the full directory of namespace/slug
+        # via the public sitemap, then hydrate via the detail endpoint.
+        sitemap_results = await self._search_via_sitemap_ids(
+            query=query,
+            seen_ids=seen_ids,
+            needed=target_count,
+        )
+        collected.extend(sitemap_results)
+
+        # 2) Fuzzy: supplement with Glama API search (can match name/description),
+        # but filter out obvious non-matches to avoid returning irrelevant results
+        # (the API often returns generic "mcp" hits).
+        if len(collected) < target_count:
+            glama_results = await self._search_glama(
+                query=query,
+                offset=0,
+                limit=min(MAX_SEARCH_LIMIT, target_count * 2),
+            )
+            q_lower = query.lower()
+            for server in glama_results:
+                if server.id in seen_ids:
+                    continue
+                if not self._server_matches_query(server, q_lower):
+                    continue
+                seen_ids.add(server.id)
+                collected.append(server)
+                if len(collected) >= target_count:
+                    break
+
+        sliced = collected[params.offset : target_count]
+        return self._disambiguate_display_names(sliced)
 
     async def get_server(self, server_id: str) -> Optional[RegistryServer]:
         """Get a single server by its ID."""
-        if self._is_cache_valid() and server_id in self._cache:
+        if server_id in self._cache:
             return self._cache[server_id]
         
+        # Avoid forcing a full list refresh for a single server lookup.
+        try:
+            raw = await self._fetch_glama_server_detail(server_id)
+        except Exception as e:
+            logger.warning(f"Failed to fetch server detail for {server_id}: {e}")
+            raw = None
+
+        if raw:
+            normalized = self._normalize_glama_server(raw)
+            if normalized:
+                async with self._lock:
+                    self._cache[normalized.id] = normalized
+                    self._raw_cache[normalized.id] = raw
+                return normalized
+
         await self._fetch_and_cache_registry()
         return self._cache.get(server_id)
 
@@ -68,93 +164,515 @@ def _is_cache_valid(self) -> bool:
         return datetime.now() - self._last_updated < self._cache_ttl
 
     async def _fetch_and_cache_registry(self):
-        """Fetches from official registry with lock to prevent race conditions."""
+        """Fetches from Glama API with lock to prevent race conditions."""
         async with self._lock:
             # Double check inside lock
             if self._is_cache_valid():
                 return
 
-            logger.info("Fetching registry data from official API...")
+            logger.info("Fetching deployable registry data from Glama API...")
             try:
-                async with httpx.AsyncClient() as client:
-                    all_raw_servers = []
-                    cursor = None
-                    
-                    for _ in range(10): 
-                        params = {"limit": 100}
-                        if cursor:
-                            params["cursor"] = cursor
-                            
-                        response = await client.get(REGISTRY_API_URL, params=params)
-                        response.raise_for_status()
-                        data = response.json()
-                        
-                        if "servers" in data:
-                            all_raw_servers.extend(data["servers"])
-                        
-                        cursor = data.get("metadata", {}).get("nextCursor")
-                        if not cursor:
-                            break
+                timeout = httpx.Timeout(DEFAULT_TIMEOUT_SECONDS)
+                async with httpx.AsyncClient(timeout=timeout) as client:
+                    all_raw_servers: List[Dict[str, Any]] = []
+                    for query in ("hosting:remote-capable", "hosting:hybrid"):
+                        all_raw_servers.extend(
+                            await self._fetch_glama_list(
+                                client=client,
+                                query=query,
+                            )
+                        )
                     
                     new_cache = {}
+                    new_raw_cache = {}  # Store raw data for form generation
                     for raw in all_raw_servers:
                         try:
-                            normalized = self._normalize_server(raw)
+                            normalized = self._normalize_glama_server(raw)
                             if normalized:
+                                if not normalized.capabilities.deployable:
+                                    continue
                                 new_cache[normalized.id] = normalized
+                                new_raw_cache[normalized.id] = raw  # Store raw data
                         except Exception as e:
-                            logger.warning(f"Failed to normalize server entry: {e}")
+                            logger.warning(
+                                f"Failed to normalize Glama server entry: {e}"
+                            )
                             continue
-                    
+
                     self._cache = new_cache
+                    self._raw_cache = new_raw_cache  # Update raw cache
                     self._last_updated = datetime.now()
-                    logger.info(f"Registry cache updated with {len(new_cache)} servers.")
+                    logger.info(
+                        "Registry cache updated with "
+                        f"{len(new_cache)} servers from Glama."
+                    )
+
+                    # Disambiguate duplicate display names within a namespace.
+                    self._cache = {
+                        s.id: s
+                        for s in self._disambiguate_display_names(
+                            list(self._cache.values())
+                        )
+                    }
                     
             except Exception as e:
-                logger.error(f"Error fetching registry data: {e}", exc_info=True)
+                logger.error(f"Error fetching Glama registry data: {e}", exc_info=True)
 
-    def _normalize_server(self, raw: Dict) -> Optional[RegistryServer]:
-        inner = raw.get("server", {})
-        meta = raw.get("_meta", {})
-        
-        full_name = inner.get("name")
-        if not full_name: 
+    async def _fetch_glama_list(
+        self,
+        client: httpx.AsyncClient,
+        query: str,
+    ) -> List[Dict[str, Any]]:
+        """
+        Fetch a Glama server list for a specific query.
+
+        Glama's API currently caps list/search results to ~100 items, so this
+        helper is mainly to retrieve a reasonably sized "deployable feed".
+        """
+        all_raw_servers: List[Dict[str, Any]] = []
+        cursor: Optional[str] = None
+
+        for _ in range(MAX_PAGES_TO_FETCH):
+            params: Dict[str, Any] = {"first": GLAMA_ITEMS_PER_PAGE, "query": query}
+            if cursor:
+                params["after"] = cursor
+
+            response = await client.get(GLAMA_API_URL, params=params)
+            response.raise_for_status()
+            data = response.json()
+            payload = data.get("data", data)
+
+            servers = payload.get("servers", [])
+            if isinstance(servers, list):
+                all_raw_servers.extend([s for s in servers if isinstance(s, dict)])
+
+            page_info = payload.get("pageInfo", {}) or {}
+            if not page_info.get("hasNextPage", False):
+                break
+
+            cursor = page_info.get("endCursor")
+            if not cursor:
+                break
+
+        return all_raw_servers
+
+    async def _search_glama(
+        self,
+        query: str,
+        offset: int,
+        limit: int,
+    ) -> List[RegistryServer]:
+        """
+        Search Glama via the API.
+
+        We only fetch enough pages to satisfy offset+limit, since Glama paginates
+        via cursors and has a max `first` of 100.
+        """
+        target_count = max(0, offset) + max(0, limit)
+        if target_count == 0:
+            return []
+
+        collected: List[RegistryServer] = []
+        seen_ids: set[str] = set()
+        cursor: Optional[str] = None
+
+        timeout = httpx.Timeout(DEFAULT_TIMEOUT_SECONDS)
+        async with httpx.AsyncClient(timeout=timeout) as client:
+            for _ in range(MAX_PAGES_TO_FETCH):
+                params: Dict[str, Any] = {"first": GLAMA_ITEMS_PER_PAGE, "query": query}
+                if cursor:
+                    params["after"] = cursor
+
+                response = await client.get(GLAMA_API_URL, params=params)
+                response.raise_for_status()
+                data = response.json()
+                payload = data.get("data", data)
+
+                servers = payload.get("servers", [])
+                if isinstance(servers, list):
+                    for raw in servers:
+                        normalized = self._normalize_glama_server(raw)
+                        if not normalized:
+                            continue
+                        if not normalized.capabilities.deployable:
+                            continue
+                        if normalized.id in seen_ids:
+                            continue
+                        seen_ids.add(normalized.id)
+                        collected.append(normalized)
+                        
+                        # Cache raw/normalized for later detail/form generation.
+                        async with self._lock:
+                            self._cache[normalized.id] = normalized
+                            self._raw_cache[normalized.id] = raw
+
+                        if len(collected) >= target_count:
+                            return collected[offset:target_count]
+
+                page_info = payload.get("pageInfo", {}) or {}
+                if not page_info.get("hasNextPage", False):
+                    break
+
+                cursor = page_info.get("endCursor")
+                if not cursor:
+                    break
+
+        return collected[offset:target_count]
+
+    def _parse_direct_server_id(self, query: str) -> Optional[str]:
+        """
+        Accept direct lookups like:
+        - "namespace/slug"
+        - "@namespace/slug"
+        - "https://glama.ai/mcp/servers/@namespace/slug"
+        """
+        q = query.strip()
+        if not q:
             return None
-            
-        namespace, name_only = full_name.split("/", 1) if "/" in full_name else ("unknown", full_name)
-        
-        packages = inner.get("packages", [])
-        remotes = inner.get("remotes", [])
-        
-        is_deployable = len(packages) > 0
-        is_connectable = len(remotes) > 0
-        
-        install_ref = None
-        if is_deployable:
-            for pkg in packages:
-                if pkg.get("registryType") == "oci":
-                    install_ref = pkg.get("identifier")
+
+        match = _SITEMAP_SERVER_RE.match(q)
+        if match:
+            return f"{match.group('namespace')}/{match.group('slug')}"
+
+        if q.startswith("@"):
+            q = q[1:]
+
+        parts = q.split("/", 1)
+        if len(parts) != 2:
+            return None
+        namespace, slug = parts
+        if not namespace or not slug:
+            return None
+        if " " in namespace or " " in slug:
+            return None
+        return f"{namespace}/{slug}"
+
+    def _server_matches_query(self, server: RegistryServer, query_lower: str) -> bool:
+        if not query_lower:
+            return True
+        if query_lower in server.id.lower():
+            return True
+        if query_lower in server.name.lower():
+            return True
+        if query_lower in server.namespace.lower():
+            return True
+        if query_lower in (server.description or "").lower():
+            return True
+        if server.repository_url and query_lower in server.repository_url.lower():
+            return True
+        if server.install_ref and query_lower in server.install_ref.lower():
+            return True
+        return False
+
+    def _is_sitemap_valid(self) -> bool:
+        if not self._sitemap_last_updated:
+            return False
+        return datetime.now() - self._sitemap_last_updated < self._sitemap_ttl
+
+    async def _ensure_sitemap_index(self) -> None:
+        if self._is_sitemap_valid() and self._sitemap_ids:
+            return
+
+        async with self._lock:
+            if self._is_sitemap_valid() and self._sitemap_ids:
+                return
+
+            logger.info("Fetching Glama MCP servers sitemap...")
+            timeout = httpx.Timeout(DEFAULT_TIMEOUT_SECONDS)
+            async with httpx.AsyncClient(timeout=timeout) as client:
+                resp = await client.get(GLAMA_SERVERS_SITEMAP_URL)
+                resp.raise_for_status()
+                xml_text = resp.text
+
+            root = ElementTree.fromstring(xml_text)
+            ids: List[str] = []
+            for loc_el in root.iter(_SITEMAP_LOC_TAG):
+                if loc_el.text is None:
+                    continue
+                url = loc_el.text.strip()
+                match = _SITEMAP_SERVER_RE.match(url)
+                if not match:
+                    continue
+                namespace = match.group("namespace")
+                slug = match.group("slug")
+                ids.append(f"{namespace}/{slug}")
+
+            # Deterministic ordering for pagination.
+            ids = sorted(set(ids))
+            self._sitemap_ids = ids
+            self._sitemap_last_updated = datetime.now()
+            logger.info(f"Sitemap index updated with {len(ids)} server IDs.")
+
+    async def _search_via_sitemap_ids(
+        self,
+        query: str,
+        seen_ids: set[str],
+        needed: int,
+    ) -> List[RegistryServer]:
+        """
+        Fallback search: match the user's query against namespace/slug from the
+        public sitemap, then hydrate via the Glama detail endpoint.
+        """
+        if needed <= 0:
+            return []
+
+        await self._ensure_sitemap_index()
+        q = query.lower()
+
+        # Find candidate IDs by substring match on "namespace/slug".
+        candidates: List[str] = []
+        for server_id in self._sitemap_ids:
+            if server_id in seen_ids:
+                continue
+            if q in server_id.lower():
+                candidates.append(server_id)
+
+        # Rank: earlier match position, then shorter IDs, then lexicographic.
+        candidates.sort(
+            key=lambda sid: (
+                sid.lower().find(q),
+                len(sid),
+                sid.lower(),
+            )
+        )
+
+        collected: List[RegistryServer] = []
+        target_count = needed
+
+        timeout = httpx.Timeout(DEFAULT_TIMEOUT_SECONDS)
+        async with httpx.AsyncClient(timeout=timeout) as client:
+            for i in range(0, len(candidates), BATCH_SIZE):
+                if len(collected) >= target_count:
                     break
-        
-        official_meta = meta.get("io.modelcontextprotocol.registry/official", {})
-        is_official = official_meta.get("status") == "active"
-        
+
+                batch = candidates[i : i + BATCH_SIZE]
+                raws = await asyncio.gather(
+                    *[
+                        self._fetch_glama_server_detail_with_client(
+                            client=client,
+                            server_id=server_id,
+                        )
+                        for server_id in batch
+                    ],
+                    return_exceptions=True,
+                )
+                for raw in raws:
+                    if isinstance(raw, Exception) or not raw:
+                        continue
+
+                    normalized = self._normalize_glama_server(raw)
+                    if not normalized:
+                        continue
+                    if not normalized.capabilities.deployable:
+                        continue
+                    if normalized.id in seen_ids:
+                        continue
+
+                    seen_ids.add(normalized.id)
+                    collected.append(normalized)
+                    
+                    async with self._lock:
+                        self._cache[normalized.id] = normalized
+                        self._raw_cache[normalized.id] = raw
+
+                    if len(collected) >= target_count:
+                        break
+
+        return collected
+
+    async def _fetch_glama_server_detail(
+        self,
+        server_id: str,
+    ) -> Optional[Dict[str, Any]]:
+        async with httpx.AsyncClient() as client:
+            return await self._fetch_glama_server_detail_with_client(
+                client=client,
+                server_id=server_id,
+            )
+
+    async def _fetch_glama_server_detail_with_client(
+        self,
+        client: httpx.AsyncClient,
+        server_id: str,
+    ) -> Optional[Dict[str, Any]]:
+        parts = server_id.split("/", 1)
+        if len(parts) != 2:
+            return None
+        namespace, slug = parts
+        url = f"{GLAMA_API_URL}/{namespace}/{slug}"
+
+        response = await client.get(url)
+        if response.status_code == 404 and slug.lower() != slug:
+            # Some sitemap slugs include uppercase; try a lowercase fallback.
+            fallback_url = f"{GLAMA_API_URL}/{namespace}/{slug.lower()}"
+            response = await client.get(fallback_url)
+        if response.status_code == 404:
+            return None
+        response.raise_for_status()
+        return response.json()
+
+    def _disambiguate_display_names(
+        self,
+        servers: List[RegistryServer],
+    ) -> List[RegistryServer]:
+        """
+        If Glama returns multiple servers with the same display name within the
+        same namespace, append the slug to make cards/search results readable.
+        """
+        name_groups: Dict[tuple[str, str], List[RegistryServer]] = {}
+        for server in servers:
+            name_groups.setdefault((server.namespace, server.name), []).append(server)
+
+        for (_, base_name), group in name_groups.items():
+            if len(group) <= 1:
+                continue
+            for server in group:
+                slug = (
+                    server.id.split("/", 1)[1]
+                    if "/" in server.id
+                    else server.id
+                )
+                server.name = f"{base_name} ({slug})"
+
+        return servers
+
+    def get_raw_server(self, server_id: str) -> Optional[Dict[str, Any]]:
+        """Return the raw Glama server payload (used for schema/tool metadata)."""
+        return self._raw_cache.get(server_id)
+
+    def extract_form_data(self, server: RegistryServer) -> Dict:
+        """
+        Extract form generation data from Glama server's environmentVariablesJsonSchema.
+
+        Args:
+            server: Normalized RegistryServer object
+
+        Returns:
+            Dict with structure:
+            {
+                "name": str,
+                "description": str,
+                "package": str,  # OCI identifier
+                "version": str,
+                "env_vars": [
+                    {
+                        "name": str,
+                        "description": str,
+                        "required": bool,
+                        "secret": bool,
+                        "format": str
+                    }
+                ]
+            }
+
+        """
+        # Get raw Glama data for this server
+        raw_server = self._raw_cache.get(server.id)
+        if not raw_server:
+            raise ValueError(f"Raw data not found for server {server.id}")
+
+        env_schema = raw_server.get("environmentVariablesJsonSchema") or {}
+        properties = env_schema.get("properties") or {}
+        required_fields = set(env_schema.get("required") or [])
+
+        # Parse JSON Schema properties into simple form field descriptors.
+        env_vars = []
+        if isinstance(properties, dict):
+            for var_name, var_schema in properties.items():
+                if not var_name or not isinstance(var_schema, dict):
+                    continue
+
+                schema_type = var_schema.get("type", "string")
+                if isinstance(schema_type, list):
+                    schema_type = next(
+                        (t for t in schema_type if t != "null"),
+                        "string",
+                    )
+
+                options = var_schema.get("enum")
+                is_secret = (
+                    var_schema.get("format") == "password"
+                    or bool(var_schema.get("writeOnly"))
+                    or any(
+                        token in var_name.lower()
+                        for token in ("secret", "token", "key", "password")
+                    )
+                )
+
+                env_vars.append({
+                    "name": var_name,
+                    "description": var_schema.get("description", "") or "",
+                    "required": var_name in required_fields,
+                    "secret": is_secret,
+                    "format": schema_type,
+                    "options": options if isinstance(options, list) else None,
+                    "default": var_schema.get("default"),
+                })
+
+        return {
+            "name": server.name,
+            "description": server.description,
+            "package": self._extract_package(raw_server, server),
+            "version": server.version,
+            "env_vars": env_vars
+        }
+
+    def _extract_package(
+        self,
+        raw_server: Dict[str, Any],
+        server: RegistryServer,
+    ) -> Optional[str]:
+        package = (
+            raw_server.get("package")
+            or raw_server.get("npmPackage")
+            or raw_server.get("npm_package")
+            or raw_server.get("packageName")
+        )
+        if isinstance(package, str) and package.strip():
+            return package.strip()
+
+        repo_url = raw_server.get("repository", {}).get("url")
+        if isinstance(repo_url, str) and repo_url.strip():
+            return repo_url.strip()
+
+        return server.repository_url or server.install_ref
+
+    def _normalize_glama_server(self, raw: Dict[str, Any]) -> Optional[RegistryServer]:
+        namespace = raw.get("namespace")
+        slug = raw.get("slug")
+        if not namespace or not slug:
+            return None
+
+        full_id = f"{namespace}/{slug}"
+
+        attributes = raw.get("attributes", [])
+        if not isinstance(attributes, list):
+            attributes = []
+
+        is_remote_capable = "hosting:remote-capable" in attributes
+        is_local_only = "hosting:local-only" in attributes
+        is_hybrid = "hosting:hybrid" in attributes
+
+        repo_url = raw.get("repository", {}).get("url")
+        if not isinstance(repo_url, str):
+            repo_url = None
+
         return RegistryServer(
-            id=full_name,
-            name=name_only,
+            id=full_id,
+            name=raw.get("name") or slug,
             namespace=namespace,
-            description=inner.get("description", "") or "",
-            version=inner.get("version", "0.0.0"),
+            description=raw.get("description", "") or "",
+            version=raw.get("version") or "1.0.0",
             homepage=None,
-            repository_url=inner.get("repository", {}).get("url"),
+            repository_url=repo_url,
             
             capabilities=RegistryServerCapabilities(
-                deployable=is_deployable,
-                connectable=is_connectable
+                deployable=(is_remote_capable or is_hybrid) and not is_local_only,
+                connectable=is_local_only or is_hybrid
             ),
             trust=RegistryServerTrust(
-                is_official=is_official,
-                last_updated=official_meta.get("updatedAt", "")
+                is_official=False,
+                last_updated=""
             ),
-            install_ref=install_ref
+            install_ref=repo_url
         )
diff --git a/backend/pyproject.toml b/backend/pyproject.toml
index fcde50a..48f892c 100644
--- a/backend/pyproject.toml
+++ b/backend/pyproject.toml
@@ -8,6 +8,7 @@ dependencies = [
     "openai>=2.11.0",
     "sse-starlette>=3.0.3",
     "uvicorn[standard]",
+    "httpx>=0.27.0",
 ]
 
 [tool.pytest.ini_options]
diff --git a/backend/tests/test_registry_forms.py b/backend/tests/test_registry_forms.py
new file mode 100644
index 0000000..8d07327
--- /dev/null
+++ b/backend/tests/test_registry_forms.py
@@ -0,0 +1,362 @@
+from unittest.mock import AsyncMock, Mock
+
+import pytest
+from fastapi import HTTPException
+
+from app.api.forms import get_registry_form_schema
+from app.schemas.dynamic_form import FormSchema
+from app.schemas.registry import (
+    RegistryServer,
+    RegistryServerCapabilities,
+    RegistryServerTrust,
+)
+from app.services.registry_service import RegistryService
+
+
+@pytest.fixture
+def mock_registry_service():
+    """Create a mock RegistryService for testing."""
+    service = Mock(spec=RegistryService)
+    return service
+
+
+@pytest.fixture
+def sample_deployable_server():
+    """Create a sample deployable registry server."""
+    return RegistryServer(
+        id="ai.test/test-server",
+        name="Test MCP Server",
+        namespace="ai.test",
+        description="A test MCP server",
+        version="1.0.0",
+        homepage=None,
+        repository_url="https://github.com/test/test-server",
+        capabilities=RegistryServerCapabilities(
+            deployable=True,
+            connectable=False
+        ),
+        trust=RegistryServerTrust(
+            is_official=False,
+            last_updated=""
+        ),
+        install_ref="https://github.com/test/test-server"
+    )
+
+
+@pytest.fixture
+def sample_glama_raw_server():
+    """Sample raw Glama server response."""
+    return {
+        "id": "test123",
+        "name": "Test MCP Server",
+        "namespace": "ai.test",
+        "slug": "test-server",
+        "description": "A test MCP server",
+        "repository": {
+            "url": "https://github.com/test/test-server"
+        },
+        "attributes": ["hosting:remote-capable"],
+        "environmentVariablesJsonSchema": {
+            "type": "object",
+            "properties": {
+                "API_KEY": {
+                    "type": "string",
+                    "description": "API key for authentication"
+                },
+                "DEBUG_MODE": {
+                    "type": "boolean",
+                    "description": "Enable debug logging"
+                }
+            },
+            "required": ["API_KEY"]
+        },
+        "tools": [
+            {"name": "search", "description": "Search tool"}
+        ],
+        "resources": [],
+        "prompts": [],
+        "spdxLicense": {
+            "name": "MIT License",
+            "url": "https://spdx.org/licenses/MIT.json"
+        },
+        "url": "https://glama.ai/mcp/servers/test123"
+    }
+
+
+@pytest.fixture
+def sample_form_data():
+    """Sample form data extracted from Glama server."""
+    return {
+        "name": "Test MCP Server",
+        "description": "A test MCP server",
+        "package": "@test/mcp-server",
+        "version": "1.0.0",
+        "env_vars": [
+            {
+                "name": "API_KEY",
+                "description": "API key for authentication",
+                "required": True,
+                "secret": True,
+                "format": "string"
+            },
+            {
+                "name": "DEBUG_MODE",
+                "description": "Enable debug logging",
+                "required": False,
+                "secret": False,
+                "format": "boolean"
+            }
+        ]
+    }
+
+
+@pytest.mark.asyncio
+async def test_get_registry_form_schema_success(
+    mock_registry_service,
+    sample_deployable_server,
+    sample_form_data,
+    sample_glama_raw_server
+):
+    """Test successful form generation from registry data."""
+    # Arrange: Setup mocks
+    mock_registry_service.get_server = AsyncMock(return_value=sample_deployable_server)
+    mock_registry_service.extract_form_data = Mock(return_value=sample_form_data)
+    mock_registry_service.get_raw_server = Mock(return_value=sample_glama_raw_server)
+
+    # Act: Call the endpoint
+    result = await get_registry_form_schema(
+        registry_id="ai.test/test-server",
+        registry_service=mock_registry_service
+    )
+
+    # Assert: Verify the result
+    assert isinstance(result, FormSchema)
+    assert result.title == "Configure Test MCP Server"
+    assert result.description == "A test MCP server"
+
+    # Verify fields
+    assert len(result.fields) == 3  # name + 2 env vars
+    assert result.fields[0].name == "name"
+    assert result.fields[0].type == "text"
+    assert result.fields[0].required
+
+    # Verify API_KEY field (should be password type because secret=True)
+    api_key_field = next(f for f in result.fields if f.name == "env_API_KEY")
+    assert api_key_field.type == "password"
+    assert api_key_field.required
+    assert "API key" in api_key_field.description
+
+    # Verify DEBUG_MODE field (boolean should map to checkbox)
+    debug_field = next(f for f in result.fields if f.name == "env_DEBUG_MODE")
+    assert debug_field.type == "checkbox"
+    assert not debug_field.required
+
+    # Verify mcp_config
+    assert result.mcp_config is not None
+    assert result.mcp_config["package"] == "@test/mcp-server"
+    assert result.mcp_config["tools"] == sample_glama_raw_server["tools"]
+    assert result.mcp_config["resources"] == []
+    assert result.mcp_config["prompts"] == []
+    assert result.mcp_config["server_info"]["source"] == "glama"
+    assert result.mcp_config["server_info"]["registry_id"] == "ai.test/test-server"
+    assert not result.mcp_config["server_info"]["is_official"]
+
+
+@pytest.mark.asyncio
+async def test_get_registry_form_schema_server_not_found(mock_registry_service):
+    """Test error when registry server is not found."""
+    # Arrange: Server doesn't exist
+    mock_registry_service.get_server = AsyncMock(return_value=None)
+
+    # Act & Assert: Should raise 404
+    with pytest.raises(HTTPException) as exc_info:
+        await get_registry_form_schema(
+            registry_id="ai.test/nonexistent",
+            registry_service=mock_registry_service
+    )
+
+    assert exc_info.value.status_code == 404
+    assert "not found in glama registry" in str(exc_info.value.detail).lower()
+
+
+@pytest.mark.asyncio
+async def test_get_registry_form_schema_local_only_server(
+    mock_registry_service,
+    sample_deployable_server
+):
+    """Test that local-only servers return 400 error."""
+    sample_deployable_server.capabilities.deployable = False
+    mock_registry_service.get_server = AsyncMock(return_value=sample_deployable_server)
+
+    # Act & Assert: Should raise 400
+    with pytest.raises(HTTPException) as exc_info:
+        await get_registry_form_schema(
+            registry_id="ai.test/test-server",
+            registry_service=mock_registry_service
+        )
+
+    assert exc_info.value.status_code == 400
+    assert "local-only" in str(exc_info.value.detail).lower()
+
+
+@pytest.mark.asyncio
+async def test_get_registry_form_schema_extract_error(
+    mock_registry_service,
+    sample_deployable_server
+):
+    """Test errors raised by extract_form_data are returned as 400."""
+    mock_registry_service.get_server = AsyncMock(return_value=sample_deployable_server)
+    mock_registry_service.extract_form_data = Mock(
+        side_effect=ValueError("Raw data not found for server ai.test/test-server")
+    )
+
+    # Act & Assert: Should raise 400
+    with pytest.raises(HTTPException) as exc_info:
+        await get_registry_form_schema(
+            registry_id="ai.test/test-server",
+            registry_service=mock_registry_service
+        )
+
+    assert exc_info.value.status_code == 400
+    assert "raw data not found" in str(exc_info.value.detail).lower()
+
+
+@pytest.mark.asyncio
+async def test_get_registry_form_schema_empty_env_vars(
+    mock_registry_service,
+    sample_deployable_server
+):
+    """Test form generation with no environment variables (valid case)."""
+    # Arrange: Server with no env vars
+    form_data = {
+        "name": "Test MCP Server",
+        "description": "A test MCP server",
+        "package": "@test/mcp-server",
+        "version": "1.0.0",
+        "env_vars": []  # No environment variables
+    }
+    mock_registry_service.get_server = AsyncMock(return_value=sample_deployable_server)
+    mock_registry_service.extract_form_data = Mock(return_value=form_data)
+    mock_registry_service.get_raw_server = Mock(return_value={})
+
+    # Act: Call the endpoint
+    result = await get_registry_form_schema(
+        registry_id="ai.test/test-server",
+        registry_service=mock_registry_service
+    )
+
+    # Assert: Should succeed with only name field
+    assert isinstance(result, FormSchema)
+    assert len(result.fields) == 1  # Only "name" field
+    assert result.fields[0].name == "name"
+    assert result.mcp_config["package"] == "@test/mcp-server"
+
+
+@pytest.mark.asyncio
+async def test_get_registry_form_schema_unexpected_error(
+    mock_registry_service,
+    sample_deployable_server
+):
+    """Test handling of unexpected errors."""
+    # Arrange: Unexpected exception
+    mock_registry_service.get_server = AsyncMock(return_value=sample_deployable_server)
+    mock_registry_service.extract_form_data = Mock(
+        side_effect=Exception("Unexpected error")
+    )
+    mock_registry_service.get_raw_server = Mock(return_value={})
+
+    # Act & Assert: Should raise 500
+    with pytest.raises(HTTPException) as exc_info:
+        await get_registry_form_schema(
+            registry_id="ai.test/test-server",
+            registry_service=mock_registry_service
+        )
+
+    assert exc_info.value.status_code == 500
+    assert "Failed to generate form schema" in str(exc_info.value.detail)
+
+
+def test_registry_service_extract_form_data():
+    """Test RegistryService.extract_form_data() method."""
+    # Create a real RegistryService instance
+    service = RegistryService()
+
+    # Create sample raw data
+    raw_data = {
+        "name": "Test MCP Server",
+        "namespace": "ai.test",
+        "slug": "test-server",
+        "repository": {"url": "https://github.com/test/test-server"},
+        "attributes": ["hosting:remote-capable"],
+        "environmentVariablesJsonSchema": {
+            "type": "object",
+            "properties": {
+                "API_KEY": {
+                    "type": "string",
+                    "description": "API key for authentication"
+                }
+            },
+            "required": ["API_KEY"]
+        }
+    }
+
+    # Create a RegistryServer
+    server = RegistryServer(
+        id="ai.test/test-server",
+        name="Test MCP Server",
+        namespace="ai.test",
+        description="A test MCP server",
+        version="1.0.0",
+        homepage=None,
+        repository_url="https://github.com/test/test-server",
+        capabilities=RegistryServerCapabilities(deployable=True, connectable=False),
+        trust=RegistryServerTrust(is_official=False, last_updated=""),
+        install_ref="https://github.com/test/test-server"
+    )
+
+    # Manually inject raw data into cache
+    service._raw_cache["ai.test/test-server"] = raw_data
+
+    # Act: Extract form data
+    result = service.extract_form_data(server)
+
+    # Assert: Verify extraction
+    assert result["name"] == "Test MCP Server"
+    assert result["package"] == "https://github.com/test/test-server"
+    assert len(result["env_vars"]) == 1
+    assert result["env_vars"][0]["name"] == "API_KEY"
+    assert result["env_vars"][0]["secret"]
+
+
+def test_registry_service_extract_form_data_no_env_schema():
+    """Test extract_form_data with server that has no environment variable schema."""
+    service = RegistryService()
+
+    # Raw data with no env schema
+    raw_data = {
+        "name": "Test MCP Server",
+        "namespace": "ai.test",
+        "slug": "test-server",
+        "repository": {"url": "https://github.com/test/test-server"},
+        "attributes": ["hosting:remote-capable"],
+    }
+
+    server = RegistryServer(
+        id="ai.test/test-server",
+        name="Test MCP Server",
+        namespace="ai.test",
+        description="A test MCP server",
+        version="1.0.0",
+        homepage=None,
+        repository_url="https://github.com/test/test-server",
+        capabilities=RegistryServerCapabilities(deployable=True, connectable=False),
+        trust=RegistryServerTrust(is_official=False, last_updated=""),
+        install_ref="https://github.com/test/test-server"
+    )
+
+    service._raw_cache["ai.test/test-server"] = raw_data
+
+    # Act: Should return empty env vars
+    result = service.extract_form_data(server)
+    assert result["package"] == "https://github.com/test/test-server"
+    assert result["env_vars"] == []
diff --git a/backend/tests/unit/test_credential_validator.py b/backend/tests/unit/test_credential_validator.py
new file mode 100644
index 0000000..c8eecb2
--- /dev/null
+++ b/backend/tests/unit/test_credential_validator.py
@@ -0,0 +1,353 @@
+"""
+Tests for Credential Validator Service
+
+Tests validation of required credentials before deployment.
+"""
+
+import pytest
+from app.services.credential_validator import CredentialValidator
+
+
+class TestCredentialValidation:
+    """Test credential validation logic"""
+
+    def test_all_required_credentials_provided(self):
+        """Test validation when all required credentials are provided"""
+        validator = CredentialValidator()
+
+        provided_credentials = {
+            "env_TICKTICK_TOKEN": "abc123",
+            "env_API_KEY": "xyz789"
+        }
+
+        required_env_vars = [
+            {"name": "TICKTICK_TOKEN", "required": True},
+            {"name": "API_KEY", "required": True}
+        ]
+
+        result = validator.validate_credentials(provided_credentials, required_env_vars)
+
+        assert result["valid"] is True
+        assert result["errors"] == []
+
+    def test_missing_required_credential(self):
+        """Test validation when a required credential is missing"""
+        validator = CredentialValidator()
+
+        provided_credentials = {
+            "env_TICKTICK_TOKEN": "abc123"
+            # Missing API_KEY
+        }
+
+        required_env_vars = [
+            {"name": "TICKTICK_TOKEN", "required": True},
+            {"name": "API_KEY", "required": True}
+        ]
+
+        result = validator.validate_credentials(provided_credentials, required_env_vars)
+
+        assert result["valid"] is False
+        assert len(result["errors"]) == 1
+        assert "API_KEY" in result["errors"][0]
+        assert "missing" in result["errors"][0].lower()
+
+    def test_empty_string_credential(self):
+        """Test validation when credential is empty string"""
+        validator = CredentialValidator()
+
+        provided_credentials = {
+            "env_TICKTICK_TOKEN": "",  # Empty string
+            "env_API_KEY": "   "  # Whitespace only
+        }
+
+        required_env_vars = [
+            {"name": "TICKTICK_TOKEN", "required": True},
+            {"name": "API_KEY", "required": True}
+        ]
+
+        result = validator.validate_credentials(provided_credentials, required_env_vars)
+
+        assert result["valid"] is False
+        assert len(result["errors"]) == 2
+        # Both should be flagged as empty
+        assert any("TICKTICK_TOKEN" in err for err in result["errors"])
+        assert any("API_KEY" in err for err in result["errors"])
+
+    def test_optional_credential_missing(self):
+        """Test that missing optional credentials don't cause validation failure"""
+        validator = CredentialValidator()
+
+        provided_credentials = {
+            "env_TICKTICK_TOKEN": "abc123"
+            # Optional API_KEY is missing - should be OK
+        }
+
+        required_env_vars = [
+            {"name": "TICKTICK_TOKEN", "required": True},
+            {"name": "API_KEY", "required": False}  # Optional
+        ]
+
+        result = validator.validate_credentials(provided_credentials, required_env_vars)
+
+        assert result["valid"] is True
+        assert result["errors"] == []
+
+    def test_multiple_missing_credentials(self):
+        """Test that all missing credentials are listed in errors"""
+        validator = CredentialValidator()
+
+        provided_credentials = {}  # No credentials provided
+
+        required_env_vars = [
+            {"name": "TICKTICK_TOKEN", "required": True},
+            {"name": "API_KEY", "required": True},
+            {"name": "DATABASE_URL", "required": True}
+        ]
+
+        result = validator.validate_credentials(provided_credentials, required_env_vars)
+
+        assert result["valid"] is False
+        assert len(result["errors"]) == 3
+        # All three should be in the error list
+        assert any("TICKTICK_TOKEN" in err for err in result["errors"])
+        assert any("API_KEY" in err for err in result["errors"])
+        assert any("DATABASE_URL" in err for err in result["errors"])
+
+    def test_credential_without_env_prefix(self):
+        """Test that credentials work both with and without 'env_' prefix"""
+        validator = CredentialValidator()
+
+        # Provide credential WITHOUT 'env_' prefix
+        provided_credentials = {
+            "TICKTICK_TOKEN": "abc123"  # No 'env_' prefix
+        }
+
+        required_env_vars = [
+            {"name": "TICKTICK_TOKEN", "required": True}
+        ]
+
+        result = validator.validate_credentials(provided_credentials, required_env_vars)
+
+        assert result["valid"] is True
+        assert result["errors"] == []
+
+    def test_mixed_prefix_and_no_prefix(self):
+        """Test credentials with mixed prefix/no-prefix formats"""
+        validator = CredentialValidator()
+
+        provided_credentials = {
+            "env_TICKTICK_TOKEN": "abc123",  # With prefix
+            "API_KEY": "xyz789"  # Without prefix
+        }
+
+        required_env_vars = [
+            {"name": "TICKTICK_TOKEN", "required": True},
+            {"name": "API_KEY", "required": True}
+        ]
+
+        result = validator.validate_credentials(provided_credentials, required_env_vars)
+
+        assert result["valid"] is True
+        assert result["errors"] == []
+
+    def test_env_var_without_name(self):
+        """Test handling of malformed env var config (missing name)"""
+        validator = CredentialValidator()
+
+        provided_credentials = {
+            "env_TICKTICK_TOKEN": "abc123"
+        }
+
+        required_env_vars = [
+            {"required": True},  # Missing 'name' field
+            {"name": "TICKTICK_TOKEN", "required": True}
+        ]
+
+        result = validator.validate_credentials(provided_credentials, required_env_vars)
+
+        # Should skip the malformed entry and validate the good one
+        assert result["valid"] is True
+        assert result["errors"] == []
+
+    def test_empty_required_env_vars_list(self):
+        """Test validation when no env vars are required"""
+        validator = CredentialValidator()
+
+        provided_credentials = {}
+        required_env_vars = []
+
+        result = validator.validate_credentials(provided_credentials, required_env_vars)
+
+        assert result["valid"] is True
+        assert result["errors"] == []
+
+    def test_all_optional_credentials(self):
+        """Test validation when all env vars are optional"""
+        validator = CredentialValidator()
+
+        provided_credentials = {}  # No credentials provided
+
+        required_env_vars = [
+            {"name": "OPTIONAL_VAR_1", "required": False},
+            {"name": "OPTIONAL_VAR_2", "required": False}
+        ]
+
+        result = validator.validate_credentials(provided_credentials, required_env_vars)
+
+        # Should be valid since all are optional
+        assert result["valid"] is True
+        assert result["errors"] == []
+
+    def test_extra_credentials_provided(self):
+        """Test that extra (not required) credentials don't cause issues"""
+        validator = CredentialValidator()
+
+        provided_credentials = {
+            "env_TICKTICK_TOKEN": "abc123",
+            "env_EXTRA_VAR": "extra_value",  # Not in required list
+            "env_ANOTHER_EXTRA": "another_value"
+        }
+
+        required_env_vars = [
+            {"name": "TICKTICK_TOKEN", "required": True}
+        ]
+
+        result = validator.validate_credentials(provided_credentials, required_env_vars)
+
+        # Extra credentials should be ignored
+        assert result["valid"] is True
+        assert result["errors"] == []
+
+
+class TestCredentialValidationSimple:
+    """Test simplified validation method"""
+
+    def test_simple_validation_all_provided(self):
+        """Test simplified validation with all credentials"""
+        validator = CredentialValidator()
+
+        provided_credentials = {
+            "env_TOKEN": "abc",
+            "env_KEY": "xyz"
+        }
+
+        required_var_names = ["TOKEN", "KEY"]
+
+        result = validator.validate_credentials_simple(
+            provided_credentials,
+            required_var_names
+        )
+
+        assert result["valid"] is True
+        assert result["errors"] == []
+
+    def test_simple_validation_missing_credential(self):
+        """Test simplified validation with missing credential"""
+        validator = CredentialValidator()
+
+        provided_credentials = {
+            "env_TOKEN": "abc"
+        }
+
+        required_var_names = ["TOKEN", "KEY"]
+
+        result = validator.validate_credentials_simple(
+            provided_credentials,
+            required_var_names
+        )
+
+        assert result["valid"] is False
+        assert len(result["errors"]) == 1
+        assert "KEY" in result["errors"][0]
+
+    def test_simple_validation_empty_list(self):
+        """Test simplified validation with no required credentials"""
+        validator = CredentialValidator()
+
+        provided_credentials = {}
+        required_var_names = []
+
+        result = validator.validate_credentials_simple(
+            provided_credentials,
+            required_var_names
+        )
+
+        assert result["valid"] is True
+        assert result["errors"] == []
+
+
+class TestCredentialValidationEdgeCases:
+    """Test edge cases and special scenarios"""
+
+    def test_none_value_credential(self):
+        """Test handling of None as credential value"""
+        validator = CredentialValidator()
+
+        provided_credentials = {
+            "env_TOKEN": None  # Explicit None
+        }
+
+        required_env_vars = [
+            {"name": "TOKEN", "required": True}
+        ]
+
+        result = validator.validate_credentials(provided_credentials, required_env_vars)
+
+        assert result["valid"] is False
+        assert len(result["errors"]) == 1
+
+    def test_numeric_credential_value(self):
+        """Test that numeric credential values are accepted"""
+        validator = CredentialValidator()
+
+        provided_credentials = {
+            "env_PORT": "8080",  # Numeric but as string
+            "env_TIMEOUT": "30"
+        }
+
+        required_env_vars = [
+            {"name": "PORT", "required": True},
+            {"name": "TIMEOUT", "required": True}
+        ]
+
+        result = validator.validate_credentials(provided_credentials, required_env_vars)
+
+        assert result["valid"] is True
+        assert result["errors"] == []
+
+    def test_credential_with_special_characters(self):
+        """Test credentials containing special characters"""
+        validator = CredentialValidator()
+
+        provided_credentials = {
+            "env_API_KEY": "sk-abc123_xyz!@#$%^&*()",
+            "env_TOKEN": "Bearer eyJ..."
+        }
+
+        required_env_vars = [
+            {"name": "API_KEY", "required": True},
+            {"name": "TOKEN", "required": True}
+        ]
+
+        result = validator.validate_credentials(provided_credentials, required_env_vars)
+
+        assert result["valid"] is True
+        assert result["errors"] == []
+
+    def test_case_sensitive_credential_names(self):
+        """Test that credential names are case-sensitive"""
+        validator = CredentialValidator()
+
+        provided_credentials = {
+            "env_ticktick_token": "abc123"  # Lowercase
+        }
+
+        required_env_vars = [
+            {"name": "TICKTICK_TOKEN", "required": True}  # Uppercase
+        ]
+
+        result = validator.validate_credentials(provided_credentials, required_env_vars)
+
+        # Should fail because case doesn't match
+        assert result["valid"] is False
+        assert len(result["errors"]) == 1
diff --git a/backend/tests/unit/test_package_validator.py b/backend/tests/unit/test_package_validator.py
new file mode 100644
index 0000000..d5fdc91
--- /dev/null
+++ b/backend/tests/unit/test_package_validator.py
@@ -0,0 +1,291 @@
+"""
+Tests for Package Validator Service
+
+Tests validation of npm and Python packages against their respective registries.
+"""
+
+import pytest
+import httpx
+from unittest.mock import patch, AsyncMock, Mock
+from app.services.package_validator import PackageValidator
+
+
+@pytest.mark.asyncio
+class TestNpmPackageValidation:
+    """Test npm package validation"""
+
+    async def test_valid_npm_package(self):
+        """Test validation of a real, existing npm package"""
+        validator = PackageValidator()
+
+        # Use a real, stable npm package for testing
+        result = await validator.validate_npm_package("express")
+
+        assert result["valid"] is True
+        assert result["error"] is None
+        assert result["version"] is not None
+        assert isinstance(result["version"], str)
+
+    async def test_valid_scoped_npm_package(self):
+        """Test validation of a scoped npm package (e.g., @user/package)"""
+        validator = PackageValidator()
+
+        # Test with a real scoped package
+        result = await validator.validate_npm_package("@types/node")
+
+        assert result["valid"] is True
+        assert result["error"] is None
+        assert result["version"] is not None
+
+    async def test_invalid_npm_package(self):
+        """Test validation of a non-existent npm package"""
+        validator = PackageValidator()
+
+        # Use a package name that definitely doesn't exist
+        fake_package = "this-package-definitely-does-not-exist-xyz-123456789"
+        result = await validator.validate_npm_package(fake_package)
+
+        assert result["valid"] is False
+        assert result["error"] is not None
+        assert f"Package '{fake_package}' not found" in result["error"]
+        assert result["version"] is None
+
+    async def test_npm_package_url_encoding(self):
+        """Test that scoped packages are properly URL-encoded"""
+        validator = PackageValidator()
+
+        # This tests the @user/package → @user%2Fpackage encoding
+        # We use a mocked response to ensure URL encoding is correct
+        with patch.object(httpx.AsyncClient, 'get') as mock_get:
+            mock_response = AsyncMock()
+            mock_response.status_code = 200
+            # json() is synchronous in httpx, so use regular Mock (not AsyncMock)
+            mock_response.json = Mock(return_value={
+                "dist-tags": {"latest": "1.0.0"}
+            })
+            mock_get.return_value = mock_response
+
+            await validator.validate_npm_package("@scope/package")
+
+            # Check that the URL was properly encoded
+            called_url = mock_get.call_args[0][0]
+            assert "%2F" in called_url
+            assert "@scope%2Fpackage" in called_url
+
+    async def test_npm_network_timeout(self):
+        """Test handling of network timeout"""
+        validator = PackageValidator()
+
+        with patch.object(httpx.AsyncClient, 'get') as mock_get:
+            # Simulate timeout exception
+            mock_get.side_effect = httpx.TimeoutException("Timeout")
+
+            result = await validator.validate_npm_package("test-package")
+
+            assert result["valid"] is False
+            assert result["error"] is not None
+            assert "timed out" in result["error"].lower() or "timeout" in result["error"].lower()
+            assert result["version"] is None
+
+    async def test_npm_network_error(self):
+        """Test handling of network connection errors"""
+        validator = PackageValidator()
+
+        with patch.object(httpx.AsyncClient, 'get') as mock_get:
+            # Simulate network error
+            mock_get.side_effect = httpx.RequestError("Connection failed")
+
+            result = await validator.validate_npm_package("test-package")
+
+            assert result["valid"] is False
+            assert result["error"] is not None
+            assert "connect" in result["error"].lower() or "network" in result["error"].lower()
+            assert result["version"] is None
+
+    async def test_npm_registry_error(self):
+        """Test handling of unexpected registry HTTP errors"""
+        validator = PackageValidator()
+
+        with patch.object(httpx.AsyncClient, 'get') as mock_get:
+            mock_response = AsyncMock()
+            mock_response.status_code = 500  # Internal server error
+            mock_get.return_value = mock_response
+
+            result = await validator.validate_npm_package("test-package")
+
+            assert result["valid"] is False
+            assert result["error"] is not None
+            assert "500" in result["error"]
+            assert result["version"] is None
+
+    async def test_npm_malformed_response(self):
+        """Test handling of malformed JSON response from registry"""
+        validator = PackageValidator()
+
+        with patch.object(httpx.AsyncClient, 'get') as mock_get:
+            mock_response = AsyncMock()
+            mock_response.status_code = 200
+            # json() is synchronous in httpx, so use regular Mock (not AsyncMock)
+            mock_response.json = Mock(return_value={})
+            mock_get.return_value = mock_response
+
+            result = await validator.validate_npm_package("test-package")
+
+            # Should still return valid=True even if version is missing
+            assert result["valid"] is True
+            assert result["version"] is None
+
+
+@pytest.mark.asyncio
+class TestPythonPackageValidation:
+    """Test Python package validation"""
+
+    async def test_valid_python_package(self):
+        """Test validation of a real, existing Python package"""
+        validator = PackageValidator()
+
+        # Use a real, stable Python package for testing
+        result = await validator.validate_python_package("requests")
+
+        assert result["valid"] is True
+        assert result["error"] is None
+        assert result["version"] is not None
+        assert isinstance(result["version"], str)
+
+    async def test_invalid_python_package(self):
+        """Test validation of a non-existent Python package"""
+        validator = PackageValidator()
+
+        # Use a package name that definitely doesn't exist
+        fake_package = "this-python-package-does-not-exist-xyz-987654321"
+        result = await validator.validate_python_package(fake_package)
+
+        assert result["valid"] is False
+        assert result["error"] is not None
+        assert f"Package '{fake_package}' not found" in result["error"]
+        assert result["version"] is None
+
+    async def test_python_package_with_hyphens(self):
+        """Test Python packages with hyphens (common in Python ecosystem)"""
+        validator = PackageValidator()
+
+        # Test with a real package that uses hyphens
+        result = await validator.validate_python_package("urllib3")
+
+        assert result["valid"] is True
+        assert result["version"] is not None
+
+    async def test_python_network_timeout(self):
+        """Test handling of network timeout for PyPI"""
+        validator = PackageValidator()
+
+        with patch.object(httpx.AsyncClient, 'get') as mock_get:
+            # Simulate timeout exception
+            mock_get.side_effect = httpx.TimeoutException("Timeout")
+
+            result = await validator.validate_python_package("test-package")
+
+            assert result["valid"] is False
+            assert result["error"] is not None
+            assert "timed out" in result["error"].lower() or "timeout" in result["error"].lower()
+            assert result["version"] is None
+
+    async def test_python_network_error(self):
+        """Test handling of network connection errors for PyPI"""
+        validator = PackageValidator()
+
+        with patch.object(httpx.AsyncClient, 'get') as mock_get:
+            # Simulate network error
+            mock_get.side_effect = httpx.RequestError("Connection failed")
+
+            result = await validator.validate_python_package("test-package")
+
+            assert result["valid"] is False
+            assert result["error"] is not None
+            assert "connect" in result["error"].lower() or "network" in result["error"].lower()
+            assert result["version"] is None
+
+    async def test_python_registry_error(self):
+        """Test handling of unexpected PyPI HTTP errors"""
+        validator = PackageValidator()
+
+        with patch.object(httpx.AsyncClient, 'get') as mock_get:
+            mock_response = AsyncMock()
+            mock_response.status_code = 503  # Service unavailable
+            mock_get.return_value = mock_response
+
+            result = await validator.validate_python_package("test-package")
+
+            assert result["valid"] is False
+            assert result["error"] is not None
+            assert "503" in result["error"]
+            assert result["version"] is None
+
+    async def test_python_malformed_response(self):
+        """Test handling of malformed JSON response from PyPI"""
+        validator = PackageValidator()
+
+        with patch.object(httpx.AsyncClient, 'get') as mock_get:
+            mock_response = AsyncMock()
+            mock_response.status_code = 200
+            # json() is synchronous in httpx, so use regular Mock (not AsyncMock)
+            mock_response.json = Mock(return_value={"info": {}})
+            mock_get.return_value = mock_response
+
+            result = await validator.validate_python_package("test-package")
+
+            # Should still return valid=True even if version is missing
+            assert result["valid"] is True
+            assert result["version"] is None
+
+
+@pytest.mark.asyncio
+class TestPackageValidatorEdgeCases:
+    """Test edge cases and special scenarios"""
+
+    async def test_empty_package_name_npm(self):
+        """Test handling of empty package name for npm"""
+        validator = PackageValidator()
+
+        result = await validator.validate_npm_package("")
+
+        # Empty package should fail validation
+        assert result["valid"] is False
+        assert result["error"] is not None
+
+    async def test_empty_package_name_python(self):
+        """Test handling of empty package name for Python"""
+        validator = PackageValidator()
+
+        result = await validator.validate_python_package("")
+
+        # Empty package should fail validation
+        assert result["valid"] is False
+        assert result["error"] is not None
+
+    async def test_special_characters_in_package_name(self):
+        """Test package names with special characters"""
+        validator = PackageValidator()
+
+        # npm packages can have @, /, and -
+        result = await validator.validate_npm_package("@scope/package-name")
+        # Should handle the encoding properly (even if package doesn't exist)
+        assert "error" in result
+
+    async def test_timeout_value_respected(self):
+        """Test that the timeout value is properly used"""
+        validator = PackageValidator()
+
+        # Verify timeout is set correctly
+        assert validator.timeout == 5.0
+
+        # We could mock httpx.AsyncClient to verify timeout is passed
+        with patch('httpx.AsyncClient') as mock_client_class:
+            mock_client_instance = AsyncMock()
+            mock_client_class.return_value.__aenter__.return_value = mock_client_instance
+            mock_client_instance.get = AsyncMock(return_value=AsyncMock(status_code=404))
+
+            await validator.validate_npm_package("test")
+
+            # Verify AsyncClient was called with correct timeout
+            mock_client_class.assert_called_with(timeout=5.0)
diff --git a/backend/uv.lock b/backend/uv.lock
index 724e064..d690de2 100644
--- a/backend/uv.lock
+++ b/backend/uv.lock
@@ -232,6 +232,7 @@ version = "0.1.0"
 source = { virtual = "." }
 dependencies = [
     { name = "fastapi" },
+    { name = "httpx" },
     { name = "openai" },
     { name = "sse-starlette" },
     { name = "uvicorn", extra = ["standard"] },
@@ -240,6 +241,7 @@ dependencies = [
 [package.metadata]
 requires-dist = [
     { name = "fastapi" },
+    { name = "httpx", specifier = ">=0.27.0" },
     { name = "openai", specifier = ">=2.11.0" },
     { name = "sse-starlette", specifier = ">=3.0.3" },
     { name = "uvicorn", extras = ["standard"] },
diff --git a/context/CURRENT_STATUS.md b/context/CURRENT_STATUS.md
index 4dd72c0..d609cc3 100644
--- a/context/CURRENT_STATUS.md
+++ b/context/CURRENT_STATUS.md
@@ -1,6 +1,6 @@
 # Current Development Status
 
-**Last Updated**: 2025-12-15
+**Last Updated**: 2025-12-17
 **Current Phase**: Phase 6 - MCP Server Container Deployment (WORKING ✅)
 
 ---
@@ -47,6 +47,7 @@
 - ✅ Frontend configured to use production backend
 - ✅ Docker image built and deployed successfully
 - ✅ Always-on backend with `min_machines_running = 1`
+- ✅ **RegistryService Refactored**: Concurrency safety, timeouts, and robustness improvements
 
 ### Phase 6: MCP Server Container Deployment (WORKING ✅)
 - ✅ Backend creates Fly Machines for deployments (when `FLY_API_TOKEN` is set)
@@ -54,6 +55,13 @@
 - ✅ Backend forwards Streamable HTTP to the MCP machine over Fly private networking
 - ✅ End-to-end connectivity verified from backend → machine on `:8080`
 
+### Phase 1: Validation & Error Handling (NEW - COMPLETED! ✅)
+- ✅ Package validation (npm and PyPI registries)
+- ✅ Credential validation (required fields checked)
+- ✅ Structured error responses with actionable help messages
+- ✅ Frontend error display (user-visible error messages)
+- ✅ Runtime detection (npm vs Python automatic)
+
 ---
 
 ## 🎉 What Works Right Now
@@ -89,12 +97,12 @@
 
 ## 🚧 What's NOT Working Yet
 
-### Hardening + UX (Next)
+### Hardening + UX (Next - Phase 2)
 
-What remains (now that machines deploy and tool calls connect):
-- Health monitoring loop (beyond Fly restart policy) and better “unhealthy” status reporting
+What remains (now that validation and deployment work):
+- ✅ ~~Package and credential validation~~ (COMPLETED in Phase 1!)
+- Health monitoring loop (beyond Fly restart policy) and better "unhealthy" status reporting
 - Clear progress/status surfaced to the frontend during machine start and package install
-- Stronger validation that analysis produced a runnable `mcp_config.package` for arbitrary repos
 
 ---
 
@@ -382,13 +390,19 @@ fly deploy --app catwalk-live-backend-dev
 
 ## 🎓 For Future Claude Sessions
 
-**You are currently at**: Phase 6 Working (Streamable HTTP end-to-end)
+**You are currently at**: Phase 1 Complete! (Validation & Error Handling) + Phase 6 Working (Streamable HTTP end-to-end)
 
-**What works**: Full backend API on Fly.io, frontend locally, deployments stored in database
+**What works**:
+- Full backend API on Fly.io
+- Frontend locally with error display
+- Package validation (npm/PyPI)
+- Credential validation
+- Deployments with structured error messages
+- End-to-end MCP tool calls
 
-**What doesn't work**: Health monitoring loop + richer deployment progress (non-blocking)
+**What doesn't work**: Health monitoring loop + richer deployment progress (Phase 2)
 
-**Next task**: Harden analysis → `mcp_config.package` mapping + improve machine health/status reporting
+**Next task**: Phase 2 - Health Monitoring & Status tracking (see `context/plans/roadmap/phase-2-monitoring.md`)
 
 **Reference code**: `remote-mcp-pilot/deploy/` has working Fly.io deployment
 
@@ -396,4 +410,5 @@ fly deploy --app catwalk-live-backend-dev
 1. This file (CURRENT_STATUS.md)
 2. `CLAUDE.md` for deployment pitfalls
 3. `context/ARCHITECTURE.md` for system design
-4. `app/api/deployments.py` to see where Fly deployment should hook in
+4. `app/api/deployments.py` to see validation integration
+5. `app/services/package_validator.py` and `credential_validator.py` for validation logic
diff --git a/context/plans/roadmap/OVERVIEW.md b/context/plans/roadmap/OVERVIEW.md
new file mode 100644
index 0000000..60314f0
--- /dev/null
+++ b/context/plans/roadmap/OVERVIEW.md
@@ -0,0 +1,113 @@
+# Catwalk-Live: Complete Product Roadmap
+**From Working Prototype → Production Platform → MCP Ecosystem Hub**
+
+## Current State (Phase 6 Working ✅)
+
+### What Works
+- ✅ Backend API deployed on Fly.io with PostgreSQL
+- ✅ TickTick MCP server works remotely end-to-end
+- ✅ Glama registry integration (12K+ servers)
+- ✅ Streamable HTTP transport (MCP 2025-06-18 spec)
+- ✅ Credential encryption and storage
+- ✅ Frontend connects to production backend
+
+### Critical Gaps (Must Fix First)
+- ❌ No package validation (deployments fail silently)
+- ❌ No runtime detection (only npm, no Python support)
+- ❌ No container observability (logs invisible)
+- ❌ No health monitoring (status optimistic, not real)
+- ❌ Poor error UX ("Failed to create deployment")
+
+## Roadmap Structure
+
+### **PART 1: Foundation (Months 1-3)** - Make It Reliable
+Focus on making current functionality work consistently for 95% of use cases.
+
+- **Phase 1**: Validation & Error Handling (2 weeks)
+- **Phase 2**: Health Monitoring & Status (2-3 weeks)
+- **Phase 3**: Multi-Runtime Support (2-3 weeks)
+
+### **PART 2: Observability (Month 4)** - See What's Happening
+Enable users to debug and understand their deployments.
+
+- **Phase 4**: Container Logs & Diagnostics (3-4 weeks)
+
+### **PART 3: Cost Optimization (Month 5)** - Make It Affordable
+Reduce infrastructure costs through smart scaling.
+
+- **Phase 5**: Serverless & Infrastructure (3-4 weeks)
+
+### **PART 4: Advanced Features (Month 6)** - Handle Edge Cases
+Support complex deployment scenarios.
+
+- **Phase 6**: Version Pinning, OAuth, GitHub-only (4-6 weeks)
+
+### **PART 5: Product Evolution (Months 7-12)** - Beyond Deployment
+Transform from deployment tool to platform.
+
+- **Phase 7**: Marketplace & Discovery
+- **Phase 8**: Developer Tools & Testing
+- **Phase 9**: Team Collaboration & Enterprise
+
+### **PART 6: Ecosystem (Year 2+)** - Platform Maturity
+Become the definitive MCP deployment platform.
+
+- **Phase 10**: Global Edge Network
+- **Phase 11**: Advanced MCP Features
+- **Phase 12**: Server Development Platform
+- **Phase 13**: AI Agent Marketplace
+- **Phase 14**: Enterprise Self-Hosted
+
+## Success Metrics Timeline
+
+### Month 3 (Foundation Complete)
+- 95% deployment success rate
+- 100% deployments have logs
+- 85% Python servers work
+- <1 min health detection
+
+### Month 6 (Core Product)
+- 70% cost reduction (serverless)
+- 1,000+ total deployments
+- 50+ active weekly users
+- 4.5/5 user satisfaction
+
+### Month 12 (Platform Evolution)
+- 10,000+ deployments
+- 500+ active weekly users
+- 50+ paying team accounts
+- $10K+ MRR
+
+### Year 2 (Ecosystem)
+- 100,000+ deployments
+- 5,000+ active weekly users
+- 500+ enterprise customers
+- $100K+ MRR
+
+## Long-Term Vision
+
+**Catwalk-Live will be the Vercel/Netlify of MCP servers** - the easiest, most reliable way to deploy, manage, and discover MCP servers globally.
+
+We'll enable:
+- **Developers** to build and test MCP servers with zero infrastructure
+- **Teams** to collaborate on shared MCP deployments
+- **Enterprises** to self-host MCP infrastructure securely
+- **AI agents** to discover and use thousands of MCP tools seamlessly
+
+## Files in This Directory
+
+- `OVERVIEW.md` (this file) - High-level roadmap summary
+- `DEV_NOTES.md` - Technical decisions and implementation guidance
+- `phase-1-validation.md` - Package & credential validation
+- `phase-2-monitoring.md` - Health checks & status tracking
+- `phase-3-runtime.md` - Python & multi-runtime support
+- `phase-4-observability.md` - Container logs & diagnostics
+- `phase-5-serverless.md` - Cost optimization & scale-to-zero
+- `phase-6-advanced.md` - Version pinning, OAuth, GitHub repos
+- `future-vision.md` - Phases 7-14 (marketplace, teams, edge, etc.)
+
+## Immediate Next Steps
+
+See `phase-1-validation.md` for detailed implementation plan and checklist.
+
+**Start Here**: Package validation is the highest-impact improvement (prevents 80% of current failures).
diff --git a/context/plans/roadmap/phase-1-validation.md b/context/plans/roadmap/phase-1-validation.md
new file mode 100644
index 0000000..3a2080c
--- /dev/null
+++ b/context/plans/roadmap/phase-1-validation.md
@@ -0,0 +1,388 @@
+# Phase 1: Validation & Error Handling
+**Duration**: 2 weeks
+**Priority**: P0 (Blocking)
+**Goal**: Prevent 80% of deployment failures before they happen
+
+## Overview
+
+Currently, deployments fail silently when:
+- Package names don't exist in npm/PyPI
+- Required credentials are missing
+- Users see generic "Failed to create deployment" errors
+
+This phase adds validation BEFORE creating Fly machines, with clear error messages guiding users to fix issues.
+
+## Success Criteria
+
+- ✅ 95% of valid npm packages deploy successfully
+- ✅ Invalid packages rejected with clear error message
+- ✅ Missing required credentials detected before deployment
+- ✅ Users see actionable error messages (not generic "failed")
+
+## Implementation Checklist
+
+### 1. Package Validation Service
+
+- [ ] **Create `backend/app/services/package_validator.py`**
+  - [ ] Implement `validate_npm_package(package: str)` method
+    - [ ] Handle scoped packages (`@user/package` → `@user%2Fpackage`)
+    - [ ] Call `https://registry.npmjs.org/{package}`
+    - [ ] Extract version from `dist-tags.latest`
+    - [ ] Return `{"valid": bool, "error": str, "version": str}`
+  - [ ] Implement `validate_python_package(package: str)` method
+    - [ ] Call `https://pypi.org/pypi/{package}/json`
+    - [ ] Extract version from `info.version`
+    - [ ] Return same format as npm validation
+  - [ ] Add timeout handling (5 seconds max)
+  - [ ] Add exception handling for network errors
+
+- [ ] **Write tests for package validation**
+  - [ ] Test valid npm package (e.g., `@alexarevalo.ai/mcp-server-ticktick`)
+  - [ ] Test invalid npm package (should return `valid: false`)
+  - [ ] Test valid Python package (e.g., `mcp-server-git`)
+  - [ ] Test invalid Python package
+  - [ ] Test network timeout scenario
+  - [ ] Test malformed package names
+
+### 2. Credential Validation Service
+
+- [ ] **Create `backend/app/services/credential_validator.py`**
+  - [ ] Implement `validate_credentials()` method
+  - [ ] Accept `provided_credentials` and `required_env_vars` params
+  - [ ] Check each required env var is present and non-empty
+  - [ ] Return `{"valid": bool, "errors": List[str]}`
+  - [ ] Handle `env_` prefix stripping (e.g., `env_API_KEY` vs `API_KEY`)
+
+- [ ] **Write tests for credential validation**
+  - [ ] Test all required credentials provided → valid
+  - [ ] Test missing required credential → invalid with error message
+  - [ ] Test optional credential missing → valid
+  - [ ] Test empty string credential → invalid
+  - [ ] Test multiple missing credentials → list all in errors
+
+### 3. Integration into Deployment Flow
+
+- [ ] **Update `backend/app/api/deployments.py`** (around line 47-120)
+  - [ ] Import validators
+  - [ ] After extracting `package` from `mcp_config`:
+    - [ ] Detect runtime (npm if starts with `@` or has `/`, else check for `.`)
+    - [ ] Call appropriate validator (`validate_npm_package` or `validate_python_package`)
+    - [ ] If validation fails, set `deployment.status = "failed"`
+    - [ ] Store error in `deployment.error_message`
+    - [ ] Raise `HTTPException(400, detail=validation["error"])`
+    - [ ] If valid, store `runtime` and `version` in `mcp_config`
+  - [ ] Before encrypting credentials:
+    - [ ] Extract `required_env_vars` from `mcp_config.env_vars`
+    - [ ] Call `credential_validator.validate_credentials()`
+    - [ ] If validation fails, rollback transaction
+    - [ ] Raise `HTTPException(400, detail={"errors": validation["errors"]})`
+
+### 4. Enhanced Error Messages
+
+- [ ] **Update error response format in `deployments.py`**
+  - [ ] Create structured error response:
+    ```python
+    {
+      "error": "error_code",
+      "message": "Human-readable message",
+      "deployment_id": "uuid",
+      "package": "package_name",
+      "help": "Actionable guidance"
+    }
+    ```
+  - [ ] Implement `_get_error_help(error)` helper function
+    - [ ] Match error patterns (connect, timeout, not found, permission)
+    - [ ] Return contextual help text
+
+- [ ] **Add error types**
+  - [ ] `credential_validation_failed` - Missing/invalid credentials
+  - [ ] `package_not_found` - Package doesn't exist
+  - [ ] `package_validation_failed` - Network/registry error
+  - [ ] `deployment_failed` - Generic error
+
+### 5. Frontend Error Display
+
+- [ ] **Update `frontend/app/configure/page.tsx`** (line 73-76)
+  - [ ] Replace console.error with proper error state
+  - [ ] Create `ErrorMessage` component
+  - [ ] Display validation errors as list (if `errorDetail.errors` exists)
+  - [ ] Display help text (if `errorDetail.help` exists)
+  - [ ] Style with error colors (red badge, etc.)
+
+- [ ] **Create `frontend/components/ErrorMessage.tsx`** (optional)
+  - [ ] Accept `error` prop with structured error
+  - [ ] Render different layouts based on error type
+  - [ ] Show actionable guidance prominently
+
+### 6. Testing & Validation
+
+- [ ] **End-to-end testing**
+  - [ ] Deploy TickTick (valid npm, valid credentials) → Should succeed
+  - [ ] Deploy with invalid package name → Should fail with "not found" error
+  - [ ] Deploy with missing TICKTICK_TOKEN → Should fail listing missing credential
+  - [ ] Deploy Python server (e.g., mcp-server-git) → Should succeed
+  - [ ] Check frontend displays errors correctly
+
+- [ ] **Run test suite**
+  ```bash
+  cd backend
+  pytest tests/test_package_validator.py -v
+  pytest tests/test_credential_validator.py -v
+  pytest tests/test_deployments.py -v
+  ruff check .
+
+  cd ../frontend
+  bun run typecheck
+  bun run lint
+  ```
+
+### 7. Documentation Updates
+
+- [ ] **Update `context/CURRENT_STATUS.md`**
+  - [ ] Move "package validation" from "What's NOT Working" to "What Works"
+  - [ ] Add Phase 1 completion status
+  - [ ] Document new error codes
+
+- [ ] **Update `context/API_SPEC.md`** (if exists)
+  - [ ] Document new error response format
+  - [ ] Add validation error examples
+
+## Files to Create
+
+- `backend/app/services/package_validator.py` (NEW)
+- `backend/app/services/credential_validator.py` (NEW)
+- `backend/tests/test_package_validator.py` (NEW)
+- `backend/tests/test_credential_validator.py` (NEW)
+- `frontend/components/ErrorMessage.tsx` (NEW - optional)
+
+## Files to Modify
+
+- `backend/app/api/deployments.py` (lines 47-120)
+- `frontend/app/configure/page.tsx` (lines 73-76)
+- `context/CURRENT_STATUS.md`
+
+## Example Code
+
+### Package Validator
+
+```python
+# backend/app/services/package_validator.py
+import httpx
+from typing import Dict, Any
+
+class PackageValidator:
+    async def validate_npm_package(self, package: str) -> Dict[str, Any]:
+        """Check if npm package exists."""
+        encoded = package.replace("/", "%2F")
+        url = f"https://registry.npmjs.org/{encoded}"
+
+        try:
+            async with httpx.AsyncClient(timeout=5.0) as client:
+                response = await client.get(url)
+
+                if response.status_code == 200:
+                    data = response.json()
+                    return {
+                        "valid": True,
+                        "error": None,
+                        "version": data.get("dist-tags", {}).get("latest")
+                    }
+                elif response.status_code == 404:
+                    return {
+                        "valid": False,
+                        "error": f"Package '{package}' not found in npm registry",
+                        "version": None
+                    }
+                else:
+                    return {
+                        "valid": False,
+                        "error": f"npm registry error: {response.status_code}",
+                        "version": None
+                    }
+        except Exception as e:
+            return {
+                "valid": False,
+                "error": f"Failed to validate package: {str(e)}",
+                "version": None
+            }
+```
+
+### Deployment Integration
+
+```python
+# In backend/app/api/deployments.py (after line 48)
+package = mcp_config.get("package")
+
+if package:
+    # Validate package exists
+    validator = PackageValidator()
+
+    # Detect runtime
+    if package.startswith("@") or "/" in package:
+        validation = await validator.validate_npm_package(package)
+        runtime = "npm"
+    elif "." in package or "_" in package:
+        validation = await validator.validate_python_package(package)
+        runtime = "python"
+    else:
+        deployment.status = "failed"
+        deployment.error_message = f"Cannot determine runtime for package: {package}"
+        await db.commit()
+        raise HTTPException(400, detail=deployment.error_message)
+
+    if not validation["valid"]:
+        deployment.status = "failed"
+        deployment.error_message = validation["error"]
+        await db.commit()
+        raise HTTPException(400, detail={
+            "error": "package_not_found",
+            "message": validation["error"],
+            "package": package,
+            "help": "Verify the package name is correct and published to npm/PyPI"
+        })
+
+    # Store validated info
+    deployment.schedule_config["mcp_config"]["runtime"] = runtime
+    deployment.schedule_config["mcp_config"]["version"] = validation["version"]
+    await db.commit()
+```
+
+## Testing Scenarios
+
+### Test 1: Valid npm package
+```bash
+curl -X POST https://catwalk-live-backend-dev.fly.dev/api/deployments \
+  -H "Content-Type: application/json" \
+  -d '{
+    "name": "TickTick Test",
+    "schedule_config": {
+      "mcp_config": {
+        "package": "@alexarevalo.ai/mcp-server-ticktick",
+        "env_vars": [{"name": "TICKTICK_TOKEN", "required": true}]
+      }
+    },
+    "credentials": {
+      "env_TICKTICK_TOKEN": "test_token"
+    }
+  }'
+
+# Expected: 201 Created (deployment proceeds)
+```
+
+### Test 2: Invalid package
+```bash
+curl -X POST https://catwalk-live-backend-dev.fly.dev/api/deployments \
+  -H "Content-Type: application/json" \
+  -d '{
+    "name": "Invalid Test",
+    "schedule_config": {
+      "mcp_config": {
+        "package": "this-package-does-not-exist-xyz"
+      }
+    },
+    "credentials": {}
+  }'
+
+# Expected: 400 Bad Request
+# {
+#   "error": "package_not_found",
+#   "message": "Package 'this-package-does-not-exist-xyz' not found in npm registry",
+#   "package": "this-package-does-not-exist-xyz",
+#   "help": "Verify the package name is correct and published to npm/PyPI"
+# }
+```
+
+### Test 3: Missing required credential
+```bash
+curl -X POST https://catwalk-live-backend-dev.fly.dev/api/deployments \
+  -H "Content-Type: application/json" \
+  -d '{
+    "name": "Missing Creds Test",
+    "schedule_config": {
+      "mcp_config": {
+        "package": "@alexarevalo.ai/mcp-server-ticktick",
+        "env_vars": [
+          {"name": "TICKTICK_TOKEN", "required": true},
+          {"name": "TICKTICK_USER", "required": true}
+        ]
+      }
+    },
+    "credentials": {
+      "env_TICKTICK_TOKEN": "test_token"
+    }
+  }'
+
+# Expected: 400 Bad Request
+# {
+#   "error": "credential_validation_failed",
+#   "errors": ["Required credential 'TICKTICK_USER' is missing or empty"]
+# }
+```
+
+## Risk Mitigation
+
+### Risk: Private npm packages fail validation
+**Solution**: Add "Skip validation" checkbox in frontend (advanced users only)
+**Implementation**: Pass `skip_validation: true` in request, bypass validator
+
+### Risk: npm/PyPI API rate limits
+**Solution**: Cache validation results for 1 hour
+**Implementation**: Add `@cache(ttl=3600)` decorator to validator methods
+
+### Risk: False negatives (package exists but isn't MCP server)
+**Solution**: Phase 2 health monitoring will catch this
+**Note**: Validation only checks package existence, not MCP compatibility
+
+## Deployment Steps
+
+1. **Create feature branch**
+   ```bash
+   git checkout -b phase-1-validation
+   ```
+
+2. **Implement backend changes**
+   ```bash
+   cd backend
+   # Create validators
+   # Update deployments.py
+   # Write tests
+   pytest -v
+   ruff check .
+   ```
+
+3. **Implement frontend changes**
+   ```bash
+   cd frontend
+   # Update configure/page.tsx
+   # Create ErrorMessage component
+   bun run typecheck
+   bun run lint
+   ```
+
+4. **Deploy to Fly.io**
+   ```bash
+   cd backend
+   fly deploy --app catwalk-live-backend-dev
+   fly logs --app catwalk-live-backend-dev  # Monitor for errors
+   ```
+
+5. **Test end-to-end**
+   - Deploy valid server (TickTick)
+   - Deploy invalid package
+   - Deploy with missing credentials
+   - Verify error messages in frontend
+
+6. **Create git tag**
+   ```bash
+   git add .
+   git commit -m "feat: add package and credential validation (Phase 1)"
+   git tag phase-1-complete
+   git push origin phase-1-validation
+   git push origin phase-1-complete
+   ```
+
+## Next Phase
+
+After Phase 1 is complete and validated, proceed to **Phase 2: Health Monitoring & Status**.
+
+See `phase-2-monitoring.md` for details.
diff --git a/context/plans/roadmap/phase-2-monitoring.md b/context/plans/roadmap/phase-2-monitoring.md
new file mode 100644
index 0000000..2f291ef
--- /dev/null
+++ b/context/plans/roadmap/phase-2-monitoring.md
@@ -0,0 +1,444 @@
+# Phase 2: Health Monitoring & Status Tracking
+**Duration**: 2-3 weeks
+**Priority**: P0 (Blocking)
+**Goal**: Know when deployments are actually working vs. just "running"
+
+## Overview
+
+Currently, deployments are marked "running" immediately after Fly machine creation, even if:
+- npm install fails
+- Package doesn't exist
+- Server crashes on startup
+- Environment variables are wrong
+
+This phase adds:
+- Background health monitoring (polls `/status` every 30s)
+- Accurate deployment status (pending → installing → starting → running → unhealthy)
+- Progress tracking during deployment
+- Frontend status indicators and auto-refresh
+
+## Success Criteria
+
+- ✅ Unhealthy deployments detected within 60 seconds
+- ✅ Deployment status accurately reflects machine state
+- ✅ Users see progress during deployment ("Installing packages...")
+- ✅ Health checks distinguish "stopped" (serverless) vs "unhealthy" (broken)
+
+## Implementation Checklist
+
+### 1. Deployment Status Enum
+
+- [ ] **Update `backend/app/models/deployment.py`** (line 28)
+  - [ ] Import `Enum` from Python's enum module
+  - [ ] Create `DeploymentStatus` enum:
+    ```python
+    class DeploymentStatus(str, Enum):
+        PENDING = "pending"
+        INSTALLING = "installing"
+        STARTING = "starting"
+        RUNNING = "running"
+        UNHEALTHY = "unhealthy"
+        STOPPED = "stopped"
+        FAILED = "failed"
+    ```
+  - [ ] Update `status` column to use enum
+  - [ ] Add `progress_message` column (nullable string)
+
+- [ ] **Create Alembic migration**
+  ```bash
+  cd backend
+  alembic revision --autogenerate -m "add deployment status enum"
+  ```
+  - [ ] Review generated migration
+  - [ ] Test locally: `alembic upgrade head`
+  - [ ] Verify enum values in database
+
+### 2. Health Monitor Service
+
+- [ ] **Create `backend/app/services/health_monitor.py`**
+  - [ ] Implement `HealthMonitor` class
+  - [ ] Add `__init__` method (initialize state)
+  - [ ] Add `start(db_session_factory)` async method:
+    - [ ] Set `self.running = True`
+    - [ ] Enter infinite loop: `while self.running:`
+    - [ ] Query deployments with `machine_id IS NOT NULL`
+    - [ ] Filter by status: `["running", "unhealthy"]`
+    - [ ] Check each deployment in parallel with `asyncio.gather()`
+    - [ ] Sleep 30 seconds between checks
+  - [ ] Add `_check_deployment_health(db, deployment)` async method:
+    - [ ] Build machine URL: `http://{machine_id}.vm.{app}.internal:8080/status`
+    - [ ] Try GET request with 5-second timeout
+    - [ ] If 200 OK → update status to "running"
+    - [ ] If non-200 → update status to "unhealthy"
+    - [ ] If ConnectError → query Fly Machines API
+      - [ ] If machine state is "stopped" → status "stopped"
+      - [ ] If machine exists but unreachable → status "unhealthy"
+    - [ ] Commit changes to database
+
+- [ ] **Write tests for health monitor**
+  - [ ] Test healthy deployment → status remains "running"
+  - [ ] Test unhealthy deployment (500 response) → status becomes "unhealthy"
+  - [ ] Test stopped machine → status becomes "stopped"
+  - [ ] Test unreachable machine → status becomes "unhealthy"
+
+### 3. App Startup Integration
+
+- [ ] **Update `backend/app/main.py`**
+  - [ ] Import `asynccontextmanager` from contextlib
+  - [ ] Import `HealthMonitor` from services
+  - [ ] Create `lifespan` context manager:
+    ```python
+    @asynccontextmanager
+    async def lifespan(app: FastAPI):
+        health_monitor = HealthMonitor()
+        health_task = asyncio.create_task(
+            health_monitor.start(get_db_session_factory())
+        )
+        yield
+        health_monitor.running = False
+        await health_task
+    ```
+  - [ ] Update `FastAPI` instantiation: `app = FastAPI(lifespan=lifespan)`
+
+- [ ] **Test health monitor startup**
+  ```bash
+  cd backend
+  uvicorn app.main:app --reload
+  # Check logs for "HealthMonitor started"
+  ```
+
+### 4. Deployment Progress Tracking
+
+- [ ] **Update `backend/app/api/deployments.py`** (line 79-90)
+  - [ ] After creating deployment record:
+    - [ ] Set `deployment.status = DeploymentStatus.INSTALLING`
+    - [ ] Set `deployment.progress_message = "Creating Fly machine..."`
+    - [ ] Commit to database
+  - [ ] After `fly_service.create_machine()` succeeds:
+    - [ ] Set `deployment.status = DeploymentStatus.STARTING`
+    - [ ] Set `deployment.progress_message = "Installing packages and starting server..."`
+    - [ ] Commit to database
+  - [ ] Implement `wait_for_health(machine_id, timeout)` helper:
+    - [ ] Poll machine `/status` every 2 seconds
+    - [ ] Return `True` if 200 OK within timeout
+    - [ ] Return `False` if timeout exceeded
+  - [ ] After machine creation:
+    - [ ] Call `wait_for_health(machine_id, timeout=60)`
+    - [ ] If healthy → status "running", clear progress_message
+    - [ ] If timeout → status "unhealthy", error_message "failed to start"
+
+### 5. Frontend Status Display
+
+- [ ] **Create `frontend/components/DeploymentProgress.tsx`**
+  - [ ] Accept `deployment` prop
+  - [ ] Define step mapping:
+    ```typescript
+    const steps = {
+      pending: 0,
+      installing: 33,
+      starting: 66,
+      running: 100
+    };
+    ```
+  - [ ] Render progress bar (0-100% width)
+  - [ ] Display `progress_message` if present
+  - [ ] Style with Tailwind (bg-blue-500 for progress)
+
+- [ ] **Create `frontend/components/StatusBadge.tsx`**
+  - [ ] Accept `status` prop
+  - [ ] Define colors:
+    ```typescript
+    const colors = {
+      running: "bg-green-500",
+      unhealthy: "bg-red-500",
+      pending: "bg-yellow-500",
+      installing: "bg-blue-500",
+      starting: "bg-blue-500",
+      stopped: "bg-gray-500",
+      failed: "bg-red-500"
+    };
+    ```
+  - [ ] Render badge with status text and color
+
+- [ ] **Update `frontend/app/dashboard/page.tsx`**
+  - [ ] Import `StatusBadge` and `DeploymentProgress`
+  - [ ] Add auto-refresh logic:
+    ```typescript
+    refetchInterval: (data) => {
+      const hasActive = data?.some(d =>
+        ["pending", "installing", "starting"].includes(d.status)
+      );
+      return hasActive ? 5000 : 30000;  // 5s if active, 30s otherwise
+    }
+    ```
+  - [ ] Render `StatusBadge` for each deployment
+  - [ ] Render `DeploymentProgress` for active deployments
+
+### 6. Fly Machines API Integration
+
+- [ ] **Update `backend/app/services/fly_deployment_service.py`**
+  - [ ] Add `get_machine(machine_id)` method:
+    ```python
+    async def get_machine(self, machine_id: str) -> Optional[Dict]:
+        url = f"{self.base_url}/apps/{self.app_name}/machines/{machine_id}"
+        async with httpx.AsyncClient() as client:
+            response = await client.get(url, headers=self._get_headers())
+            if response.status_code == 200:
+                return response.json()
+            return None
+    ```
+  - [ ] Use in health monitor to check machine state
+
+### 7. Testing & Validation
+
+- [ ] **End-to-end testing**
+  - [ ] Deploy TickTick server
+  - [ ] Watch status transitions:
+    ```bash
+    watch -n 2 'curl -s https://catwalk-live-backend-dev.fly.dev/api/deployments/{id} | jq ".status, .progress_message"'
+    ```
+  - [ ] Verify: pending → installing → starting → running
+  - [ ] Deploy server that fails to start (invalid package)
+  - [ ] Verify: pending → installing → starting → unhealthy
+  - [ ] Stop machine manually: `fly machine stop {machine_id}`
+  - [ ] Verify: running → stopped (not unhealthy)
+
+- [ ] **Run test suite**
+  ```bash
+  cd backend
+  pytest tests/test_health_monitor.py -v
+  pytest tests/test_deployments.py -v
+  ruff check .
+
+  cd ../frontend
+  bun run typecheck
+  bun run lint
+  ```
+
+### 8. Documentation Updates
+
+- [ ] **Update `context/CURRENT_STATUS.md`**
+  - [ ] Move "Health monitoring" from "What's NOT Working" to "What Works"
+  - [ ] Add Phase 2 completion status
+  - [ ] Document status enum values
+
+## Files to Create
+
+- `backend/app/services/health_monitor.py` (NEW)
+- `backend/tests/test_health_monitor.py` (NEW)
+- `backend/alembic/versions/xxx_add_deployment_status_enum.py` (NEW migration)
+- `frontend/components/DeploymentProgress.tsx` (NEW)
+- `frontend/components/StatusBadge.tsx` (NEW)
+
+## Files to Modify
+
+- `backend/app/models/deployment.py` (add status enum, progress_message column)
+- `backend/app/main.py` (add lifespan with health monitor)
+- `backend/app/api/deployments.py` (add progress tracking)
+- `backend/app/services/fly_deployment_service.py` (add get_machine method)
+- `frontend/app/dashboard/page.tsx` (add status display and auto-refresh)
+
+## Example Code
+
+### Health Monitor
+
+```python
+# backend/app/services/health_monitor.py
+import asyncio
+import httpx
+from sqlalchemy import select
+from app.models.deployment import Deployment, DeploymentStatus
+
+class HealthMonitor:
+    def __init__(self):
+        self.running = False
+        self.check_interval = 30
+
+    async def start(self, db_session_factory):
+        self.running = True
+
+        while self.running:
+            async with db_session_factory() as db:
+                result = await db.execute(
+                    select(Deployment).where(
+                        Deployment.machine_id.isnot(None),
+                        Deployment.status.in_([
+                            DeploymentStatus.RUNNING,
+                            DeploymentStatus.UNHEALTHY
+                        ])
+                    )
+                )
+                deployments = result.scalars().all()
+
+                await asyncio.gather(*[
+                    self._check_deployment_health(db, d)
+                    for d in deployments
+                ])
+
+            await asyncio.sleep(self.check_interval)
+
+    async def _check_deployment_health(self, db, deployment):
+        machine_url = f"http://{deployment.machine_id}.vm.{app_name}.internal:8080/status"
+
+        try:
+            async with httpx.AsyncClient(timeout=5.0) as client:
+                response = await client.get(machine_url)
+
+                if response.status_code == 200:
+                    if deployment.status != DeploymentStatus.RUNNING:
+                        deployment.status = DeploymentStatus.RUNNING
+                        deployment.error_message = None
+                        await db.commit()
+                else:
+                    if deployment.status != DeploymentStatus.UNHEALTHY:
+                        deployment.status = DeploymentStatus.UNHEALTHY
+                        deployment.error_message = f"Health check failed: {response.status_code}"
+                        await db.commit()
+
+        except httpx.ConnectError:
+            # Check if machine is stopped (serverless) or actually unhealthy
+            from app.services.fly_deployment_service import FlyDeploymentService
+            fly_service = FlyDeploymentService()
+            machine_info = await fly_service.get_machine(deployment.machine_id)
+
+            if machine_info and machine_info.get("state") == "stopped":
+                if deployment.status != DeploymentStatus.STOPPED:
+                    deployment.status = DeploymentStatus.STOPPED
+                    await db.commit()
+            else:
+                if deployment.status != DeploymentStatus.UNHEALTHY:
+                    deployment.status = DeploymentStatus.UNHEALTHY
+                    deployment.error_message = "Machine unreachable"
+                    await db.commit()
+```
+
+### Progress Tracking
+
+```python
+# In backend/app/api/deployments.py (after machine creation)
+if machine_id:
+    deployment.machine_id = machine_id
+    deployment.status = DeploymentStatus.STARTING
+    deployment.progress_message = "Installing packages and starting server..."
+    await db.commit()
+
+    # Wait for health check to pass
+    health_ok = await wait_for_health(machine_id, timeout=60)
+
+    if health_ok:
+        deployment.status = DeploymentStatus.RUNNING
+        deployment.progress_message = None
+        await db.commit()
+    else:
+        deployment.status = DeploymentStatus.UNHEALTHY
+        deployment.error_message = "Server failed to start within 60 seconds"
+        await db.commit()
+
+async def wait_for_health(machine_id: str, timeout: int) -> bool:
+    start_time = asyncio.get_event_loop().time()
+    machine_url = f"http://{machine_id}.vm.{app_name}.internal:8080/status"
+
+    while (asyncio.get_event_loop().time() - start_time) < timeout:
+        try:
+            async with httpx.AsyncClient(timeout=5.0) as client:
+                response = await client.get(machine_url)
+                if response.status_code == 200:
+                    return True
+        except httpx.ConnectError:
+            pass
+
+        await asyncio.sleep(2)
+
+    return False
+```
+
+## Testing Scenarios
+
+### Test 1: Healthy deployment
+1. Deploy TickTick with valid credentials
+2. Watch logs: `fly logs --app catwalk-live-backend-dev`
+3. Expected status transitions:
+   - pending (0s)
+   - installing (5s)
+   - starting (15s)
+   - running (30s)
+
+### Test 2: Unhealthy deployment
+1. Deploy with invalid package name
+2. Expected status transitions:
+   - pending → installing → starting → unhealthy
+3. Error message should contain "failed to start"
+
+### Test 3: Serverless stopped machine
+1. Deploy valid server, wait for "running"
+2. Stop machine: `fly machine stop {machine_id} --app {app_name}`
+3. Wait 60 seconds for health monitor to detect
+4. Expected: status transitions running → stopped (not unhealthy)
+
+## Risk Mitigation
+
+### Risk: Health monitor crashes and doesn't restart
+**Solution**: Implement watchdog process or use Fly.io restart policy
+**Mitigation**: Log health monitor heartbeat every 60s for monitoring
+
+### Risk: 30-second polling too slow
+**Solution**: Make check_interval configurable via environment variable
+**Future**: Add webhook support for instant notifications
+
+### Risk: False positives (stopped vs unhealthy)
+**Solution**: Query Fly Machines API to check actual machine state
+**Implemented**: In `_check_deployment_health` ConnectError handler
+
+## Deployment Steps
+
+1. **Create feature branch**
+   ```bash
+   git checkout -b phase-2-monitoring
+   ```
+
+2. **Implement backend changes**
+   ```bash
+   cd backend
+   # Create health_monitor.py
+   # Update models/deployment.py
+   # Generate migration
+   alembic upgrade head
+   # Test locally
+   pytest -v
+   ```
+
+3. **Implement frontend changes**
+   ```bash
+   cd frontend
+   # Create StatusBadge and DeploymentProgress components
+   # Update dashboard/page.tsx
+   bun run typecheck
+   ```
+
+4. **Deploy to Fly.io**
+   ```bash
+   cd backend
+   fly deploy --app catwalk-live-backend-dev
+   # Monitor for health monitor startup
+   fly logs --app catwalk-live-backend-dev | grep "HealthMonitor"
+   ```
+
+5. **Test end-to-end**
+   - Deploy test server
+   - Watch status transitions in frontend
+   - Verify health monitor detects failures
+
+6. **Create git tag**
+   ```bash
+   git add .
+   git commit -m "feat: add health monitoring and status tracking (Phase 2)"
+   git tag phase-2-complete
+   git push origin phase-2-monitoring
+   git push origin phase-2-complete
+   ```
+
+## Next Phase
+
+After Phase 2 is complete and validated, proceed to **Phase 3: Multi-Runtime Support**.
+
+See `phase-3-runtime.md` for details.
diff --git a/context/plans/roadmap/phase-3-runtime.md b/context/plans/roadmap/phase-3-runtime.md
new file mode 100644
index 0000000..de7d55f
--- /dev/null
+++ b/context/plans/roadmap/phase-3-runtime.md
@@ -0,0 +1,478 @@
+# Phase 3: Multi-Runtime Support (Python + npm)
+**Duration**: 2-3 weeks
+**Priority**: P0 (Blocking)
+**Goal**: Support Python MCP servers (covers 80% of Glama registry)
+
+## Overview
+
+Currently, the platform only supports npm packages via hardcoded `npx -y $MCP_PACKAGE` in Dockerfile.
+
+This excludes:
+- Python MCP servers (mcp-server-git, mcp-server-postgres, mcp-server-sqlite, etc.)
+- Custom run commands
+- GitHub-only repos
+
+This phase adds unified runtime support through:
+- Enhanced Dockerfile (Node.js + Python in single image)
+- Runtime detection from package analysis
+- Entrypoint script for dynamic command selection
+
+## Success Criteria
+
+- ✅ 85% of Python MCP servers deploy successfully
+- ✅ Runtime auto-detected from package format
+- ✅ Both npm and Python packages work in production
+- ✅ Dockerfile builds successfully on Fly.io
+
+## Implementation Checklist
+
+### 1. Enhanced Dockerfile
+
+- [ ] **Rewrite `deploy/Dockerfile`**
+  - [ ] Start with `FROM python:3.12-slim`
+  - [ ] Install Node.js 20:
+    ```dockerfile
+    RUN apt-get update && \
+        apt-get install -y curl && \
+        curl -fsSL https://deb.nodesource.com/setup_20.x | bash - && \
+        apt-get install -y nodejs && \
+        rm -rf /var/lib/apt/lists/*
+    ```
+  - [ ] Install mcp-proxy: `RUN pip install "mcp-proxy>=0.10.0"`
+  - [ ] Copy entrypoint script: `COPY entrypoint.sh /app/`
+  - [ ] Make executable: `RUN chmod +x /app/entrypoint.sh`
+  - [ ] Set CMD: `CMD ["/app/entrypoint.sh"]`
+
+- [ ] **Test Dockerfile locally**
+  ```bash
+  cd deploy
+  docker build -t catwalk-mcp-test .
+  docker run -e MCP_RUNTIME=npm -e MCP_PACKAGE=@modelcontextprotocol/server-memory catwalk-mcp-test
+  # Should start successfully
+  ```
+
+### 2. Entrypoint Script
+
+- [ ] **Create `deploy/entrypoint.sh`**
+  - [ ] Add shebang: `#!/bin/sh`
+  - [ ] Add `set -e` for error handling
+  - [ ] Validate `MCP_RUNTIME` is set
+  - [ ] Validate `MCP_PACKAGE` is set (unless runtime=custom)
+  - [ ] Implement runtime selection:
+    ```bash
+    case "$MCP_RUNTIME" in
+      npm)
+        echo "Starting npm package: $MCP_PACKAGE"
+        exec mcp-proxy --host=:: --port=8080 --pass-environment -- npx -y "$MCP_PACKAGE"
+        ;;
+      python)
+        echo "Starting Python module: $MCP_PACKAGE"
+        pip install --no-cache-dir "$MCP_PACKAGE" || true
+        exec mcp-proxy --host=:: --port=8080 --pass-environment -- python -m "$MCP_PACKAGE"
+        ;;
+      custom)
+        echo "Starting custom command: $MCP_COMMAND"
+        exec mcp-proxy --host=:: --port=8080 --pass-environment -- sh -c "$MCP_COMMAND"
+        ;;
+      *)
+        echo "Error: Invalid MCP_RUNTIME=$MCP_RUNTIME"
+        exit 1
+        ;;
+    esac
+    ```
+
+- [ ] **Test entrypoint script**
+  ```bash
+  # Test npm runtime
+  docker run -e MCP_RUNTIME=npm -e MCP_PACKAGE=@modelcontextprotocol/server-memory catwalk-mcp-test
+
+  # Test Python runtime
+  docker run -e MCP_RUNTIME=python -e MCP_PACKAGE=mcp-server-git catwalk-mcp-test
+
+  # Test invalid runtime
+  docker run -e MCP_RUNTIME=invalid catwalk-mcp-test
+  # Should exit with error
+  ```
+
+### 3. Runtime Detection in Analysis
+
+- [ ] **Update `backend/app/prompts/analysis_prompt.py`**
+  - [ ] Add "runtime" field to expected output:
+    ```python
+    "runtime": "string (npm, python, or custom)",
+    ```
+  - [ ] Update instructions to check package.json or setup.py
+  - [ ] Emphasize runtime detection importance
+
+- [ ] **Test analysis with Python repos**
+  ```bash
+  curl -X POST /api/analyze -d '{"repo_url": "https://github.com/modelcontextprotocol/servers/tree/main/src/git"}'
+  # Should return runtime: "python"
+  ```
+
+### 4. FlyDeploymentService Runtime Support
+
+- [ ] **Update `backend/app/services/fly_deployment_service.py`** (line 64-68)
+  - [ ] Extract runtime from mcp_config:
+    ```python
+    runtime = mcp_config.get("runtime", "npm")  # Default npm for backwards compat
+    ```
+  - [ ] Add runtime to environment variables:
+    ```python
+    env["MCP_RUNTIME"] = runtime
+    env["MCP_PACKAGE"] = package
+    ```
+  - [ ] For custom runtime, add MCP_COMMAND:
+    ```python
+    if runtime == "custom":
+        env["MCP_COMMAND"] = mcp_config.get("run_command", "")
+    ```
+
+- [ ] **Test Fly machine creation**
+  ```bash
+  # Check that MCP_RUNTIME is set in machine env
+  fly machine list --app catwalk-live-mcp-servers
+  fly machine show {machine_id}
+  # Verify env vars include MCP_RUNTIME
+  ```
+
+### 5. Frontend Runtime Indicator
+
+- [ ] **Create `frontend/components/RuntimeBadge.tsx`**
+  - [ ] Accept `runtime` prop
+  - [ ] Define colors and icons:
+    ```typescript
+    const styles = {
+      npm: { bg: "bg-red-500", icon: "📦" },
+      python: { bg: "bg-blue-500", icon: "🐍" },
+      custom: { bg: "bg-purple-500", icon: "⚙️" }
+    };
+    ```
+  - [ ] Render badge with icon and runtime name
+
+- [ ] **Update `frontend/components/DeploymentCard.tsx`**
+  - [ ] Import `RuntimeBadge`
+  - [ ] Extract runtime from `deployment.schedule_config.mcp_config.runtime`
+  - [ ] Render `<RuntimeBadge runtime={runtime} />` next to status
+
+### 6. Python Package Validation
+
+- [ ] **Verify `PackageValidator.validate_python_package` exists** (from Phase 1)
+  - [ ] Should check `https://pypi.org/pypi/{package}/json`
+  - [ ] Should return same format as npm validation
+
+- [ ] **Update deployment validation** (in `deployments.py`)
+  - [ ] For Python packages, call `validate_python_package()`
+  - [ ] Detect runtime from package format:
+    ```python
+    if package.startswith("@") or "/" in package:
+        runtime = "npm"
+    elif "." in package or "_" in package:
+        runtime = "python"
+    else:
+        runtime = "npm"  # Default
+    ```
+
+### 7. Docker Image Build & Deployment
+
+- [ ] **Build and push new Docker image**
+  ```bash
+  cd deploy
+  docker build -t registry.fly.io/catwalk-live-mcp-servers:unified .
+  docker push registry.fly.io/catwalk-live-mcp-servers:unified
+  ```
+
+- [ ] **Update Fly.io secret**
+  ```bash
+  fly secrets set FLY_MCP_IMAGE=registry.fly.io/catwalk-live-mcp-servers:unified \
+    --app catwalk-live-backend-dev
+  ```
+
+- [ ] **Verify existing deployments still work**
+  - [ ] Test TickTick (npm) deployment
+  - [ ] Ensure backward compatibility
+
+### 8. .gitattributes for Line Endings
+
+- [ ] **Create/update `.gitattributes`** (in repository root)
+  ```
+  *.sh text eol=lf
+  ```
+  - [ ] Ensures entrypoint.sh always has LF line endings
+  - [ ] Prevents CRLF issues on Windows
+
+- [ ] **Verify line endings**
+  ```bash
+  file deploy/entrypoint.sh
+  # Should show: "shell script, ASCII text executable"
+  # NOT: "shell script, ASCII text executable, with CRLF line terminators"
+  ```
+
+### 9. Testing & Validation
+
+- [ ] **End-to-end testing - npm packages**
+  - [ ] Deploy TickTick (@alexarevalo.ai/mcp-server-ticktick)
+  - [ ] Deploy filesystem (@modelcontextprotocol/server-filesystem)
+  - [ ] Verify both work correctly
+
+- [ ] **End-to-end testing - Python packages**
+  - [ ] Deploy git server (mcp-server-git)
+  - [ ] Deploy sqlite server (mcp-server-sqlite)
+  - [ ] Verify both install and start correctly
+
+- [ ] **Check container logs**
+  ```bash
+  # For Python deployment
+  fly logs --app catwalk-live-mcp-{deployment-id} | grep "Starting Python"
+  # Should see: "Starting Python module: mcp-server-git"
+
+  # For npm deployment
+  fly logs --app catwalk-live-mcp-{deployment-id} | grep "Starting npm"
+  # Should see: "Starting npm package: @alexarevalo.ai/mcp-server-ticktick"
+  ```
+
+- [ ] **Run test suite**
+  ```bash
+  cd backend
+  pytest -v
+  ruff check .
+
+  cd ../frontend
+  bun run typecheck
+  bun run lint
+  ```
+
+### 10. Documentation Updates
+
+- [ ] **Update `context/CURRENT_STATUS.md`**
+  - [ ] Add "Python MCP servers" to "What Works"
+  - [ ] Update Phase 3 completion status
+  - [ ] Document runtime support
+
+- [ ] **Update `context/TECH_STACK.md`** (if exists)
+  - [ ] Add runtime support details
+  - [ ] Document Docker image composition
+
+## Files to Create
+
+- `deploy/entrypoint.sh` (NEW)
+- `.gitattributes` (NEW or UPDATE)
+- `frontend/components/RuntimeBadge.tsx` (NEW)
+
+## Files to Modify
+
+- `deploy/Dockerfile` (complete rewrite)
+- `backend/app/prompts/analysis_prompt.py` (add runtime field)
+- `backend/app/services/fly_deployment_service.py` (add runtime env vars)
+- `frontend/components/DeploymentCard.tsx` (add runtime badge)
+
+## Example Code
+
+### Dockerfile
+
+```dockerfile
+# deploy/Dockerfile
+FROM python:3.12-slim
+
+WORKDIR /app
+
+# Install Node.js 20 for npm packages
+RUN apt-get update && \
+    apt-get install -y curl && \
+    curl -fsSL https://deb.nodesource.com/setup_20.x | bash - && \
+    apt-get install -y nodejs && \
+    rm -rf /var/lib/apt/lists/*
+
+# Install mcp-proxy
+RUN pip install --no-cache-dir "mcp-proxy>=0.10.0"
+
+# Copy and set up entrypoint
+COPY entrypoint.sh /app/entrypoint.sh
+RUN chmod +x /app/entrypoint.sh
+
+EXPOSE 8080
+
+CMD ["/app/entrypoint.sh"]
+```
+
+### Entrypoint Script
+
+```bash
+#!/bin/sh
+set -e
+
+# Validate required environment variables
+if [ -z "$MCP_RUNTIME" ]; then
+    echo "Error: MCP_RUNTIME environment variable is required"
+    exit 1
+fi
+
+if [ -z "$MCP_PACKAGE" ] && [ "$MCP_RUNTIME" != "custom" ]; then
+    echo "Error: MCP_PACKAGE environment variable is required"
+    exit 1
+fi
+
+# Runtime selection
+case "$MCP_RUNTIME" in
+  npm)
+    echo "Starting npm package: $MCP_PACKAGE"
+    exec mcp-proxy --host=:: --port=8080 --pass-environment -- npx -y "$MCP_PACKAGE"
+    ;;
+  python)
+    echo "Starting Python module: $MCP_PACKAGE"
+    # Install package if not already present
+    pip install --no-cache-dir "$MCP_PACKAGE" || true
+    exec mcp-proxy --host=:: --port=8080 --pass-environment -- python -m "$MCP_PACKAGE"
+    ;;
+  custom)
+    if [ -z "$MCP_COMMAND" ]; then
+        echo "Error: MCP_COMMAND required for custom runtime"
+        exit 1
+    fi
+    echo "Starting custom command: $MCP_COMMAND"
+    exec mcp-proxy --host=:: --port=8080 --pass-environment -- sh -c "$MCP_COMMAND"
+    ;;
+  *)
+    echo "Error: Invalid MCP_RUNTIME=$MCP_RUNTIME (must be npm, python, or custom)"
+    exit 1
+    ;;
+esac
+```
+
+## Testing Scenarios
+
+### Test 1: npm package (TickTick)
+```bash
+# Deploy via frontend or API
+{
+  "name": "TickTick npm",
+  "schedule_config": {
+    "mcp_config": {
+      "package": "@alexarevalo.ai/mcp-server-ticktick",
+      "runtime": "npm"
+    }
+  },
+  "credentials": {"env_TICKTICK_TOKEN": "..."}
+}
+
+# Expected container logs:
+# "Starting npm package: @alexarevalo.ai/mcp-server-ticktick"
+# "mcp-proxy listening on [::]:8080"
+```
+
+### Test 2: Python package (git server)
+```bash
+{
+  "name": "Git Python",
+  "schedule_config": {
+    "mcp_config": {
+      "package": "mcp-server-git",
+      "runtime": "python"
+    }
+  },
+  "credentials": {}
+}
+
+# Expected container logs:
+# "Starting Python module: mcp-server-git"
+# "Collecting mcp-server-git..."
+# "mcp-proxy listening on [::]:8080"
+```
+
+### Test 3: Invalid runtime
+```bash
+{
+  "schedule_config": {
+    "mcp_config": {
+      "package": "test",
+      "runtime": "java"  # Invalid
+    }
+  }
+}
+
+# Expected container logs:
+# "Error: Invalid MCP_RUNTIME=java"
+# Container should exit with code 1
+```
+
+## Risk Mitigation
+
+### Risk: Larger Docker image size
+**Impact**: ~500MB (Node + Python) vs ~200MB (npm only)
+**Mitigation**: Future optimization with multi-stage builds
+**Acceptable**: For MVP, simplicity > size
+
+### Risk: CRLF line endings break entrypoint.sh
+**Mitigation**: Add .gitattributes rule for `*.sh text eol=lf`
+**Validation**: Test on Windows machine before deployment
+
+### Risk: pip install fails during container startup
+**Mitigation**: `pip install ... || true` allows container to start
+**Detection**: Health monitor will mark deployment unhealthy if server doesn't start
+
+### Risk: Python version incompatibility
+**Mitigation**: Use Python 3.12 (latest stable)
+**Future**: Allow users to specify Python version
+
+## Deployment Steps
+
+1. **Create feature branch**
+   ```bash
+   git checkout -b phase-3-runtime
+   ```
+
+2. **Implement changes**
+   ```bash
+   # Create entrypoint.sh
+   # Update Dockerfile
+   # Update analysis prompt
+   # Update FlyDeploymentService
+   # Create .gitattributes
+   ```
+
+3. **Build and test Docker image locally**
+   ```bash
+   cd deploy
+   docker build -t catwalk-mcp-test .
+   # Test npm runtime
+   docker run -e MCP_RUNTIME=npm -e MCP_PACKAGE=@modelcontextprotocol/server-memory catwalk-mcp-test
+   # Test Python runtime
+   docker run -e MCP_RUNTIME=python -e MCP_PACKAGE=mcp-server-git catwalk-mcp-test
+   ```
+
+4. **Push image to Fly.io registry**
+   ```bash
+   docker tag catwalk-mcp-test registry.fly.io/catwalk-live-mcp-servers:unified
+   docker push registry.fly.io/catwalk-live-mcp-servers:unified
+   ```
+
+5. **Update backend secret**
+   ```bash
+   fly secrets set FLY_MCP_IMAGE=registry.fly.io/catwalk-live-mcp-servers:unified --app catwalk-live-backend-dev
+   ```
+
+6. **Deploy backend**
+   ```bash
+   cd backend
+   fly deploy --app catwalk-live-backend-dev
+   ```
+
+7. **Test end-to-end**
+   - Deploy npm server (TickTick)
+   - Deploy Python server (git)
+   - Verify both work in Claude
+
+8. **Create git tag**
+   ```bash
+   git add .
+   git commit -m "feat: add multi-runtime support (npm + Python) (Phase 3)"
+   git tag phase-3-complete
+   git push origin phase-3-runtime
+   git push origin phase-3-complete
+   ```
+
+## Next Phase
+
+After Phase 3 is complete and validated, proceed to **Phase 4: Observability (Container Logs)**.
+
+See `phase-4-observability.md` for details.
diff --git a/context/plans/roadmap/phase-4-observability.md b/context/plans/roadmap/phase-4-observability.md
new file mode 100644
index 0000000..fcbb070
--- /dev/null
+++ b/context/plans/roadmap/phase-4-observability.md
@@ -0,0 +1,486 @@
+# Phase 4: Observability - Container Logs & Diagnostics
+**Duration**: 3-4 weeks
+**Priority**: P1 (High)
+**Goal**: Surface container logs so users can debug deployment failures
+
+## Overview
+
+Currently, when deployments fail, users have no visibility into:
+- Package installation progress
+- npm/pip errors
+- Server startup failures
+- Runtime errors
+
+This phase adds:
+- PostgreSQL log storage (last 1000 lines per deployment)
+- Fly.io log collection service
+- Historical log viewing API
+- Real-time WebSocket log streaming
+- Frontend log viewer component
+
+## Success Criteria
+
+- ✅ 100% of deployments have accessible logs
+- ✅ Users can view last 1000 lines of container output
+- ✅ Real-time log streaming during deployment
+- ✅ Logs help diagnose 90% of failures
+
+## Implementation Checklist
+
+### 1. Database Schema for Logs
+
+- [ ] **Create Alembic migration for `deployment_logs` table**
+  ```bash
+  cd backend
+  alembic revision -m "add deployment logs table"
+  ```
+
+- [ ] **Edit migration file** (`backend/alembic/versions/xxx_add_deployment_logs.py`)
+  - [ ] Add create_table:
+    ```python
+    op.create_table(
+        'deployment_logs',
+        sa.Column('id', UUID, primary_key=True, server_default=sa.text('gen_random_uuid()')),
+        sa.Column('deployment_id', UUID, sa.ForeignKey('deployments.id', ondelete='CASCADE'), nullable=False),
+        sa.Column('timestamp', sa.DateTime(timezone=True), nullable=False, server_default=sa.func.now()),
+        sa.Column('stream', sa.String(10), nullable=False),
+        sa.Column('message', sa.Text, nullable=False),
+        sa.CheckConstraint("stream IN ('stdout', 'stderr')", name='check_stream_type')
+    )
+    ```
+  - [ ] Add index:
+    ```python
+    op.create_index('idx_deployment_logs_timestamp', 'deployment_logs', ['deployment_id', 'timestamp'])
+    ```
+
+- [ ] **Apply migration**
+  ```bash
+  alembic upgrade head
+  # Verify table exists
+  fly postgres connect --app catwalk-live-db-dev
+  \d deployment_logs
+  ```
+
+### 2. DeploymentLog Model
+
+- [ ] **Create `backend/app/models/deployment_log.py`**
+  - [ ] Define DeploymentLog model:
+    ```python
+    class DeploymentLog(Base):
+        __tablename__ = "deployment_logs"
+
+        id: Mapped[uuid.UUID] = mapped_column(UUID, primary_key=True, server_default=text("gen_random_uuid()"))
+        deployment_id: Mapped[uuid.UUID] = mapped_column(UUID, ForeignKey("deployments.id", ondelete="CASCADE"))
+        timestamp: Mapped[datetime] = mapped_column(DateTime(timezone=True), server_default=func.now())
+        stream: Mapped[str] = mapped_column(String(10))
+        message: Mapped[str] = mapped_column(Text)
+
+        deployment: Mapped["Deployment"] = relationship(back_populates="logs")
+    ```
+
+- [ ] **Update `backend/app/models/deployment.py`**
+  - [ ] Add relationship: `logs: Mapped[List["DeploymentLog"]] = relationship(back_populates="deployment")`
+
+- [ ] **Update `backend/app/models/__init__.py`**
+  - [ ] Import DeploymentLog: `from .deployment_log import DeploymentLog`
+
+### 3. Log Collection Service
+
+- [ ] **Create `backend/app/services/log_collector.py`**
+  - [ ] Implement `LogCollector` class
+  - [ ] Add `tail_machine_logs(deployment_id, machine_id, db)` async method:
+    - [ ] Build Fly Machines API logs URL:
+      ```python
+      url = f"https://api.machines.dev/v1/apps/{app_name}/machines/{machine_id}/logs"
+      ```
+    - [ ] Stream logs with httpx:
+      ```python
+      async with client.stream("GET", url, headers=auth_headers) as stream:
+          async for line in stream.aiter_lines():
+              # Parse Fly log format
+              log_entry = parse_fly_log(line)
+              # Store in database
+              await store_log(db, deployment_id, log_entry)
+      ```
+  - [ ] Add `parse_fly_log(line)` helper:
+    - [ ] Parse format: `timestamp stream message`
+    - [ ] Return `{"timestamp": dt, "stream": str, "message": str}`
+  - [ ] Add `store_log(db, deployment_id, log_entry)` helper:
+    - [ ] Insert into deployment_logs table
+    - [ ] Implement retention (keep last 1000 lines):
+      ```python
+      await db.execute(
+          delete(DeploymentLog)
+          .where(DeploymentLog.deployment_id == deployment_id)
+          .where(
+              DeploymentLog.id.not_in(
+                  select(DeploymentLog.id)
+                  .where(DeploymentLog.deployment_id == deployment_id)
+                  .order_by(DeploymentLog.timestamp.desc())
+                  .limit(1000)
+              )
+          )
+      )
+      ```
+
+- [ ] **Test log collection locally** (mock Fly API response)
+
+### 4. Log API Endpoints
+
+- [ ] **Update `backend/app/api/deployments.py`**
+  - [ ] Add `GET /deployments/{deployment_id}/logs` endpoint:
+    ```python
+    @router.get("/{deployment_id}/logs")
+    async def get_deployment_logs(
+        deployment_id: str,
+        limit: int = Query(100, le=1000),
+        stream: Optional[str] = Query(None, regex="^(stdout|stderr)$"),
+        db: AsyncSession = Depends(get_db)
+    ):
+        query = (
+            select(DeploymentLog)
+            .where(DeploymentLog.deployment_id == deployment_id)
+            .order_by(DeploymentLog.timestamp.desc())
+            .limit(limit)
+        )
+        if stream:
+            query = query.where(DeploymentLog.stream == stream)
+
+        result = await db.execute(query)
+        logs = result.scalars().all()
+
+        return {
+            "logs": [
+                {
+                    "timestamp": log.timestamp.isoformat(),
+                    "stream": log.stream,
+                    "message": log.message
+                }
+                for log in reversed(logs)  # Oldest first
+            ]
+        }
+    ```
+
+- [ ] **Add WebSocket endpoint for streaming** (optional for Phase 4)
+  - [ ] `GET /deployments/{deployment_id}/logs/stream`
+  - [ ] WebSocket implementation:
+    ```python
+    @router.websocket("/{deployment_id}/logs/stream")
+    async def stream_deployment_logs(
+        websocket: WebSocket,
+        deployment_id: str
+    ):
+        await websocket.accept()
+        # Subscribe to log events for this deployment
+        # (Requires event broadcasting system)
+    ```
+  - [ ] Note: May defer WebSocket to future phase if complex
+
+### 5. Integrate Log Collection into Deployment Flow
+
+- [ ] **Update `backend/app/api/deployments.py`** (after machine creation)
+  - [ ] Start log collection asynchronously:
+    ```python
+    if machine_id:
+        # Start collecting logs in background
+        log_collector = LogCollector()
+        asyncio.create_task(
+            log_collector.tail_machine_logs(
+                deployment_id=str(deployment.id),
+                machine_id=machine_id,
+                db=db
+            )
+        )
+    ```
+  - [ ] Note: Background task runs independently of HTTP request
+
+### 6. Frontend Log Viewer Component
+
+- [ ] **Create `frontend/components/LogViewer.tsx`**
+  - [ ] Accept `deploymentId` prop
+  - [ ] Fetch logs on mount:
+    ```typescript
+    useEffect(() => {
+        getDeploymentLogs(deploymentId, 100).then(setLogs);
+    }, [deploymentId]);
+    ```
+  - [ ] Render terminal-style log display:
+    - [ ] Black background (`bg-gray-900`)
+    - [ ] Monospace font (`font-mono`)
+    - [ ] Color-coded streams (stdout: gray, stderr: red)
+    - [ ] Timestamps in gray
+    - [ ] Auto-scroll to bottom
+  - [ ] Add refresh button
+  - [ ] Add stream filter (all / stdout / stderr)
+
+- [ ] **Update `frontend/app/dashboard/[id]/page.tsx`** (deployment detail page)
+  - [ ] Import LogViewer
+  - [ ] Render `<LogViewer deploymentId={id} />` in tab or section
+
+- [ ] **Add to `frontend/lib/api.ts`**
+  - [ ] Create `getDeploymentLogs(deploymentId, limit)` function:
+    ```typescript
+    export async function getDeploymentLogs(
+        deploymentId: string,
+        limit: number = 100
+    ): Promise<LogEntry[]> {
+        const res = await fetch(
+            `/api/deployments/${deploymentId}/logs?limit=${limit}`
+        );
+        if (!res.ok) throw new Error("Failed to fetch logs");
+        const data = await res.json();
+        return data.logs;
+    }
+    ```
+
+### 7. Fly.io Logs API Integration
+
+- [ ] **Update `backend/app/services/fly_deployment_service.py`**
+  - [ ] Add `get_machine_logs(machine_id)` method:
+    ```python
+    async def get_machine_logs(self, machine_id: str):
+        """Stream logs from Fly machine."""
+        url = f"{self.base_url}/apps/{self.app_name}/machines/{machine_id}/logs"
+        headers = self._get_headers()
+
+        async with httpx.AsyncClient() as client:
+            async with client.stream("GET", url, headers=headers, timeout=None) as response:
+                async for line in response.aiter_lines():
+                    yield line
+    ```
+
+- [ ] **Test Fly logs API access**
+  ```bash
+  curl -H "Authorization: Bearer $FLY_API_TOKEN" \
+    https://api.machines.dev/v1/apps/catwalk-live-mcp-servers/machines/{machine_id}/logs
+  ```
+
+### 8. Testing & Validation
+
+- [ ] **Test log storage**
+  - [ ] Deploy test server
+  - [ ] Check database for logs:
+    ```sql
+    SELECT COUNT(*) FROM deployment_logs WHERE deployment_id = '{id}';
+    ```
+  - [ ] Verify last 1000 lines retained
+
+- [ ] **Test log API**
+  ```bash
+  curl https://catwalk-live-backend-dev.fly.dev/api/deployments/{id}/logs?limit=50
+  # Should return last 50 log entries
+  ```
+
+- [ ] **Test frontend log viewer**
+  - [ ] Visit deployment detail page
+  - [ ] Verify logs displayed
+  - [ ] Test stream filter (stdout/stderr/all)
+  - [ ] Test refresh button
+
+- [ ] **End-to-end test: Failed deployment**
+  - [ ] Deploy server with invalid package
+  - [ ] Check logs show npm/pip error
+  - [ ] Verify error visible in frontend log viewer
+
+### 9. Documentation Updates
+
+- [ ] **Update `context/CURRENT_STATUS.md`**
+  - [ ] Move "Container logs" from "What's NOT Working" to "What Works"
+  - [ ] Add Phase 4 completion status
+
+- [ ] **Update `context/API_SPEC.md`**
+  - [ ] Document `GET /deployments/{id}/logs` endpoint
+  - [ ] Add example responses
+
+## Files to Create
+
+- `backend/app/models/deployment_log.py` (NEW)
+- `backend/app/services/log_collector.py` (NEW)
+- `backend/alembic/versions/xxx_add_deployment_logs.py` (NEW migration)
+- `frontend/components/LogViewer.tsx` (NEW)
+- `frontend/app/dashboard/[id]/page.tsx` (NEW - deployment detail page)
+
+## Files to Modify
+
+- `backend/app/models/deployment.py` (add logs relationship)
+- `backend/app/models/__init__.py` (import DeploymentLog)
+- `backend/app/api/deployments.py` (add log endpoints, start collection)
+- `backend/app/services/fly_deployment_service.py` (add get_machine_logs)
+- `frontend/lib/api.ts` (add getDeploymentLogs function)
+
+## Example Code
+
+### Log Viewer Component
+
+```typescript
+// frontend/components/LogViewer.tsx
+"use client";
+
+import { useEffect, useState, useRef } from "react";
+import { getDeploymentLogs } from "@/lib/api";
+
+interface LogEntry {
+    timestamp: string;
+    stream: "stdout" | "stderr";
+    message: string;
+}
+
+export default function LogViewer({ deploymentId }: { deploymentId: string }) {
+    const [logs, setLogs] = useState<LogEntry[]>([]);
+    const [filter, setFilter] = useState<"all" | "stdout" | "stderr">("all");
+    const logEndRef = useRef<HTMLDivElement>(null);
+
+    const loadLogs = async () => {
+        const data = await getDeploymentLogs(deploymentId, 100);
+        setLogs(data);
+    };
+
+    useEffect(() => {
+        loadLogs();
+    }, [deploymentId]);
+
+    useEffect(() => {
+        logEndRef.current?.scrollIntoView({ behavior: "smooth" });
+    }, [logs]);
+
+    const filteredLogs = filter === "all"
+        ? logs
+        : logs.filter(log => log.stream === filter);
+
+    return (
+        <div className="space-y-4">
+            <div className="flex justify-between">
+                <h3 className="text-lg font-semibold">Container Logs</h3>
+                <div className="flex gap-2">
+                    <select
+                        value={filter}
+                        onChange={(e) => setFilter(e.target.value as any)}
+                        className="px-2 py-1 rounded border"
+                    >
+                        <option value="all">All</option>
+                        <option value="stdout">stdout</option>
+                        <option value="stderr">stderr</option>
+                    </select>
+                    <button onClick={loadLogs} className="btn-sm">
+                        Refresh
+                    </button>
+                </div>
+            </div>
+
+            <div className="bg-gray-900 rounded-lg p-4 h-96 overflow-y-auto font-mono text-sm">
+                {filteredLogs.map((log, i) => (
+                    <div
+                        key={i}
+                        className={log.stream === "stderr" ? "text-red-400" : "text-gray-300"}
+                    >
+                        <span className="text-gray-500">
+                            {new Date(log.timestamp).toLocaleTimeString()}
+                        </span>
+                        {" "}
+                        <span className="text-yellow-500">[{log.stream}]</span>
+                        {" "}
+                        {log.message}
+                    </div>
+                ))}
+                <div ref={logEndRef} />
+            </div>
+        </div>
+    );
+}
+```
+
+## Testing Scenarios
+
+### Test 1: Successful deployment logs
+1. Deploy valid server (TickTick)
+2. View logs at `/dashboard/{id}`
+3. Expected logs:
+   ```
+   Starting npm package: @alexarevalo.ai/mcp-server-ticktick
+   Installing packages...
+   mcp-proxy listening on [::]:8080
+   ```
+
+### Test 2: Failed deployment logs
+1. Deploy with invalid package name
+2. View logs
+3. Expected logs:
+   ```
+   Starting npm package: nonexistent-package
+   npm ERR! 404 Not Found
+   npm ERR! '@nonexistent-package' is not in the npm registry
+   ```
+
+### Test 3: Log retention
+1. Generate >1000 log lines (deploy chatty server)
+2. Query database:
+   ```sql
+   SELECT COUNT(*) FROM deployment_logs WHERE deployment_id = '{id}';
+   ```
+3. Expected: Exactly 1000 rows (oldest deleted)
+
+## Risk Mitigation
+
+### Risk: High log volume overwhelms database
+**Mitigation**: 1000-line limit per deployment with auto-cleanup
+**Future**: Archive old logs to S3
+
+### Risk: Fly logs API rate limits
+**Mitigation**: Single stream per deployment, closed when deployment inactive
+**Future**: Use Fly logs retention instead of streaming
+
+### Risk: Real-time streaming complexity
+**Mitigation**: Start with polling (GET /logs), add WebSocket later
+**Acceptable**: 5-10s delay for log updates is fine for debugging
+
+## Deployment Steps
+
+1. **Create feature branch**
+   ```bash
+   git checkout -b phase-4-observability
+   ```
+
+2. **Implement backend changes**
+   ```bash
+   cd backend
+   alembic revision -m "add deployment logs"
+   # Edit migration
+   alembic upgrade head
+   # Create log_collector.py
+   # Update deployments.py
+   pytest -v
+   ```
+
+3. **Implement frontend changes**
+   ```bash
+   cd frontend
+   # Create LogViewer component
+   # Create deployment detail page
+   bun run typecheck
+   ```
+
+4. **Deploy to Fly.io**
+   ```bash
+   cd backend
+   fly deploy --app catwalk-live-backend-dev
+   ```
+
+5. **Test end-to-end**
+   - Deploy test server
+   - View logs in frontend
+   - Verify logs update on refresh
+
+6. **Create git tag**
+   ```bash
+   git add .
+   git commit -m "feat: add container logs and diagnostics (Phase 4)"
+   git tag phase-4-complete
+   git push origin phase-4-observability
+   git push origin phase-4-complete
+   ```
+
+## Next Phase
+
+After Phase 4 is complete, proceed to **Phase 5: Serverless & Cost Optimization**.
+
+See `phase-5-serverless.md` for details.
diff --git a/context/plans/roadmap/phase-5-serverless.md b/context/plans/roadmap/phase-5-serverless.md
new file mode 100644
index 0000000..3a4e4d4
--- /dev/null
+++ b/context/plans/roadmap/phase-5-serverless.md
@@ -0,0 +1,248 @@
+# Phase 5: Serverless & Cost Optimization
+**Duration**: 3-4 weeks
+**Priority**: P1 (High)
+**Goal**: 70% cost reduction through scale-to-zero machines
+
+## Overview
+
+MCP servers are typically used intermittently (user asks Claude → tool call → idle). Currently, all Fly machines run 24/7 (~$1.94/month each). This phase implements Fly.io's scale-to-zero feature to:
+- Auto-stop machines when idle
+- Auto-wake on incoming requests
+- Reduce costs by 70-90% for idle deployments
+
+## Success Criteria
+
+- ✅ 70-90% cost reduction for idle deployments
+- ✅ Cold start time < 10 seconds (P95)
+- ✅ Health monitor distinguishes "stopped" vs "unhealthy"
+- ✅ Users understand serverless behavior (not perceived as "broken")
+
+## Implementation Checklist
+
+### 1. Serverless Fly Machine Configuration
+
+- [ ] **Update `backend/app/services/fly_deployment_service.py`** (line 70-86)
+  - [ ] Add services configuration to machine config:
+    ```python
+    config = {
+        "config": {
+            "image": self.image,
+            "guest": {...},
+            "env": env,
+            "restart": {"policy": "always"},
+            # NEW: Serverless configuration
+            "services": [{
+                "protocol": "tcp",
+                "internal_port": 8080,
+                "auto_stop_machines": "stop",      # Scale to zero
+                "auto_start_machines": True,       # Wake on request
+                "min_machines_running": 0,         # Allow zero
+                "concurrency": {
+                    "type": "requests",
+                    "hard_limit": 10               # Max concurrent requests
+                }
+            }]
+        }
+    }
+    ```
+
+- [ ] **Test serverless configuration**
+  - [ ] Deploy test server
+  - [ ] Wait 5 minutes (default idle timeout)
+  - [ ] Check machine state: `fly machine list --app {app_name}`
+  - [ ] Expected: Machine state = "stopped"
+
+### 2. Cold Start Optimization
+
+- [ ] **Pre-cache common packages in Dockerfile**
+  - [ ] Update `deploy/Dockerfile`:
+    ```dockerfile
+    # After mcp-proxy install, pre-install popular packages
+    RUN npm install -g \
+        @modelcontextprotocol/server-filesystem \
+        @modelcontextprotocol/server-github \
+        @modelcontextprotocol/server-memory \
+        @alexarevalo.ai/mcp-server-ticktick
+
+    RUN pip install --no-cache-dir \
+        mcp-server-git \
+        mcp-server-time \
+        mcp-server-sqlite
+    ```
+  - [ ] Rebuild and push image
+
+- [ ] **Add warm-up request after machine creation**
+  - [ ] In `deployments.py` after machine creation:
+    ```python
+    if machine_id:
+        # Send warm-up request to pre-populate cache
+        await send_warmup_request(machine_id)
+
+        # Then wait for health
+        health_ok = await wait_for_health(machine_id, timeout=60)
+    ```
+  - [ ] Implement `send_warmup_request()`:
+    ```python
+    async def send_warmup_request(machine_id: str):
+        machine_url = f"http://{machine_id}.vm.{app}.internal:8080/mcp"
+        try:
+            async with httpx.AsyncClient(timeout=10.0) as client:
+                await client.post(
+                    machine_url,
+                    json={"jsonrpc": "2.0", "id": 1, "method": "ping"},
+                    headers={"Accept": "application/json"}
+                )
+        except Exception as e:
+            logger.warning(f"Warmup request failed: {e}")
+    ```
+
+### 3. Health Monitor Serverless Adaptation
+
+- [ ] **Update `backend/app/services/health_monitor.py`**
+  - [ ] Modify `_check_deployment_health()` to handle stopped machines:
+    ```python
+    except httpx.ConnectError:
+        # Query Fly API to check machine state
+        machine_info = await fly_service.get_machine(deployment.machine_id)
+
+        if machine_info and machine_info.get("state") == "stopped":
+            # Normal serverless behavior
+            if deployment.status != DeploymentStatus.STOPPED:
+                deployment.status = DeploymentStatus.STOPPED
+                await db.commit()
+        else:
+            # Actually unhealthy
+            if deployment.status != DeploymentStatus.UNHEALTHY:
+                deployment.status = DeploymentStatus.UNHEALTHY
+                deployment.error_message = "Machine unreachable"
+                await db.commit()
+    ```
+
+- [ ] **Test health monitor with stopped machine**
+  - [ ] Deploy server, wait for "running"
+  - [ ] Wait 5+ minutes for auto-stop
+  - [ ] Check status becomes "stopped" (not "unhealthy")
+
+### 4. Frontend Cold Start Indicator
+
+- [ ] **Update `frontend/components/StatusBadge.tsx`**
+  - [ ] Add tooltip for "stopped" status:
+    ```typescript
+    {status === "stopped" && (
+        <span className="text-xs text-gray-400 ml-2">
+            (will wake on next request)
+        </span>
+    )}
+    ```
+
+- [ ] **Add cold start progress indicator** (optional)
+  - [ ] Show "Waking server..." during first request after idle
+  - [ ] Estimated time: "5-10 seconds"
+
+### 5. Cost Analytics Dashboard
+
+- [ ] **Create `frontend/components/CostDashboard.tsx`**
+  - [ ] Calculate estimated monthly cost:
+    ```typescript
+    const calculateCost = () => {
+        let total = 0;
+        for (const d of deployments) {
+            if (d.status === "running") {
+                total += 1.94;  // Always-on cost
+            } else if (d.status === "stopped") {
+                total += 0.05;  // Minimal idle cost
+            }
+        }
+        return total.toFixed(2);
+    };
+    ```
+  - [ ] Display:
+    - Total estimated monthly cost
+    - Number of active vs idle deployments
+    - Savings compared to always-on
+
+- [ ] **Add to dashboard page**
+  - [ ] Render `<CostDashboard deployments={deployments} />` at top
+
+### 6. Testing & Validation
+
+- [ ] **Test serverless lifecycle**
+  1. Deploy server → Status "running"
+  2. Wait 5 minutes → Status "stopped"
+  3. Send tool call from Claude → Machine wakes
+  4. Measure cold start time
+  5. Expected: < 10 seconds to first response
+
+- [ ] **Test concurrent requests during cold start**
+  - [ ] Send 5 requests simultaneously to stopped machine
+  - [ ] Verify all succeed (no errors)
+  - [ ] Check only 1 machine started
+
+- [ ] **Cost validation**
+  - [ ] Check Fly.io invoice after 1 week
+  - [ ] Expected: ~$0.05/deployment instead of ~$1.94
+
+### 7. Documentation
+
+- [ ] **Update `context/CURRENT_STATUS.md`**
+  - [ ] Add "Serverless scale-to-zero" to "What Works"
+  - [ ] Document cost savings
+
+- [ ] **Create user guide** (in frontend or docs)
+  - [ ] Explain serverless behavior
+  - [ ] Set expectations for cold start latency
+
+## Files to Create
+
+- `frontend/components/CostDashboard.tsx` (NEW)
+
+## Files to Modify
+
+- `backend/app/services/fly_deployment_service.py` (add services config)
+- `backend/app/api/deployments.py` (add warm-up request)
+- `backend/app/services/health_monitor.py` (handle stopped state)
+- `deploy/Dockerfile` (pre-cache packages)
+- `frontend/components/StatusBadge.tsx` (add stopped tooltip)
+
+## Cold Start Optimization Results
+
+Expected performance (based on Fly.io docs):
+
+| Scenario | Cold Start Time |
+|----------|-----------------|
+| Pre-cached package (TickTick) | 3-5 seconds |
+| Non-cached npm package | 8-12 seconds |
+| Python package (pip install) | 10-15 seconds |
+
+Target: P95 < 10 seconds (achieved with pre-caching)
+
+## Risk Mitigation
+
+### Risk: Users perceive cold starts as "broken"
+**Mitigation**: Clear UI messaging ("Waking server..."), set expectations
+**Education**: Add info tooltip explaining serverless behavior
+
+### Risk: Cold start timeout in Claude
+**Mitigation**: Claude's default timeout is 60s, well above our 10s target
+**Fallback**: Always-on mode toggle for users who need instant response
+
+### Risk: Package not pre-cached → slow cold start
+**Mitigation**: Pre-cache top 20 packages (cover 80% of deployments)
+**Future**: Allow users to opt into always-on mode
+
+## Deployment Steps
+
+1. Create feature branch
+2. Update FlyDeploymentService with serverless config
+3. Update Dockerfile with pre-cached packages
+4. Rebuild and push Docker image
+5. Deploy backend
+6. Test serverless lifecycle
+7. Monitor costs for 1 week
+8. Create git tag `phase-5-complete`
+
+## Next Phase
+
+Proceed to **Phase 6: Advanced Features** (version pinning, OAuth, GitHub-only repos).
+
+See `phase-6-advanced.md` for details.
diff --git a/context/plans/roadmap/phase-6-advanced.md b/context/plans/roadmap/phase-6-advanced.md
new file mode 100644
index 0000000..3da4350
--- /dev/null
+++ b/context/plans/roadmap/phase-6-advanced.md
@@ -0,0 +1,175 @@
+# Phase 6: Advanced Features
+**Duration**: 4-6 weeks
+**Priority**: P2 (Medium)
+**Goal**: Support edge cases and advanced deployment scenarios
+
+## Overview
+
+After the core platform is solid (Phases 1-5), this phase adds advanced features for power users:
+- **Version pinning**: Deploy specific package versions (not just latest)
+- **OAuth credential flows**: Multi-step auth with callbacks
+- **GitHub-only repos**: Support repos not published to npm/PyPI
+- **Complex credentials**: JSON files, SSH keys, multi-field credentials
+
+## Success Criteria
+
+- ✅ 30% of deployments use version pinning
+- ✅ OAuth flows work for 5+ common services
+- ✅ GitHub-only repos deployable via `npx github:user/repo`
+- ✅ Users can upload JSON credential files
+
+## Implementation Checklist
+
+### 1. Version Pinning
+
+- [ ] **Update analysis to extract version**
+  - [ ] Modify `backend/app/prompts/analysis_prompt.py`
+  - [ ] Add `"version": "1.2.3"` to expected output
+  - [ ] Default to `null` if version not specified
+
+- [ ] **Add version to deployment schema**
+  - [ ] Update `backend/app/schemas/deployment.py`
+  - [ ] Add optional `version` field to DeploymentCreate
+
+- [ ] **Validate version exists**
+  - [ ] In `package_validator.py`, add `validate_npm_package_version(package, version)`
+  - [ ] Check `https://registry.npmjs.org/{package}/{version}`
+  - [ ] Return error if version doesn't exist
+
+- [ ] **Use versioned package in deployment**
+  - [ ] In `fly_deployment_service.py`:
+    ```python
+    if version:
+        env["MCP_PACKAGE"] = f"{package}@{version}"
+    else:
+        env["MCP_PACKAGE"] = package  # Latest
+    ```
+
+- [ ] **Add version selector to frontend**
+  - [ ] In `configure/page.tsx`, add version input (optional)
+  - [ ] Show detected version from analysis as default
+
+### 2. OAuth Credential Flows
+
+- [ ] **Create OAuth router**
+  - [ ] Create `backend/app/api/oauth.py`
+  - [ ] Add `/oauth/{service}/authorize` endpoint (GET)
+    - [ ] Redirect to OAuth provider
+    - [ ] Store state in cache for CSRF protection
+  - [ ] Add `/oauth/{service}/callback` endpoint (GET)
+    - [ ] Verify state
+    - [ ] Exchange code for token
+    - [ ] Return token to frontend
+
+- [ ] **Add OAuth configs**
+  - [ ] Create `backend/app/oauth/configs.py`
+  - [ ] Define configs for common services:
+    - GitHub: client_id, client_secret, scopes
+    - Google: similar
+    - Others as needed
+
+- [ ] **Frontend OAuth flow**
+  - [ ] Add "Connect with GitHub" button in credential form
+  - [ ] Open OAuth popup window
+  - [ ] Listen for callback with token
+  - [ ] Auto-fill credential field with token
+
+### 3. GitHub-only Repository Support
+
+- [ ] **Update analysis prompt**
+  - [ ] Accept GitHub URLs as package names
+  - [ ] Format: `"package": "github:user/repo"`
+
+- [ ] **Validate GitHub repos**
+  - [ ] In `package_validator.py`, add `validate_github_repo(url)`
+  - [ ] Check repo exists via GitHub API
+  - [ ] Return error if repo not found or not accessible
+
+- [ ] **Support `npx github:user/repo`**
+  - [ ] In `entrypoint.sh`, handle `github:` prefix:
+    ```bash
+    npm)
+      if [[ "$MCP_PACKAGE" == github:* ]]; then
+        echo "Installing from GitHub: $MCP_PACKAGE"
+      fi
+      exec mcp-proxy ... -- npx -y "$MCP_PACKAGE"
+      ;;
+    ```
+
+### 4. Complex Credential Types
+
+- [ ] **Add credential type to env var schema**
+  - [ ] In analysis, extract `"type": "text|password|json|file"`
+  - [ ] Default to "password" if secret=true, else "text"
+
+- [ ] **Update form builder**
+  - [ ] In `frontend/components/dynamic-form/FormBuilder.tsx`
+  - [ ] Add field type handlers:
+    ```typescript
+    switch (field.type) {
+      case "json":
+        return <textarea placeholder="Paste JSON here" .../>;
+      case "file":
+        return <input type="file" accept=".pem,.json,.key" .../>;
+      default:
+        return <input type={field.secret ? "password" : "text"} .../>;
+    }
+    ```
+
+- [ ] **Handle file uploads**
+  - [ ] Convert file to base64 in frontend
+  - [ ] Send as string in credential
+  - [ ] Decode in backend before passing to container
+
+### 5. Testing
+
+- [ ] **Test version pinning**
+  - [ ] Deploy TickTick@1.0.0 (if version exists)
+  - [ ] Verify correct version installed in container
+
+- [ ] **Test OAuth flow** (use GitHub as example)
+  - [ ] Start OAuth flow
+  - [ ] Complete authorization
+  - [ ] Verify token received
+  - [ ] Deploy with OAuth token
+
+- [ ] **Test GitHub repo**
+  - [ ] Deploy `github:modelcontextprotocol/servers`
+  - [ ] Verify `npx github:...` works
+
+- [ ] **Test JSON credential**
+  - [ ] Upload service account JSON
+  - [ ] Verify deployed correctly as env var
+
+## Files to Create
+
+- `backend/app/api/oauth.py` (NEW)
+- `backend/app/oauth/configs.py` (NEW)
+- `backend/tests/test_oauth.py` (NEW)
+
+## Files to Modify
+
+- `backend/app/prompts/analysis_prompt.py` (add version field)
+- `backend/app/services/package_validator.py` (add version validation, GitHub validation)
+- `backend/app/services/fly_deployment_service.py` (use versioned package)
+- `backend/app/schemas/deployment.py` (add version field)
+- `deploy/entrypoint.sh` (handle github: prefix)
+- `frontend/components/dynamic-form/FormBuilder.tsx` (add field types)
+- `frontend/app/configure/page.tsx` (add version input, OAuth buttons)
+
+## Deployment Steps
+
+1. Create feature branch
+2. Implement version pinning
+3. Implement OAuth flows (start with GitHub)
+4. Implement GitHub-only repos
+5. Implement complex credentials
+6. Test each feature independently
+7. Deploy to production
+8. Create git tag `phase-6-complete`
+
+## Next Steps
+
+After Phase 6, the core platform is feature-complete. Proceed to **long-term vision** (marketplace, teams, edge, etc.).
+
+See `future-vision.md` for Phases 7-14+.
diff --git a/frontend/app/configure/page.tsx b/frontend/app/configure/page.tsx
index 9f74005..dec494b 100644
--- a/frontend/app/configure/page.tsx
+++ b/frontend/app/configure/page.tsx
@@ -2,28 +2,58 @@
 
 import { useSearchParams, useRouter } from "next/navigation";
 import { useQuery, useMutation } from "@tanstack/react-query";
-import { getFormSchema, createDeployment } from "@/lib/api";
+import { getFormSchema, getRegistryFormSchema, createDeployment, registry } from "@/lib/api";
 import FormBuilder from "@/components/dynamic-form/FormBuilder";
-import { Loader2, ArrowLeft } from "lucide-react";
+import { Loader2, ArrowLeft, AlertCircle } from "lucide-react";
 import Link from "next/link";
-import { Suspense } from "react";
+import { Suspense, useState } from "react";
 
 function ConfigureContent() {
     const searchParams = useSearchParams();
     const router = useRouter();
+    const [deploymentError, setDeploymentError] = useState<any>(null);
 
     const serviceType = searchParams.get("service") || "custom";
-    const repoUrl = searchParams.get("repo");
+    const paramRepoUrl = searchParams.get("repo");
+    const registryId = searchParams.get("registryId");
 
-    // Fetch schema
-    const { data: schema, isLoading, error } = useQuery({
-        queryKey: ["formSchema", serviceType, repoUrl],
-        queryFn: () => getFormSchema(serviceType, repoUrl),
+    // Fetch registry data if registryId is present
+    const { data: registryServer, isLoading: isRegistryLoading } = useQuery({
+        queryKey: ["registryServer", registryId],
+        queryFn: () => registry.get(registryId!),
+        enabled: !!registryId
     });
 
+    const isLocalOnlyRegistryServer = !!registryId && !!registryServer && !registryServer.capabilities.deployable;
+
+    // Determine which flow to use:
+    // - Registry flow: If registryId exists AND server is deployable
+    // - GitHub flow: Otherwise (manual repo URL)
+    const useRegistryFlow = !!registryId && registryServer?.capabilities.deployable;
+    const finalRepoUrl = paramRepoUrl || registryServer?.repository_url;
+
+    // Fetch schema using appropriate flow
+    const { data: schema, isLoading: isSchemaLoading, error } = useQuery({
+        queryKey: useRegistryFlow
+            ? ["formSchema", "registry", registryId]
+            : ["formSchema", serviceType, finalRepoUrl],
+        queryFn: () => {
+            if (useRegistryFlow) {
+                // Fast path: Parse registry data directly (no LLM)
+                return getRegistryFormSchema(registryId!);
+            } else {
+                // Slow path: Analyze GitHub repo with Claude (existing flow)
+                return getFormSchema(serviceType, finalRepoUrl!);
+            }
+        },
+        enabled: !isLocalOnlyRegistryServer && (useRegistryFlow ? !!registryId : !!finalRepoUrl)
+    });
+
+    const isLoading = isRegistryLoading || isSchemaLoading;
+
     // Submit to real API
     const mutation = useMutation({
-        mutationFn: async (formData: any) => {
+        mutationFn: async (formData: Record<string, any>) => {
             // Segregate name from credentials
             const { name, ...credentials } = formData;
 
@@ -39,10 +69,21 @@ function ConfigureContent() {
             });
         },
         onSuccess: () => {
+            // Clear any previous errors on success
+            setDeploymentError(null);
             router.push("/dashboard");
         },
-        onError: (err) => {
-            console.error(err);
+        onError: (err: any) => {
+            console.error("Deployment error:", err);
+            // Extract structured error from response
+            // The error detail could be in err.response.data.detail or err.detail
+            const errorDetail = err?.response?.data?.detail || err?.detail || {
+                error: "deployment_failed",
+                message: err?.message || "Failed to create deployment",
+            };
+            setDeploymentError(errorDetail);
+            // Scroll to top so user sees the error
+            window.scrollTo({ top: 0, behavior: "smooth" });
         }
     });
 
@@ -55,6 +96,25 @@ function ConfigureContent() {
     }
 
     if (error || !schema) {
+        if (isLocalOnlyRegistryServer) {
+            return (
+                <div className="min-h-screen flex flex-col items-center justify-center gap-4 text-center p-6">
+                    <p className="text-[var(--pk-status-red)]">
+                        This server is marked local-only and can’t be deployed to cloud machines.
+                    </p>
+                    {registryServer?.repository_url && (
+                        <p className="text-[var(--pk-text-secondary)] text-sm break-all max-w-2xl">
+                            Repository:{" "}
+                            <span className="text-white font-mono">
+                                {registryServer.repository_url}
+                            </span>
+                        </p>
+                    )}
+                    <Link href="/" className="btn-aurora">Back to Registry</Link>
+                </div>
+            );
+        }
+
         return (
             <div className="min-h-screen flex flex-col items-center justify-center gap-4 text-center p-6">
                 <p className="text-[var(--pk-status-red)]">Failed to load configuration form.</p>
@@ -72,16 +132,65 @@ function ConfigureContent() {
 
             <div className="space-y-2">
                 <h1 className="text-3xl font-bold text-gradient">Configure Deployment</h1>
-                {repoUrl && (
+                {finalRepoUrl && (
                     <p className="text-[var(--pk-text-secondary)] text-sm break-all">
-                        Repository: <span className="text-white font-mono">{repoUrl}</span>
+                        Repository: <span className="text-white font-mono">{finalRepoUrl}</span>
                     </p>
                 )}
             </div>
 
+            {/* Error Display */}
+            {deploymentError && (
+                <div className="p-4 bg-red-900/20 border border-red-500/50 rounded-lg space-y-3">
+                    <div className="flex items-start gap-3">
+                        <AlertCircle className="text-red-400 flex-shrink-0 mt-0.5" size={20} />
+                        <div className="flex-1 space-y-2">
+                            <h3 className="font-semibold text-red-300">
+                                {deploymentError.error === "credential_validation_failed"
+                                    ? "Missing Required Credentials"
+                                    : deploymentError.error === "package_not_found"
+                                    ? "Package Not Found"
+                                    : "Deployment Failed"}
+                            </h3>
+
+                            {deploymentError.message && (
+                                <p className="text-red-200 text-sm">{deploymentError.message}</p>
+                            )}
+
+                            {deploymentError.errors && Array.isArray(deploymentError.errors) && (
+                                <ul className="list-disc list-inside text-red-200 text-sm space-y-1">
+                                    {deploymentError.errors.map((err: string, idx: number) => (
+                                        <li key={idx}>{err}</li>
+                                    ))}
+                                </ul>
+                            )}
+
+                            {deploymentError.package && (
+                                <p className="text-red-300 text-sm font-mono bg-red-950/50 px-2 py-1 rounded">
+                                    Package: {deploymentError.package}
+                                </p>
+                            )}
+
+                            {deploymentError.help && (
+                                <p className="text-red-300/80 text-sm italic border-l-2 border-red-500/30 pl-3">
+                                    {deploymentError.help}
+                                </p>
+                            )}
+
+                            <button
+                                onClick={() => setDeploymentError(null)}
+                                className="text-sm text-red-300 hover:text-red-200 underline transition-colors"
+                            >
+                                Dismiss
+                            </button>
+                        </div>
+                    </div>
+                </div>
+            )}
+
             <FormBuilder
                 schema={schema}
-                onSubmit={async (data) => mutation.mutateAsync(data)}
+                onSubmit={async (data) => { await mutation.mutateAsync(data); }}
                 isLoading={mutation.isPending}
             />
         </div>
diff --git a/frontend/app/page.tsx b/frontend/app/page.tsx
index f9113b3..7f188f1 100644
--- a/frontend/app/page.tsx
+++ b/frontend/app/page.tsx
@@ -1,11 +1,13 @@
 "use client";
 
-import { ArrowRight, Search, Zap, Loader2 } from "lucide-react";
+import { ArrowRight, Search, Zap, Loader2, Globe, Github } from "lucide-react";
 import Link from "next/link";
 import { useState } from "react";
 import { useRouter } from "next/navigation";
 import { useMutation } from "@tanstack/react-query";
 import { analyzeRepo } from "@/lib/api";
+import { Tabs, TabsContent, TabsList, TabsTrigger } from "@/components/ui/tabs";
+import RegistryFeed from "@/components/registry/RegistryFeed";
 
 export default function Home() {
   const [repoUrl, setRepoUrl] = useState("");
@@ -14,11 +16,7 @@ export default function Home() {
   const { mutate, isPending: isAnalyzing, error } = useMutation({
     mutationFn: analyzeRepo,
     onSuccess: (data) => {
-      // Redirect to configure page with result data or just the type if we know it
-      // For now, let's assume we want to configure "openai" by default or infer from data
-      // In a real scenario, data.data might contain "detected_service"
-
-      const serviceType = "custom"; // Default/Fallback for now until analysis is detailed
+      const serviceType = "custom";
       router.push(`/configure?service=${serviceType}&repo=${encodeURIComponent(repoUrl)}`);
     }
   });
@@ -30,73 +28,93 @@ export default function Home() {
   };
 
   return (
-    <div className="min-h-screen flex flex-col items-center justify-center p-6 relative overflow-hidden">
+    <div className="min-h-screen flex flex-col items-center p-6 relative overflow-hidden bg-background">
 
       {/* Background Gradient Blob */}
-      <div className="absolute top-1/2 left-1/2 -translate-x-1/2 -translate-y-1/2 w-[500px] h-[500px] bg-[var(--pk-accent-primary-from)]/20 rounded-full blur-[120px] pointer-events-none" />
+      <div className="absolute top-0 left-1/2 -translate-x-1/2 w-[600px] h-[600px] bg-[var(--pk-accent-primary-from)]/10 rounded-full blur-[120px] pointer-events-none" />
 
-      <main className="relative z-10 w-full max-w-3xl text-center space-y-10">
+      <main className="relative z-10 w-full max-w-5xl text-center space-y-10 pt-20">
 
         {/* Hero Text */}
         <div className="space-y-4 animate-in fade-in slide-in-from-bottom-8 duration-700">
           <div className="inline-flex items-center gap-2 px-3 py-1 rounded-full bg-white/5 border border-white/10 text-xs font-medium text-[var(--pk-text-accent)] mb-4">
             <Zap size={12} className="text-[var(--pk-status-green)]" />
-            <span>Catwalk Live v0.1</span>
+            <span>Catwalk Live v0.2</span>
           </div>
-          <h1 className="text-5xl md:text-7xl font-bold tracking-tight text-white">
-            Orchestrate your <br />
-            <span className="text-gradient">AI Agents</span> securely.
+          <h1 className="text-4xl md:text-6xl font-bold tracking-tight text-white mb-2">
+            Build your <span className="text-gradient">AI Toolchain</span>
           </h1>
-          <p className="text-lg md:text-xl text-[var(--pk-text-secondary)] max-w-2xl mx-auto">
-            Deploy, manage, and monitor your MCP servers and LLM agents with a modern, secure dashboard.
+          <p className="text-lg text-[var(--pk-text-secondary)] max-w-2xl mx-auto">
+            Discover, deploy, and connect MCP servers securely.
           </p>
         </div>
 
-        {/* Input Action */}
-        <div className="animate-in fade-in slide-in-from-bottom-8 duration-1000 delay-200">
-          <form onSubmit={handleAnalyze} className="relative max-w-xl mx-auto group">
-            <div className="absolute inset-0 bg-gradient-to-r from-[var(--pk-accent-primary-from)] to-[var(--pk-accent-primary-to)] rounded-2xl blur opacity-20 group-hover:opacity-40 transition-opacity duration-500" />
-            <div className="relative flex items-center bg-[#0B0C15] border border-white/10 rounded-2xl p-2 shadow-2xl">
-              <Search className="ml-4 text-[var(--pk-text-secondary)]" size={20} />
-              <input
-                type="text"
-                placeholder="Enter GitHub Repository URL..."
-                className="flex-1 bg-transparent border-none outline-none text-white px-4 py-3 placeholder:text-[var(--pk-text-secondary)]/50"
-                value={repoUrl}
-                onChange={(e) => setRepoUrl(e.target.value)}
-              />
-              <button
-                type="submit"
-                disabled={isAnalyzing}
-                className="bg-white text-black font-semibold px-6 py-3 rounded-xl hover:bg-white/90 transition-colors flex items-center gap-2 disabled:opacity-70 disabled:cursor-not-allowed"
-              >
-                {isAnalyzing ? (
-                  <>
-                    <Loader2 className="animate-spin" size={18} />
-                    <span>Analyzing...</span>
-                  </>
-                ) : (
-                  <>
-                    <span>Analyze</span>
-                    <ArrowRight size={18} />
-                  </>
-                )}
-              </button>
+        {/* Main Content Area */}
+        <div className="animate-in fade-in slide-in-from-bottom-8 duration-1000 delay-200 text-left">
+          <Tabs defaultValue="registry" className="w-full">
+            <div className="flex justify-center mb-8">
+              <TabsList className="bg-white/5 border border-white/10 p-1 rounded-xl">
+                <TabsTrigger value="manual" className="data-[state=active]:bg-white/10 rounded-lg px-6">
+                  <Github className="w-4 h-4 mr-2" />
+                  Import from GitHub
+                </TabsTrigger>
+                <TabsTrigger value="registry" className="data-[state=active]:bg-white/10 rounded-lg px-6">
+                  <Globe className="w-4 h-4 mr-2" />
+                  Browse Registry
+                </TabsTrigger>
+              </TabsList>
             </div>
-          </form>
 
-          {error && (
-            <p className="text-[var(--pk-status-red)] text-center mt-4 animate-in fade-in">
-              {(error as Error).message || "Analysis failed. Please try again."}
-            </p>
-          )}
+            <TabsContent value="manual" className="max-w-xl mx-auto mt-10">
+              <form onSubmit={handleAnalyze} className="relative group">
+                <div className="absolute inset-0 bg-gradient-to-r from-[var(--pk-accent-primary-from)] to-[var(--pk-accent-primary-to)] rounded-2xl blur opacity-20 group-hover:opacity-40 transition-opacity duration-500" />
+                <div className="relative flex items-center bg-[#0B0C15] border border-white/10 rounded-2xl p-2 shadow-2xl">
+                  <Search className="ml-4 text-[var(--pk-text-secondary)]" size={20} />
+                  <input
+                    type="text"
+                    placeholder="Enter GitHub Repository URL..."
+                    className="flex-1 bg-transparent border-none outline-none text-white px-4 py-3 placeholder:text-[var(--pk-text-secondary)]/50"
+                    value={repoUrl}
+                    onChange={(e) => setRepoUrl(e.target.value)}
+                  />
+                  <button
+                    type="submit"
+                    disabled={isAnalyzing}
+                    className="bg-white text-black font-semibold px-6 py-3 rounded-xl hover:bg-white/90 transition-colors flex items-center gap-2 disabled:opacity-70 disabled:cursor-not-allowed"
+                  >
+                    {isAnalyzing ? (
+                      <>
+                        <Loader2 className="animate-spin" size={18} />
+                        <span>Analyzing...</span>
+                      </>
+                    ) : (
+                      <>
+                        <span>Analyze</span>
+                        <ArrowRight size={18} />
+                      </>
+                    )}
+                  </button>
+                </div>
+              </form>
+              {error && (
+                <p className="text-[var(--pk-status-red)] text-center mt-4 animate-in fade-in">
+                  {(error as Error).message || "Analysis failed. Please try again."}
+                </p>
+              )}
+            </TabsContent>
 
-          <div className="mt-8 flex items-center justify-center gap-6 text-sm text-[var(--pk-text-secondary)]">
-            <Link href="/dashboard" className="hover:text-white transition-colors">Skip to Dashboard</Link>
-            <span>•</span>
-            <a href="https://github.com/zenchantlive/catwalk" target="_blank" className="hover:text-white transition-colors">View Documentation</a>
-          </div>
+            <TabsContent value="registry" className="pt-4">
+              <RegistryFeed />
+            </TabsContent>
+          </Tabs>
         </div>
+
+        <div className="mt-20 flex items-center justify-center gap-6 text-sm text-[var(--pk-text-secondary)]">
+          <Link href="/dashboard" className="hover:text-white transition-colors">Go to Dashboard</Link>
+          <span>•</span>
+          <a href="https://github.com/zenchantlive/catwalk" target="_blank" className="hover:text-white transition-colors">Docs</a>
+        </div>
+
       </main>
     </div>
   );
diff --git a/frontend/bun.lock b/frontend/bun.lock
index c3b8392..c144bdd 100644
--- a/frontend/bun.lock
+++ b/frontend/bun.lock
@@ -6,7 +6,10 @@
       "name": "frontend",
       "dependencies": {
         "@hookform/resolvers": "^5.2.2",
+        "@radix-ui/react-slot": "^1.2.4",
+        "@radix-ui/react-tabs": "^1.1.13",
         "@tanstack/react-query": "^5.90.12",
+        "class-variance-authority": "^0.7.1",
         "clsx": "^2.1.1",
         "lucide-react": "^0.560.0",
         "next": "16.0.10",
@@ -190,6 +193,36 @@
 
     "@nolyfill/is-core-module": ["@nolyfill/is-core-module@1.0.39", "", {}, "sha512-nn5ozdjYQpUCZlWGuxcJY/KpxkWQs4DcbMCmKojjyrYDEAGy4Ce19NN4v5MduafTwJlbKc99UA8YhSVqq9yPZA=="],
 
+    "@radix-ui/primitive": ["@radix-ui/primitive@1.1.3", "", {}, "sha512-JTF99U/6XIjCBo0wqkU5sK10glYe27MRRsfwoiq5zzOEZLHU3A3KCMa5X/azekYRCJ0HlwI0crAXS/5dEHTzDg=="],
+
+    "@radix-ui/react-collection": ["@radix-ui/react-collection@1.1.7", "", { "dependencies": { "@radix-ui/react-compose-refs": "1.1.2", "@radix-ui/react-context": "1.1.2", "@radix-ui/react-primitive": "2.1.3", "@radix-ui/react-slot": "1.2.3" }, "peerDependencies": { "@types/react": "*", "@types/react-dom": "*", "react": "^16.8 || ^17.0 || ^18.0 || ^19.0 || ^19.0.0-rc", "react-dom": "^16.8 || ^17.0 || ^18.0 || ^19.0 || ^19.0.0-rc" }, "optionalPeers": ["@types/react", "@types/react-dom"] }, "sha512-Fh9rGN0MoI4ZFUNyfFVNU4y9LUz93u9/0K+yLgA2bwRojxM8JU1DyvvMBabnZPBgMWREAJvU2jjVzq+LrFUglw=="],
+
+    "@radix-ui/react-compose-refs": ["@radix-ui/react-compose-refs@1.1.2", "", { "peerDependencies": { "@types/react": "*", "react": "^16.8 || ^17.0 || ^18.0 || ^19.0 || ^19.0.0-rc" }, "optionalPeers": ["@types/react"] }, "sha512-z4eqJvfiNnFMHIIvXP3CY57y2WJs5g2v3X0zm9mEJkrkNv4rDxu+sg9Jh8EkXyeqBkB7SOcboo9dMVqhyrACIg=="],
+
+    "@radix-ui/react-context": ["@radix-ui/react-context@1.1.2", "", { "peerDependencies": { "@types/react": "*", "react": "^16.8 || ^17.0 || ^18.0 || ^19.0 || ^19.0.0-rc" }, "optionalPeers": ["@types/react"] }, "sha512-jCi/QKUM2r1Ju5a3J64TH2A5SpKAgh0LpknyqdQ4m6DCV0xJ2HG1xARRwNGPQfi1SLdLWZ1OJz6F4OMBBNiGJA=="],
+
+    "@radix-ui/react-direction": ["@radix-ui/react-direction@1.1.1", "", { "peerDependencies": { "@types/react": "*", "react": "^16.8 || ^17.0 || ^18.0 || ^19.0 || ^19.0.0-rc" }, "optionalPeers": ["@types/react"] }, "sha512-1UEWRX6jnOA2y4H5WczZ44gOOjTEmlqv1uNW4GAJEO5+bauCBhv8snY65Iw5/VOS/ghKN9gr2KjnLKxrsvoMVw=="],
+
+    "@radix-ui/react-id": ["@radix-ui/react-id@1.1.1", "", { "dependencies": { "@radix-ui/react-use-layout-effect": "1.1.1" }, "peerDependencies": { "@types/react": "*", "react": "^16.8 || ^17.0 || ^18.0 || ^19.0 || ^19.0.0-rc" }, "optionalPeers": ["@types/react"] }, "sha512-kGkGegYIdQsOb4XjsfM97rXsiHaBwco+hFI66oO4s9LU+PLAC5oJ7khdOVFxkhsmlbpUqDAvXw11CluXP+jkHg=="],
+
+    "@radix-ui/react-presence": ["@radix-ui/react-presence@1.1.5", "", { "dependencies": { "@radix-ui/react-compose-refs": "1.1.2", "@radix-ui/react-use-layout-effect": "1.1.1" }, "peerDependencies": { "@types/react": "*", "@types/react-dom": "*", "react": "^16.8 || ^17.0 || ^18.0 || ^19.0 || ^19.0.0-rc", "react-dom": "^16.8 || ^17.0 || ^18.0 || ^19.0 || ^19.0.0-rc" }, "optionalPeers": ["@types/react", "@types/react-dom"] }, "sha512-/jfEwNDdQVBCNvjkGit4h6pMOzq8bHkopq458dPt2lMjx+eBQUohZNG9A7DtO/O5ukSbxuaNGXMjHicgwy6rQQ=="],
+
+    "@radix-ui/react-primitive": ["@radix-ui/react-primitive@2.1.3", "", { "dependencies": { "@radix-ui/react-slot": "1.2.3" }, "peerDependencies": { "@types/react": "*", "@types/react-dom": "*", "react": "^16.8 || ^17.0 || ^18.0 || ^19.0 || ^19.0.0-rc", "react-dom": "^16.8 || ^17.0 || ^18.0 || ^19.0 || ^19.0.0-rc" }, "optionalPeers": ["@types/react", "@types/react-dom"] }, "sha512-m9gTwRkhy2lvCPe6QJp4d3G1TYEUHn/FzJUtq9MjH46an1wJU+GdoGC5VLof8RX8Ft/DlpshApkhswDLZzHIcQ=="],
+
+    "@radix-ui/react-roving-focus": ["@radix-ui/react-roving-focus@1.1.11", "", { "dependencies": { "@radix-ui/primitive": "1.1.3", "@radix-ui/react-collection": "1.1.7", "@radix-ui/react-compose-refs": "1.1.2", "@radix-ui/react-context": "1.1.2", "@radix-ui/react-direction": "1.1.1", "@radix-ui/react-id": "1.1.1", "@radix-ui/react-primitive": "2.1.3", "@radix-ui/react-use-callback-ref": "1.1.1", "@radix-ui/react-use-controllable-state": "1.2.2" }, "peerDependencies": { "@types/react": "*", "@types/react-dom": "*", "react": "^16.8 || ^17.0 || ^18.0 || ^19.0 || ^19.0.0-rc", "react-dom": "^16.8 || ^17.0 || ^18.0 || ^19.0 || ^19.0.0-rc" }, "optionalPeers": ["@types/react", "@types/react-dom"] }, "sha512-7A6S9jSgm/S+7MdtNDSb+IU859vQqJ/QAtcYQcfFC6W8RS4IxIZDldLR0xqCFZ6DCyrQLjLPsxtTNch5jVA4lA=="],
+
+    "@radix-ui/react-slot": ["@radix-ui/react-slot@1.2.4", "", { "dependencies": { "@radix-ui/react-compose-refs": "1.1.2" }, "peerDependencies": { "@types/react": "*", "react": "^16.8 || ^17.0 || ^18.0 || ^19.0 || ^19.0.0-rc" }, "optionalPeers": ["@types/react"] }, "sha512-Jl+bCv8HxKnlTLVrcDE8zTMJ09R9/ukw4qBs/oZClOfoQk/cOTbDn+NceXfV7j09YPVQUryJPHurafcSg6EVKA=="],
+
+    "@radix-ui/react-tabs": ["@radix-ui/react-tabs@1.1.13", "", { "dependencies": { "@radix-ui/primitive": "1.1.3", "@radix-ui/react-context": "1.1.2", "@radix-ui/react-direction": "1.1.1", "@radix-ui/react-id": "1.1.1", "@radix-ui/react-presence": "1.1.5", "@radix-ui/react-primitive": "2.1.3", "@radix-ui/react-roving-focus": "1.1.11", "@radix-ui/react-use-controllable-state": "1.2.2" }, "peerDependencies": { "@types/react": "*", "@types/react-dom": "*", "react": "^16.8 || ^17.0 || ^18.0 || ^19.0 || ^19.0.0-rc", "react-dom": "^16.8 || ^17.0 || ^18.0 || ^19.0 || ^19.0.0-rc" }, "optionalPeers": ["@types/react", "@types/react-dom"] }, "sha512-7xdcatg7/U+7+Udyoj2zodtI9H/IIopqo+YOIcZOq1nJwXWBZ9p8xiu5llXlekDbZkca79a/fozEYQXIA4sW6A=="],
+
+    "@radix-ui/react-use-callback-ref": ["@radix-ui/react-use-callback-ref@1.1.1", "", { "peerDependencies": { "@types/react": "*", "react": "^16.8 || ^17.0 || ^18.0 || ^19.0 || ^19.0.0-rc" }, "optionalPeers": ["@types/react"] }, "sha512-FkBMwD+qbGQeMu1cOHnuGB6x4yzPjho8ap5WtbEJ26umhgqVXbhekKUQO+hZEL1vU92a3wHwdp0HAcqAUF5iDg=="],
+
+    "@radix-ui/react-use-controllable-state": ["@radix-ui/react-use-controllable-state@1.2.2", "", { "dependencies": { "@radix-ui/react-use-effect-event": "0.0.2", "@radix-ui/react-use-layout-effect": "1.1.1" }, "peerDependencies": { "@types/react": "*", "react": "^16.8 || ^17.0 || ^18.0 || ^19.0 || ^19.0.0-rc" }, "optionalPeers": ["@types/react"] }, "sha512-BjasUjixPFdS+NKkypcyyN5Pmg83Olst0+c6vGov0diwTEo6mgdqVR6hxcEgFuh4QrAs7Rc+9KuGJ9TVCj0Zzg=="],
+
+    "@radix-ui/react-use-effect-event": ["@radix-ui/react-use-effect-event@0.0.2", "", { "dependencies": { "@radix-ui/react-use-layout-effect": "1.1.1" }, "peerDependencies": { "@types/react": "*", "react": "^16.8 || ^17.0 || ^18.0 || ^19.0 || ^19.0.0-rc" }, "optionalPeers": ["@types/react"] }, "sha512-Qp8WbZOBe+blgpuUT+lw2xheLP8q0oatc9UpmiemEICxGvFLYmHm9QowVZGHtJlGbS6A6yJ3iViad/2cVjnOiA=="],
+
+    "@radix-ui/react-use-layout-effect": ["@radix-ui/react-use-layout-effect@1.1.1", "", { "peerDependencies": { "@types/react": "*", "react": "^16.8 || ^17.0 || ^18.0 || ^19.0 || ^19.0.0-rc" }, "optionalPeers": ["@types/react"] }, "sha512-RbJRS4UWQFkzHTTwVymMTUv8EqYhOp8dOOviLj2ugtTiXRaRQS7GLGxZTLL1jWhMeoSCf5zmcZkqTl9IiYfXcQ=="],
+
     "@rtsao/scc": ["@rtsao/scc@1.1.0", "", {}, "sha512-zt6OdqaDoOnJ1ZYsCYGt9YmWzDXl4vQdKTyJev62gFhRGKdx7mcT54V9KIjg+d2wi9EXsPvAPKe7i7WjfVWB8g=="],
 
     "@standard-schema/utils": ["@standard-schema/utils@0.3.0", "", {}, "sha512-e7Mew686owMaPJVNNLs55PUvgz371nKgwsc4vxE49zsODpJEnxgxRo2y/OKrqueavXgZNMDVj3DdHFlaSAeU8g=="],
@@ -364,6 +397,8 @@
 
     "chalk": ["chalk@4.1.2", "", { "dependencies": { "ansi-styles": "^4.1.0", "supports-color": "^7.1.0" } }, "sha512-oKnbhFyRIXpUuez8iBMmyEa4nbj4IOQyuhc/wy9kY7/WVPcwIO9VA668Pu8RkO7+0G76SLROeyw9CpQ061i4mA=="],
 
+    "class-variance-authority": ["class-variance-authority@0.7.1", "", { "dependencies": { "clsx": "^2.1.1" } }, "sha512-Ka+9Trutv7G8M6WT6SeiRWz792K5qEqIGEGzXKhAE6xOWAY6pPH8U+9IY3oCMv6kqTmLsv7Xh/2w2RigkePMsg=="],
+
     "client-only": ["client-only@0.0.1", "", {}, "sha512-IV3Ou0jSMzZrd3pZ48nLkT9DA7Ag1pnPzaiQhpW7c3RbcqqzvzzVu+L8gfqMp/8IM2MQtSiqaCxrrcfu8I8rMA=="],
 
     "clsx": ["clsx@2.1.1", "", {}, "sha512-eYm0QWBtUrBWZWG0d386OGAw16Z995PiOVo2B7bjWSbHedGl5e0ZWaq65kOGgUSNesEIDkB9ISbTg/JK9dhCZA=="],
@@ -886,6 +921,10 @@
 
     "@eslint/eslintrc/globals": ["globals@14.0.0", "", {}, "sha512-oahGvuMGQlPw/ivIYBjVSrWAfWLBeku5tpPE2fOPLi+WHffIWbuh2tCjhyQhTBPMf5E9jDEH4FOmTYgYwbKwtQ=="],
 
+    "@radix-ui/react-collection/@radix-ui/react-slot": ["@radix-ui/react-slot@1.2.3", "", { "dependencies": { "@radix-ui/react-compose-refs": "1.1.2" }, "peerDependencies": { "@types/react": "*", "react": "^16.8 || ^17.0 || ^18.0 || ^19.0 || ^19.0.0-rc" }, "optionalPeers": ["@types/react"] }, "sha512-aeNmHnBxbi2St0au6VBVC7JXFlhLlOnvIIlePNniyUNAClzmtAUEY8/pBiK3iHjufOlwA+c20/8jngo7xcrg8A=="],
+
+    "@radix-ui/react-primitive/@radix-ui/react-slot": ["@radix-ui/react-slot@1.2.3", "", { "dependencies": { "@radix-ui/react-compose-refs": "1.1.2" }, "peerDependencies": { "@types/react": "*", "react": "^16.8 || ^17.0 || ^18.0 || ^19.0 || ^19.0.0-rc" }, "optionalPeers": ["@types/react"] }, "sha512-aeNmHnBxbi2St0au6VBVC7JXFlhLlOnvIIlePNniyUNAClzmtAUEY8/pBiK3iHjufOlwA+c20/8jngo7xcrg8A=="],
+
     "@tailwindcss/oxide-wasm32-wasi/@emnapi/core": ["@emnapi/core@1.7.1", "", { "dependencies": { "@emnapi/wasi-threads": "1.1.0", "tslib": "^2.4.0" }, "bundled": true }, "sha512-o1uhUASyo921r2XtHYOHy7gdkGLge8ghBEQHMWmyJFoXlpU58kIrhhN3w26lpQb6dspetweapMn2CSNwQ8I4wg=="],
 
     "@tailwindcss/oxide-wasm32-wasi/@emnapi/runtime": ["@emnapi/runtime@1.7.1", "", { "dependencies": { "tslib": "^2.4.0" }, "bundled": true }, "sha512-PVtJr5CmLwYAU9PZDMITZoR5iAOShYREoR45EyyLrbntV50mdePTgUn4AmOw90Ifcj+x2kRjdzr1HP3RrNiHGA=="],
diff --git a/frontend/components/dynamic-form/FormBuilder.tsx b/frontend/components/dynamic-form/FormBuilder.tsx
index 2254c13..c11ed6b 100644
--- a/frontend/components/dynamic-form/FormBuilder.tsx
+++ b/frontend/components/dynamic-form/FormBuilder.tsx
@@ -34,6 +34,14 @@ interface FormBuilderProps {
 export default function FormBuilder({ schema, onSubmit, isLoading }: FormBuilderProps) {
     const [showPassword, setShowPassword] = useState<Record<string, boolean>>({});
 
+    if (!schema?.fields) {
+        return (
+            <div className="card-glass p-6 w-full max-w-lg mx-auto text-center">
+                <p className="text-[var(--pk-status-red)]">Invalid form schema: Missing fields definition.</p>
+            </div>
+        );
+    }
+
     // Dynamically generate Zod schema based on props
     const generateZodSchema = (fields: FormField[]) => {
         const shape: Record<string, z.ZodTypeAny> = {};
diff --git a/frontend/components/registry/RegistryFeed.tsx b/frontend/components/registry/RegistryFeed.tsx
new file mode 100644
index 0000000..5e6da1b
--- /dev/null
+++ b/frontend/components/registry/RegistryFeed.tsx
@@ -0,0 +1,73 @@
+"use client";
+
+import { useEffect, useState } from "react";
+import { registry, RegistryServer } from "@/lib/api";
+import { ServerCard } from "@/components/registry/ServerCard";
+import { Input } from "@/components/ui/input";
+import { Search, Loader2 } from "lucide-react";
+
+export default function RegistryFeed() {
+    const [servers, setServers] = useState<RegistryServer[]>([]);
+    const [isLoading, setIsLoading] = useState(true);
+    const [searchQuery, setSearchQuery] = useState("");
+    const [debouncedQuery, setDebouncedQuery] = useState("");
+
+    // Debounce search input
+    useEffect(() => {
+        const timer = setTimeout(() => setDebouncedQuery(searchQuery), 300);
+        return () => clearTimeout(timer);
+    }, [searchQuery]);
+
+    // Fetch data
+    useEffect(() => {
+        const fetchData = async () => {
+            setIsLoading(true);
+            try {
+                const results = await registry.search(debouncedQuery);
+                setServers(results);
+            } catch (error) {
+                console.error("Failed to fetch registry:", error);
+            } finally {
+                setIsLoading(false);
+            }
+        };
+
+        fetchData();
+    }, [debouncedQuery]);
+
+    return (
+        <div className="space-y-6">
+            {/* Search Header */}
+            <div className="sticky top-0 z-10 bg-black/50 backdrop-blur-xl p-4 -mx-4 border-b border-white/10">
+                <div className="relative max-w-2xl mx-auto">
+                    <Search className="absolute left-3 top-1/2 -translate-y-1/2 text-white/50 w-5 h-5" />
+                    <Input
+                        placeholder="Search MCP servers (e.g. 'stripe', 'browser')..."
+                        className="pl-10 bg-white/5 border-white/10 text-white placeholder:text-white/30 focus:bg-white/10 transition-all font-light text-lg h-12 rounded-xl"
+                        value={searchQuery}
+                        onChange={(e) => setSearchQuery(e.target.value)}
+                    />
+                </div>
+            </div>
+
+            {/* Grid */}
+            {isLoading ? (
+                <div className="flex justify-center py-20">
+                    <Loader2 className="w-8 h-8 text-white/30 animate-spin" />
+                </div>
+            ) : servers.length === 0 ? (
+                <div className="text-center py-20 text-white/30">
+                    No servers found matching "{debouncedQuery}"
+                </div>
+            ) : (
+                <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-4">
+                    {servers.map((server) => (
+                        <div key={server.id} className="h-full">
+                            <ServerCard server={server} />
+                        </div>
+                    ))}
+                </div>
+            )}
+        </div>
+    );
+}
diff --git a/frontend/components/registry/ServerCard.tsx b/frontend/components/registry/ServerCard.tsx
new file mode 100644
index 0000000..ff8d5c6
--- /dev/null
+++ b/frontend/components/registry/ServerCard.tsx
@@ -0,0 +1,73 @@
+import { RegistryServer } from "@/lib/api";
+import { Badge } from "@/components/ui/badge";
+import { Button } from "@/components/ui/button";
+import { Card, CardContent, CardFooter, CardHeader } from "@/components/ui/card";
+import { Check, Rocket, Link as LinkIcon } from "lucide-react";
+import Link from "next/link";
+
+interface ServerCardProps {
+    server: RegistryServer;
+}
+
+export function ServerCard({ server }: ServerCardProps) {
+    const isDeployable = server.capabilities.deployable;
+
+    // Generate avatar letter from first char of name
+    const initial = server.name.charAt(0).toUpperCase();
+
+    return (
+        <Card className="flex flex-col h-full bg-white/5 backdrop-blur-md border-white/10 hover:border-white/20 transition-all duration-300 group">
+            <CardHeader className="pb-3 flex-row gap-4 items-start space-y-0">
+                {/* Icon Placeholder */}
+                <div className="w-12 h-12 rounded-xl bg-gradient-to-br from-indigo-500/20 to-purple-500/20 flex items-center justify-center border border-white/10 shrink-0 group-hover:scale-105 transition-transform">
+                    <span className="text-xl font-bold bg-clip-text text-transparent bg-gradient-to-r from-indigo-400 to-purple-400">
+                        {initial}
+                    </span>
+                </div>
+
+                <div className="flex-1 min-w-0">
+                    <div className="flex items-center gap-2 mb-1">
+                        <h3 className="font-semibold text-lg text-white truncate" title={server.name}>
+                            {server.name}
+                        </h3>
+                        {server.trust.is_official && (
+                            <div className="text-blue-400" title="Official">
+                                <Badge variant="secondary" className="h-5 px-1.5 bg-blue-500/10 text-blue-400 border-blue-500/20 text-[10px]">
+                                    <Check className="w-3 h-3 mr-1" />
+                                    OFFICIAL
+                                </Badge>
+                            </div>
+                        )}
+                    </div>
+                    <p className="text-xs text-white/50 truncate">
+                        {server.namespace} • v{server.version}
+                    </p>
+                </div>
+            </CardHeader>
+
+            <CardContent className="flex-1 pb-4">
+                <p className="text-sm text-white/70 line-clamp-3 leading-relaxed">
+                    {server.description || "No description provided."}
+                </p>
+            </CardContent>
+
+            <CardFooter className="pt-0 gap-2">
+                {isDeployable ? (
+                    <Link href={`/configure?registryId=${encodeURIComponent(server.id)}`} className="w-full">
+                        <Button className="w-full bg-white/10 hover:bg-white/20 text-white border border-white/10 backdrop-blur-sm">
+                            <Rocket className="w-4 h-4 mr-2" />
+                            Deploy
+                        </Button>
+                    </Link>
+                ) : (
+                    <Link href={`/configure?registryId=${encodeURIComponent(server.id)}&mode=connect`} className="w-full">
+                        <Button variant="outline" className="w-full border-white/10 text-white/70 hover:text-white hover:bg-white/5">
+                            <LinkIcon className="w-4 h-4 mr-2" />
+                            Connect
+                        </Button>
+                    </Link>
+                )}
+            </CardFooter>
+        </Card>
+    );
+}
diff --git a/frontend/components/ui/Button.tsx b/frontend/components/ui/Button.tsx
deleted file mode 100644
index b27bcf1..0000000
--- a/frontend/components/ui/Button.tsx
+++ /dev/null
@@ -1,21 +0,0 @@
-import * as React from "react"
-
-export interface ButtonProps extends React.ButtonHTMLAttributes<HTMLButtonElement> {
-    asChild?: boolean
-}
-
-const Button = React.forwardRef<HTMLButtonElement, ButtonProps>(
-    ({ className, asChild = false, ...props }, ref) => {
-        const Comp = "button"
-        return (
-            <Comp
-                className={className}
-                ref={ref}
-                {...props}
-            />
-        )
-    }
-)
-Button.displayName = "Button"
-
-export { Button }
diff --git a/frontend/components/ui/badge.tsx b/frontend/components/ui/badge.tsx
new file mode 100644
index 0000000..44ca81a
--- /dev/null
+++ b/frontend/components/ui/badge.tsx
@@ -0,0 +1,36 @@
+import * as React from "react"
+import { cva, type VariantProps } from "class-variance-authority"
+
+import { cn } from "@/lib/utils"
+
+const badgeVariants = cva(
+    "inline-flex items-center rounded-full border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2",
+    {
+        variants: {
+            variant: {
+                default:
+                    "border-transparent bg-primary text-primary-foreground hover:bg-primary/80",
+                secondary:
+                    "border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80",
+                destructive:
+                    "border-transparent bg-destructive text-destructive-foreground hover:bg-destructive/80",
+                outline: "text-foreground",
+            },
+        },
+        defaultVariants: {
+            variant: "default",
+        },
+    }
+)
+
+export interface BadgeProps
+    extends React.HTMLAttributes<HTMLDivElement>,
+    VariantProps<typeof badgeVariants> { }
+
+function Badge({ className, variant, ...props }: BadgeProps) {
+    return (
+        <div className={cn(badgeVariants({ variant }), className)} {...props} />
+    )
+}
+
+export { Badge, badgeVariants }
diff --git a/frontend/components/ui/button.tsx b/frontend/components/ui/button.tsx
new file mode 100644
index 0000000..28b2fa4
--- /dev/null
+++ b/frontend/components/ui/button.tsx
@@ -0,0 +1,56 @@
+import * as React from "react"
+import { Slot } from "@radix-ui/react-slot"
+import { cva, type VariantProps } from "class-variance-authority"
+
+import { cn } from "@/lib/utils"
+
+const buttonVariants = cva(
+    "inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50",
+    {
+        variants: {
+            variant: {
+                default: "bg-primary text-primary-foreground hover:bg-primary/90",
+                destructive:
+                    "bg-destructive text-destructive-foreground hover:bg-destructive/90",
+                outline:
+                    "border border-input bg-background hover:bg-accent hover:text-accent-foreground",
+                secondary:
+                    "bg-secondary text-secondary-foreground hover:bg-secondary/80",
+                ghost: "hover:bg-accent hover:text-accent-foreground",
+                link: "text-primary underline-offset-4 hover:underline",
+            },
+            size: {
+                default: "h-10 px-4 py-2",
+                sm: "h-9 rounded-md px-3",
+                lg: "h-11 rounded-md px-8",
+                icon: "h-10 w-10",
+            },
+        },
+        defaultVariants: {
+            variant: "default",
+            size: "default",
+        },
+    }
+)
+
+export interface ButtonProps
+    extends React.ButtonHTMLAttributes<HTMLButtonElement>,
+    VariantProps<typeof buttonVariants> {
+    asChild?: boolean
+}
+
+const Button = React.forwardRef<HTMLButtonElement, ButtonProps>(
+    ({ className, variant, size, asChild = false, ...props }, ref) => {
+        const Comp = asChild ? Slot : "button"
+        return (
+            <Comp
+                className={cn(buttonVariants({ variant, size, className }))}
+                ref={ref}
+                {...props}
+            />
+        )
+    }
+)
+Button.displayName = "Button"
+
+export { Button, buttonVariants }
diff --git a/frontend/components/ui/card.tsx b/frontend/components/ui/card.tsx
new file mode 100644
index 0000000..5b8e64f
--- /dev/null
+++ b/frontend/components/ui/card.tsx
@@ -0,0 +1,79 @@
+import * as React from "react"
+
+import { cn } from "@/lib/utils"
+
+const Card = React.forwardRef<
+    HTMLDivElement,
+    React.HTMLAttributes<HTMLDivElement>
+>(({ className, ...props }, ref) => (
+    <div
+        ref={ref}
+        className={cn(
+            "rounded-lg border bg-card text-card-foreground shadow-sm",
+            className
+        )}
+        {...props}
+    />
+))
+Card.displayName = "Card"
+
+const CardHeader = React.forwardRef<
+    HTMLDivElement,
+    React.HTMLAttributes<HTMLDivElement>
+>(({ className, ...props }, ref) => (
+    <div
+        ref={ref}
+        className={cn("flex flex-col space-y-1.5 p-6", className)}
+        {...props}
+    />
+))
+CardHeader.displayName = "CardHeader"
+
+const CardTitle = React.forwardRef<
+    HTMLParagraphElement,
+    React.HTMLAttributes<HTMLHeadingElement>
+>(({ className, ...props }, ref) => (
+    <h3
+        ref={ref}
+        className={cn(
+            "text-2xl font-semibold leading-none tracking-tight",
+            className
+        )}
+        {...props}
+    />
+))
+CardTitle.displayName = "CardTitle"
+
+const CardDescription = React.forwardRef<
+    HTMLParagraphElement,
+    React.HTMLAttributes<HTMLParagraphElement>
+>(({ className, ...props }, ref) => (
+    <p
+        ref={ref}
+        className={cn("text-sm text-muted-foreground", className)}
+        {...props}
+    />
+))
+CardDescription.displayName = "CardDescription"
+
+const CardContent = React.forwardRef<
+    HTMLDivElement,
+    React.HTMLAttributes<HTMLDivElement>
+>(({ className, ...props }, ref) => (
+    <div ref={ref} className={cn("p-6 pt-0", className)} {...props} />
+))
+CardContent.displayName = "CardContent"
+
+const CardFooter = React.forwardRef<
+    HTMLDivElement,
+    React.HTMLAttributes<HTMLDivElement>
+>(({ className, ...props }, ref) => (
+    <div
+        ref={ref}
+        className={cn("flex items-center p-6 pt-0", className)}
+        {...props}
+    />
+))
+CardFooter.displayName = "CardFooter"
+
+export { Card, CardHeader, CardFooter, CardTitle, CardDescription, CardContent }
diff --git a/frontend/components/ui/input.tsx b/frontend/components/ui/input.tsx
new file mode 100644
index 0000000..d191cb8
--- /dev/null
+++ b/frontend/components/ui/input.tsx
@@ -0,0 +1,25 @@
+import * as React from "react"
+
+import { cn } from "@/lib/utils"
+
+export interface InputProps
+    extends React.InputHTMLAttributes<HTMLInputElement> { }
+
+const Input = React.forwardRef<HTMLInputElement, InputProps>(
+    ({ className, type, ...props }, ref) => {
+        return (
+            <input
+                type={type}
+                className={cn(
+                    "flex h-10 w-full rounded-md border border-input bg-background px-3 py-2 text-sm ring-offset-background file:border-0 file:bg-transparent file:text-sm file:font-medium placeholder:text-muted-foreground focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:cursor-not-allowed disabled:opacity-50",
+                    className
+                )}
+                ref={ref}
+                {...props}
+            />
+        )
+    }
+)
+Input.displayName = "Input"
+
+export { Input }
diff --git a/frontend/components/ui/tabs.tsx b/frontend/components/ui/tabs.tsx
new file mode 100644
index 0000000..d6cb359
--- /dev/null
+++ b/frontend/components/ui/tabs.tsx
@@ -0,0 +1,53 @@
+import * as React from "react"
+import * as TabsPrimitive from "@radix-ui/react-tabs"
+
+import { cn } from "@/lib/utils"
+
+const Tabs = TabsPrimitive.Root
+
+const TabsList = React.forwardRef<
+    React.ElementRef<typeof TabsPrimitive.List>,
+    React.ComponentPropsWithoutRef<typeof TabsPrimitive.List>
+>(({ className, ...props }, ref) => (
+    <TabsPrimitive.List
+        ref={ref}
+        className={cn(
+            "inline-flex h-10 items-center justify-center rounded-md bg-muted p-1 text-muted-foreground",
+            className
+        )}
+        {...props}
+    />
+))
+TabsList.displayName = TabsPrimitive.List.displayName
+
+const TabsTrigger = React.forwardRef<
+    React.ElementRef<typeof TabsPrimitive.Trigger>,
+    React.ComponentPropsWithoutRef<typeof TabsPrimitive.Trigger>
+>(({ className, ...props }, ref) => (
+    <TabsPrimitive.Trigger
+        ref={ref}
+        className={cn(
+            "inline-flex items-center justify-center whitespace-nowrap rounded-sm px-3 py-1.5 text-sm font-medium ring-offset-background transition-all focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 data-[state=active]:bg-background data-[state=active]:text-foreground data-[state=active]:shadow-sm",
+            className
+        )}
+        {...props}
+    />
+))
+TabsTrigger.displayName = TabsPrimitive.Trigger.displayName
+
+const TabsContent = React.forwardRef<
+    React.ElementRef<typeof TabsPrimitive.Content>,
+    React.ComponentPropsWithoutRef<typeof TabsPrimitive.Content>
+>(({ className, ...props }, ref) => (
+    <TabsPrimitive.Content
+        ref={ref}
+        className={cn(
+            "mt-2 ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2",
+            className
+        )}
+        {...props}
+    />
+))
+TabsContent.displayName = TabsPrimitive.Content.displayName
+
+export { Tabs, TabsList, TabsTrigger, TabsContent }
diff --git a/frontend/package.json b/frontend/package.json
index af865be..b196427 100644
--- a/frontend/package.json
+++ b/frontend/package.json
@@ -11,7 +11,10 @@
   },
   "dependencies": {
     "@hookform/resolvers": "^5.2.2",
+    "@radix-ui/react-slot": "^1.2.4",
+    "@radix-ui/react-tabs": "^1.1.13",
     "@tanstack/react-query": "^5.90.12",
+    "class-variance-authority": "^0.7.1",
     "clsx": "^2.1.1",
     "lucide-react": "^0.560.0",
     "next": "16.0.10",
diff --git a/test_output.txt b/test_output.txt
new file mode 100644
index 0000000..0d281ea
--- /dev/null
+++ b/test_output.txt
@@ -0,0 +1,46 @@
+============================= test session starts =============================
+platform win32 -- Python 3.11.3, pytest-7.4.2, pluggy-1.0.0 -- C:\Python311\python.exe
+cachedir: .pytest_cache
+benchmark: 4.0.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
+rootdir: C:\Users\Zenchant\catwalk\catwalk-live\backend
+configfile: pytest.ini
+plugins: langsmith-0.3.13, anyio-3.6.2, asyncio-0.21.0, benchmark-4.0.0, cov-4.0.0, integration-0.2.3, mock-3.10.0, recording-0.12.2
+asyncio: mode=Mode.AUTO
+collecting ... collected 0 items / 1 error
+
+=================================== ERRORS ====================================
+________________ ERROR collecting tests/test_registry_forms.py ________________
+ImportError while importing test module 'C:\Users\Zenchant\catwalk\catwalk-live\backend\tests\test_registry_forms.py'.
+Hint: make sure your test modules/packages have valid Python names.
+Traceback:
+..\..\AppData\Roaming\Python\Python311\site-packages\_pytest\python.py:617: in _importtestmodule
+    mod = import_path(self.path, mode=importmode, root=self.config.rootpath)
+..\..\AppData\Roaming\Python\Python311\site-packages\_pytest\pathlib.py:567: in import_path
+    importlib.import_module(module_name)
+C:\Python311\Lib\importlib\__init__.py:126: in import_module
+    return _bootstrap._gcd_import(name[level:], package, level)
+<frozen importlib._bootstrap>:1206: in _gcd_import
+    ???
+<frozen importlib._bootstrap>:1178: in _find_and_load
+    ???
+<frozen importlib._bootstrap>:1149: in _find_and_load_unlocked
+    ???
+<frozen importlib._bootstrap>:690: in _load_unlocked
+    ???
+..\..\AppData\Roaming\Python\Python311\site-packages\_pytest\assertion\rewrite.py:178: in exec_module
+    exec(co, module.__dict__)
+backend\tests\test_registry_forms.py:4: in <module>
+    from fastapi import HTTPException
+C:\Python311\Lib\site-packages\fastapi\__init__.py:7: in <module>
+    from .applications import FastAPI as FastAPI
+C:\Python311\Lib\site-packages\fastapi\applications.py:16: in <module>
+    from fastapi import routing
+C:\Python311\Lib\site-packages\fastapi\routing.py:22: in <module>
+    from fastapi import params
+C:\Python311\Lib\site-packages\fastapi\params.py:4: in <module>
+    from pydantic.fields import FieldInfo, Undefined
+E   ImportError: cannot import name 'Undefined' from 'pydantic.fields' (C:\Python311\Lib\site-packages\pydantic\fields.py)
+=========================== short test summary info ===========================
+ERROR backend\tests\test_registry_forms.py
+!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
+============================== 1 error in 0.88s ===============================
